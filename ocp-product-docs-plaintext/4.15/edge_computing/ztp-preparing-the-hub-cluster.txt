Preparing the hub cluster for ZTP

To use RHACM in a disconnected environment, create a mirror registry that mirrors the Red Hat OpenShift Container Platform release images and Operator Lifecycle Manager (OLM) catalog that contains the required Operator images. OLM manages, installs, and upgrades Operators and their dependencies in the cluster. You can also use a disconnected mirror host to serve the RHCOS ISO and RootFS disk images that are used to provision the bare-metal hosts.
Telco RAN DU 4.15 validated software components
The Red Hat telco RAN DU 4.15 solution has been validated using the following Red Hat software products for Red Hat OpenShift Container Platform managed clusters and hub clusters.
Recommended hub cluster specifications and managed cluster limits for GitOps ZTP
With GitOps Zero Touch Provisioning (ZTP), you can manage thousands of clusters in geographically dispersed regions and networks. The Red Hat Performance and Scale lab successfully created and managed 3500 virtual single-node OpenShift clusters with a reduced DU profile from a single Red Hat Advanced Cluster Management (RHACM) hub cluster in a lab environment.

In real-world situations, the scaling limits for the number of clusters that you can manage will vary depending on various factors affecting the hub cluster. For example:


Hub cluster resources
Available hub cluster host resources (CPU, memory, storage) are an important factor in determining how many clusters the hub cluster can manage.
The more resources allocated to the hub cluster, the more managed clusters it can accommodate.
Hub cluster storage
The hub cluster host storage IOPS rating and whether the hub cluster hosts use NVMe storage can affect hub cluster performance and the number of clusters it can manage.
Network bandwidth and latency
Slow or high-latency network connections between the hub cluster and managed clusters can impact how the hub cluster manages multiple clusters.
Managed cluster size and complexity
The size and complexity of the managed clusters also affects the capacity of the hub cluster.
Larger managed clusters with more nodes, namespaces, and resources require additional processing and management resources.
Similarly, clusters with complex configurations such as the RAN DU profile or diverse workloads can require more resources from the hub cluster.
Number of managed policies
The number of policies managed by the hub cluster scaled over the number of managed clusters bound to those policies is an important factor that determines how many clusters can be managed.
Monitoring and management workloads
RHACM continuously monitors and manages the managed clusters.
The number and complexity of monitoring and management workloads running on the hub cluster can affect its capacity.
Intensive monitoring or frequent reconciliation operations can require additional resources, potentially limiting the number of manageable clusters.
RHACM version and configuration
Different versions of RHACM can have varying performance characteristics and resource requirements.
Additionally, the configuration settings of RHACM, such as the number of concurrent reconciliations or the frequency of health checks, can affect the managed cluster capacity of the hub cluster.


Use the following representative configuration and network specifications to develop your own Hub cluster and network specifications.

The following guidelines are based on internal lab benchmark testing only and do not represent complete bare-metal host specifications.

The following network specifications are representative of a typical real-world RAN network and were applied to the scale lab environment during testing.

Creating and managing single-node OpenShift clusters with RHACM
Installing GitOps ZTP in a disconnected environment
Use Red Hat Advanced Cluster Management (RHACM), Red Hat OpenShift GitOps, and Topology Aware Lifecycle Manager (TALM) on the hub cluster in the disconnected environment to manage the deployment of multiple managed clusters.

You have installed the Red Hat OpenShift Container Platform CLI (oc).

You have logged in as a user with cluster-admin privileges.

You have configured a disconnected mirror registry for use in the cluster.


Install RHACM in the hub cluster. See Installing RHACM in a disconnected environment.

Install GitOps and TALM in the hub cluster.


Installing OpenShift GitOps

Installing TALM

Mirroring an Operator catalog
Adding RHCOS ISO and RootFS images to the disconnected mirror host
Before you begin installing clusters in the disconnected environment with Red Hat Advanced Cluster Management (RHACM), you must first host Red Hat Enterprise Linux CoreOS (RHCOS) images for it to use. Use a disconnected mirror to host the RHCOS images.

Deploy and configure an HTTP server to host the RHCOS image resources on the network. You must be able to access the HTTP server from your computer, and from the machines that you create.


The RHCOS images might not change with every release of Red Hat OpenShift Container Platform. You must download images with the highest version that is less than or equal to the version that you install. Use the image versions that match your Red Hat OpenShift Container Platform version if they are available. You require ISO and RootFS images to install RHCOS on the hosts. RHCOS QCOW2 images are not supported for this installation type.
Log in to the mirror host.

Obtain the RHCOS ISO and RootFS images from mirror.openshift.com, for example:


Verify that the images downloaded successfully and are being served on the disconnected mirror host, for example:


Creating a mirror registry

Mirroring images for a disconnected installation
Enabling the assisted service
Red Hat Advanced Cluster Management (RHACM) uses the assisted service to deploy Red Hat OpenShift Container Platform clusters. The assisted service is deployed automatically when you enable the MultiClusterHub Operator on Red Hat Advanced Cluster Management (RHACM). After that, you need to configure the Provisioning resource to watch all namespaces and to update the AgentServiceConfig custom resource (CR) with references to the ISO and RootFS images that are hosted on the mirror registry HTTP server.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.

You have RHACM with MultiClusterHub enabled.


Enable the Provisioning resource to watch all namespaces and configure mirrors for disconnected environments. For more information, see Enabling the central infrastructure management service.

Update the AgentServiceConfig CR by running the following command:

Add the following entry to the items.spec.osImages field in the CR:
Configuring the hub cluster to use a disconnected mirror registry
You can configure the hub cluster to use a disconnected mirror registry for a disconnected environment.

You have a disconnected hub cluster installation with Red Hat Advanced Cluster Management (RHACM) 2.8 installed.

You have hosted the rootfs and iso images on an HTTP server. See the Additional resources section for guidance about Mirroring the OpenShift Container Platform image repository.


If you enable TLS for the HTTP server, you must confirm the root certificate is signed by an authority trusted by the client and verify the trusted certificate chain between your Red Hat OpenShift Container Platform hub and managed clusters and the HTTP server. Using a server configured with an untrusted certificate prevents the images from being downloaded to the image creation service. Using untrusted HTTPS servers is not supported.
Create a ConfigMap containing the mirror registry config:


A valid NTP server is required during cluster installation. Ensure that a suitable NTP server is available and can be reached from the installed clusters through the disconnected network.
Mirroring the OpenShift Container Platform image repository
Configuring the hub cluster to use unauthenticated registries
You can configure the hub cluster to use unauthenticated registries. Unauthenticated registries does not require authentication to access and download images.

You have installed and configured a hub cluster and installed Red Hat Advanced Cluster Management (RHACM) on the hub cluster.

You have installed the OpenShift Container Platform CLI (oc).

You have logged in as a user with cluster-admin privileges.

You have configured an unauthenticated registry for use with the hub cluster.


Update the AgentServiceConfig custom resource (CR) by running the following command:

Add the unauthenticatedRegistries field in the CR:


Mirror registries are automatically added to the ignore list and do not need to be added under spec.unauthenticatedRegistries. Specifying the PUBLIC_CONTAINER_REGISTRIES environment variable in the ConfigMap overrides the default values with the specified value. The PUBLIC_CONTAINER_REGISTRIES defaults are quay.io and registry.svc.ci.openshift.org.
Verify that you can access the newly added registry from the hub cluster by running the following commands:

Open a debug shell prompt to the hub cluster:

Test access to the unauthenticated registry by running the following command:
Configuring the hub cluster with ArgoCD
You can configure the hub cluster with a set of ArgoCD applications that generate the required installation and policy custom resources (CRs) for each site with GitOps Zero Touch Provisioning (ZTP).

Red Hat Advanced Cluster Management (RHACM) uses SiteConfig CRs to generate the Day 1 managed cluster installation CRs for ArgoCD. Each ArgoCD application can manage a maximum of 300 SiteConfig CRs.
You have a Red Hat OpenShift Container Platform hub cluster with Red Hat Advanced Cluster Management (RHACM) and Red Hat OpenShift GitOps installed.

You have extracted the reference deployment from the GitOps ZTP plugin container as described in the "Preparing the GitOps ZTP site configuration repository" section. Extracting the reference deployment creates the out/argocd/deployment directory referenced in the following procedure.


Prepare the ArgoCD pipeline configuration:


To install the GitOps ZTP plugin, patch the ArgoCD instance in the hub cluster by using the patch file that you previously extracted into the out/argocd/deployment/ directory.
Run the following command:

In RHACM 2.7 and later, the multicluster engine enables the cluster-proxy-addon feature by default.
Apply the following patch to disable the cluster-proxy-addon feature and remove the relevant hub cluster and managed pods that are responsible for this add-on.
Run the following command:

Apply the pipeline configuration to your hub cluster by running the following command:
Preparing the GitOps ZTP site configuration repository
Before you can use the GitOps Zero Touch Provisioning (ZTP) pipeline, you need to prepare the Git repository to host the site configuration data.

You have configured the hub cluster GitOps applications for generating the required installation and policy custom resources (CRs).

You have deployed the managed clusters using GitOps ZTP.


Create a directory structure with separate paths for the SiteConfig and PolicyGenTemplate CRs.

Export the argocd directory from the ztp-site-generate container image using the following commands:

Check that the out directory contains the following subdirectories:

Copy the out/source-crs folder and contents to the PolicyGentemplate directory.

The out/extra-manifests directory contains the reference manifests for a RAN DU cluster.
Copy the out/extra-manifests directory into the SiteConfig folder.
This directory should contain CRs from the ztp-site-generate container only.
Do not add user-provided CRs here.
If you want to work with user-provided CRs you must create another directory for that content.
For example:

Commit the directory structure and the kustomization.yaml files and push to your Git repository.
The initial push to Git should include the kustomization.yaml files.


You can use the directory structure under out/argocd/example as a reference for the structure and content of your Git repository. That structure includes SiteConfig and PolicyGenTemplate reference CRs for single-node, three-node, and standard clusters. Remove references to cluster types that you are not using.

For all cluster types, you must:

Add the source-crs subdirectory to the policygentemplate directory.

Add the extra-manifests directory to the siteconfig directory.


The following example describes a set of CRs for a network of single-node clusters:

example/
  ├── policygentemplates
  │   ├── common-ranGen.yaml
  │   ├── example-sno-site.yaml
  │   ├── group-du-sno-ranGen.yaml
  │   ├── group-du-sno-validator-ranGen.yaml
  │   ├── kustomization.yaml
  │   ├── source-crs/
  │   └── ns.yaml
  └── siteconfig
        ├── example-sno.yaml
        ├── extra-manifests/ 1
        ├── custom-manifests/ 2
        ├── KlusterletAddonConfigOverride.yaml
        └── kustomization.yaml
Contains reference manifests from the ztp-container.

Contains custom manifests.


Preparing the GitOps ZTP site configuration repository for version independence
You can use GitOps ZTP to manage source custom resources (CRs) for managed clusters that are running different versions of Red Hat OpenShift Container Platform. This means that the version of Red Hat OpenShift Container Platform running on the hub cluster can be independent of the version running on the managed clusters.

Create a directory structure with separate paths for the SiteConfig and PolicyGenTemplate CRs.

Within the PolicyGenTemplate directory, create a directory for each Red Hat OpenShift Container Platform version you want to make available.
For each version, create the following resources:

In the /siteconfig directory, create a subdirectory for each Red Hat OpenShift Container Platform version you want to make available. For each version, create at least one directory for reference CRs to be copied from the container. There is no restriction on the naming of directories or on the number of reference directories. If you want to work with custom manifests, you must create a separate directory for them.

Edit the SiteConfig CR to include the search paths of any directories you have created.
The first directory that is listed under extraManifests.searchPaths must be the directory containing the reference manifests.
Consider the order in which the directories are listed.
In cases where directories contain files with the same name, the file in the final directory takes precedence.

Edit the top-level kustomization.yaml file to control which Red Hat OpenShift Container Platform versions are active. The following is an example of a kustomization.yaml file at the top level: