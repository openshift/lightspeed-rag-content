Updating managed clusters in a disconnected environment with the Topology Aware Lifecycle Manager

You can use the Topology Aware Lifecycle Manager (TALM) to manage the software lifecycle of Red Hat OpenShift Container Platform managed clusters. TALM uses Red Hat Advanced Cluster Management (RHACM) policies to perform changes on the target clusters.

For more information about the Topology Aware Lifecycle Manager, see About the Topology Aware Lifecycle Manager.
Updating clusters in a disconnected environment
You can upgrade managed clusters and Operators for managed clusters that you have deployed using GitOps Zero Touch Provisioning (ZTP) and Topology Aware Lifecycle Manager (TALM).

Setting up the environment
TALM can perform both platform and Operator updates.

You must mirror both the platform image and Operator images that you want to update to in your mirror registry before you can use TALM to update your disconnected clusters. Complete the following steps to mirror the images:

For platform updates, you must perform the following steps:

For Operator updates, you must perform the following task:


For more information about how to update GitOps Zero Touch Provisioning (ZTP), see Upgrading GitOps ZTP.

For more information about how to mirror an Red Hat OpenShift Container Platform image repository, see Mirroring the Red Hat OpenShift Container Platform image repository.

For more information about how to mirror Operator catalogs for disconnected clusters, see Mirroring Operator catalogs for use with disconnected clusters.

For more information about how to prepare the disconnected environment and mirroring the desired image repository, see Preparing the disconnected environment.

For more information about update channels and releases, see Understanding update channels and releases.
Performing a platform update
You can perform a platform update with the TALM.

Install the Topology Aware Lifecycle Manager (TALM).

Update GitOps Zero Touch Provisioning (ZTP) to the latest version.

Provision one or more managed clusters with GitOps ZTP.

Mirror the desired image repository.

Log in as a user with cluster-admin privileges.

Create RHACM policies in the hub cluster.


Create a PolicyGenTemplate CR for the platform update:

Create the ClusterGroupUpdate CR for the platform update with the spec.enable field set to false.

Optional: Pre-cache the images for the platform update.

Start the platform update:


For more information about mirroring the images in a disconnected environment, see Preparing the disconnected environment.
Performing an Operator update
You can perform an Operator update with the TALM.

Install the Topology Aware Lifecycle Manager (TALM).

Update GitOps Zero Touch Provisioning (ZTP) to the latest version.

Provision one or more managed clusters with GitOps ZTP.

Mirror the desired index image, bundle images, and all Operator images referenced in the bundle images.

Log in as a user with cluster-admin privileges.

Create RHACM policies in the hub cluster.


Update the PolicyGenTemplate CR for the Operator update.

Apply the required catalog source updates before starting the Operator update.

Create the ClusterGroupUpgrade CR for the Operator update with the spec.enable field set to false.

Optional: Pre-cache the images for the Operator update.

Start the Operator update.


For more information about updating GitOps ZTP, see Upgrading GitOps ZTP.

Troubleshooting missed Operator updates due to out-of-date policy compliance states.


Troubleshooting missed Operator updates due to out-of-date policy compliance states
In some scenarios, Topology Aware Lifecycle Manager (TALM) might miss Operator updates due to an out-of-date policy compliance state.

After a catalog source update, it takes time for the Operator Lifecycle Manager (OLM) to update the subscription status. The status of the subscription policy might continue to show as compliant while TALM decides whether remediation is needed. As a result, the Operator specified in the subscription policy does not get upgraded.

To avoid this scenario, add another catalog source configuration to the PolicyGenTemplate and specify this configuration in the subscription for any Operators that require an update.

Add a catalog source configuration in the PolicyGenTemplate resource:

Update the Subscription resource to point to the new configuration for Operators that require an update:
Performing a platform and an Operator update together
You can perform a platform and an Operator update at the same time.

Install the Topology Aware Lifecycle Manager (TALM).

Update GitOps Zero Touch Provisioning (ZTP) to the latest version.

Provision one or more managed clusters with GitOps ZTP.

Log in as a user with cluster-admin privileges.

Create RHACM policies in the hub cluster.


Create the PolicyGenTemplate CR for the updates by following the steps described in the "Performing a platform update" and "Performing an Operator update" sections.

Apply the prep work for the platform and the Operator update.

Create the ClusterGroupUpdate CR for the platform and the Operator update with the spec.enable field set to false.

Optional: Pre-cache the images for the platform and the Operator update.

Start the platform and Operator update.
Removing Performance Addon Operator subscriptions from deployed clusters
In earlier versions of Red Hat OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In Red Hat OpenShift Container Platform 4.11 or later, these functions are part of the Node Tuning Operator.

Do not install the Performance Addon Operator on clusters running Red Hat OpenShift Container Platform 4.11 or later. If you upgrade to Red Hat OpenShift Container Platform 4.11 or later, the Node Tuning Operator automatically removes the Performance Addon Operator.

You need to remove any policies that create Performance Addon Operator subscriptions to prevent a re-installation of the Operator.
The reference DU profile includes the Performance Addon Operator in the PolicyGenTemplate CR common-ranGen.yaml. To remove the subscription from deployed managed clusters, you must update common-ranGen.yaml.

If you install Performance Addon Operator 4.10.3-5 or later on Red Hat OpenShift Container Platform 4.11 or later, the Performance Addon Operator detects the cluster version and automatically hibernates to avoid interfering with the Node Tuning Operator functions. However, to ensure best performance, remove the Performance Addon Operator from your Red Hat OpenShift Container Platform 4.11 clusters.
Create a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for ArgoCD.

Update to Red Hat OpenShift Container Platform 4.11 or later.

Log in as a user with cluster-admin privileges.


Change the complianceType to mustnothave for the Performance Addon Operator namespace, Operator group, and subscription in the common-ranGen.yaml file.

Merge the changes with your custom site repository and wait for the ArgoCD application to synchronize the change to the hub cluster. The status of the common-subscriptions-policy policy changes to Non-Compliant.

Apply the change to your target clusters by using the Topology Aware Lifecycle Manager. For more information about rolling out configuration changes, see the "Additional resources" section.

Monitor the process. When the status of the common-subscriptions-policy policy for a target cluster  is Compliant, the Performance Addon Operator has been removed from the cluster. Get the status of the common-subscriptions-policy by running the following command:

Delete the Performance Addon Operator namespace, Operator group and subscription CRs from .spec.sourceFiles in the common-ranGen.yaml file.

Merge the changes with your custom site repository and wait for the ArgoCD application to synchronize the change to the hub cluster. The policy remains compliant.
Pre-caching user-specified images with TALM on single-node OpenShift clusters
You can pre-cache application-specific workload images on single-node OpenShift clusters before upgrading your applications.

You can specify the configuration options for the pre-caching jobs using the following custom resources (CR):

PreCachingConfig CR

ClusterGroupUpgrade CR


All fields in the PreCachingConfig CR are optional.
apiVersion: ran.openshift.io/v1alpha1
kind: PreCachingConfig
metadata:
  name: exampleconfig
  namespace: exampleconfig-ns
spec:
  overrides: 1
    platformImage: quay.io/openshift-release-dev/ocp-release@sha256:3d5800990dee7cd4727d3fe238a97e2d2976d3808fc925ada29c559a47e2e1ef
    operatorsIndexes:
      - registry.example.com:5000/custom-redhat-operators:1.0.0
    operatorsPackagesAndChannels:
      - local-storage-operator: stable
      - ptp-operator: stable
      - sriov-network-operator: stable
  spaceRequired: 30 Gi 2
  excludePrecachePatterns: 3
    - aws
    - vsphere
  additionalImages: 4
    - quay.io/exampleconfig/application1@sha256:3d5800990dee7cd4727d3fe238a97e2d2976d3808fc925ada29c559a47e2e1ef
    - quay.io/exampleconfig/application2@sha256:3d5800123dee7cd4727d3fe238a97e2d2976d3808fc925ada29c559a47adfaef
    - quay.io/exampleconfig/applicationN@sha256:4fe1334adfafadsf987123adfffdaf1243340adfafdedga0991234afdadfsa09
By default, TALM automatically populates the platformImage, operatorsIndexes, and the operatorsPackagesAndChannels fields from the policies of the managed clusters. You can specify values to override the default TALM-derived values for these fields.

Specifies the minimum required disk space on the cluster. If unspecified, TALM defines a default value for Red Hat OpenShift Container Platform images. The disk space field must include an integer value and the storage unit. For example: 40 GiB, 200 MB, 1 TiB.

Specifies the images to exclude from pre-caching based on image name matching.

Specifies the list of additional images to pre-cache.


apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu
spec:
  preCaching: true 1
  preCachingConfigRef:
    name: exampleconfig 2
    namespace: exampleconfig-ns 3
The preCaching field set to true enables the pre-caching job.

The preCachingConfigRef.name field specifies the PreCachingConfig CR that you want to use.

The preCachingConfigRef.namespace specifies the namespace of the PreCachingConfig CR that you want to use.


Creating the custom resources for pre-caching
You must create the PreCachingConfig CR before or concurrently with the ClusterGroupUpgrade CR.

Create the PreCachingConfig CR with the list of additional images you want to pre-cache.

Create a ClusterGroupUpgrade CR with the preCaching field set to true and specify the PreCachingConfig CR created in the previous step:

When you want to start pre-caching the images, apply the ClusterGroupUpgrade CR by running the following command:


TALM verifies the ClusterGroupUpgrade CR.

From this point, you can continue with the TALM pre-caching workflow.

All sites are pre-cached concurrently.
Check the pre-caching status on the hub cluster where the ClusterUpgradeGroup CR is applied by running the following command:

You can find the pre-caching job by running the following command on the managed cluster:

You can check the status of the pod created for the pre-caching job by running the following command:

You can get live updates on the status of the job by running the following command:

To verify the pre-cache job is successfully completed, run the following command:

To verify that the images are successfully pre-cached on the single-node OpenShift, do the following:


For more information about the TALM precaching workflow, see Using the container image precache feature.
About the auto-created ClusterGroupUpgrade CR for GitOps ZTP
TALM has a controller called ManagedClusterForCGU that monitors the Ready state of the ManagedCluster CRs on the hub cluster and creates the ClusterGroupUpgrade CRs for GitOps Zero Touch Provisioning (ZTP).

For any managed cluster in the Ready state without a ztp-done label applied, the ManagedClusterForCGU controller automatically creates a ClusterGroupUpgrade CR in the ztp-install namespace with its associated RHACM policies that are created during the GitOps ZTP process. TALM then remediates the set of configuration policies that are listed in the auto-created ClusterGroupUpgrade CR to push the configuration CRs to the managed cluster.

If there are no policies for the managed cluster at the time when the cluster becomes Ready, a ClusterGroupUpgrade CR with no policies is created. Upon completion of the ClusterGroupUpgrade the managed cluster is labeled as ztp-done. If there are policies that you want to apply for that managed cluster, manually create a ClusterGroupUpgrade as a day-2 operation.

apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  generation: 1
  name: spoke1
  namespace: ztp-install
  ownerReferences:
  - apiVersion: cluster.open-cluster-management.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: ManagedCluster
    name: spoke1
    uid: 98fdb9b2-51ee-4ee7-8f57-a84f7f35b9d5
  resourceVersion: "46666836"
  uid: b8be9cd2-764f-4a62-87d6-6b767852c7da
spec:
  actions:
    afterCompletion:
      addClusterLabels:
        ztp-done: "" 1
      deleteClusterLabels:
        ztp-running: ""
      deleteObjects: true
    beforeEnable:
      addClusterLabels:
        ztp-running: "" 2
  clusters:
  - spoke1
  enable: true
  managedPolicies:
  - common-spoke1-config-policy
  - common-spoke1-subscriptions-policy
  - group-spoke1-config-policy
  - spoke1-config-policy
  - group-spoke1-validator-du-policy
  preCaching: false
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240
Applied to the managed cluster when TALM completes the cluster configuration.

Applied to the managed cluster when TALM starts deploying the configuration policies.