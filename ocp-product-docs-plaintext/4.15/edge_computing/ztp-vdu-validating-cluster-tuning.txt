Validating single-node OpenShift cluster tuning for vDU application workloads

Before you can deploy virtual distributed unit (vDU) applications, you need to tune and configure the cluster host firmware and various other cluster configuration settings. Use the following information to validate the cluster configuration to support vDU workloads.

Workload partitioning in single-node OpenShift with GitOps ZTP

Reference configuration for deploying vDUs on single-node OpenShift
Recommended firmware configuration for vDU cluster hosts
Use the following table as the basis to configure the cluster host firmware for vDU applications running on Red Hat OpenShift Container Platform 4.15.

The following table is a general recommendation for vDU cluster host firmware configuration. Exact firmware settings will depend on your requirements and specific hardware platform. Automatic setting of firmware is not handled by the zero touch provisioning pipeline.

Enable global SR-IOV and VT-d settings in the firmware for the host. These settings are relevant to bare-metal environments.
Enable both C-states and OS-controlled P-States to allow per pod power management.
Recommended cluster configurations to run vDU applications
Clusters running virtualized distributed unit (vDU) applications require a highly tuned and optimized configuration. The following information describes the various elements that you require to support vDU workloads in Red Hat OpenShift Container Platform 4.15 clusters.

Recommended cluster MachineConfig CRs for single-node OpenShift clusters
Check that the MachineConfig custom resources (CRs) that you extract from the ztp-site-generate container are applied in the cluster. The CRs can be found in the extracted out/source-crs/extra-manifest/ folder.

The following MachineConfig CRs from the ztp-site-generate container configure the cluster host:


In Red Hat OpenShift Container Platform 4.14 and later, you configure workload partitioning with the cpuPartitioningMode field in the SiteConfig CR.
Workload partitioning in single-node OpenShift with GitOps ZTP

Extracting source CRs from the ztp-site-generate container
Recommended cluster Operators
The following Operators are required for clusters running virtualized distributed unit (vDU) applications and are a part of the baseline reference configuration:

Node Tuning Operator (NTO). NTO packages functionality that was previously delivered with the Performance Addon Operator, which is now a part of NTO.

PTP Operator

SR-IOV Network Operator

Red Hat OpenShift Logging Operator

Local Storage Operator
Recommended cluster kernel configuration
Always use the latest supported real-time kernel version in your cluster. Ensure that you apply the following configurations in the cluster:

Ensure that the following additionalKernelArgs are set in the cluster performance profile:

Ensure that the performance-patch profile in the Tuned CR configures the correct CPU isolation set that matches the isolated CPU set in the related PerformanceProfile CR, for example:
Checking the realtime kernel version
Always use the latest version of the realtime kernel in your Red Hat OpenShift Container Platform clusters. If you are unsure about the kernel version that is in use in the cluster, you can compare the current realtime kernel version to the release version with the following procedure.

You have installed the OpenShift CLI (oc).

You are logged in as a user with cluster-admin privileges.

You have installed podman.


Run the following command to get the cluster version:

Get the release image SHA number:

Run the release image container and extract the kernel version that is packaged with cluster's current release:


Check that the kernel version listed for the cluster's current release matches actual realtime kernel that is running in the cluster. Run the following commands to check the running realtime kernel version:

Open a remote shell connection to the cluster node:

Check the realtime kernel version:
Checking that the recommended cluster configurations are applied
You can check that clusters are running the correct configuration. The following procedure describes how to check the various configurations that you require to deploy a DU application in Red Hat OpenShift Container Platform 4.15 clusters.

You have deployed a cluster and tuned it for vDU workloads.

You have installed the OpenShift CLI (oc).

You have logged in as a user with cluster-admin privileges.


Check that the default OperatorHub sources are disabled. Run the following command:

Check that all required CatalogSource resources are annotated for workload partitioning (PreferredDuringScheduling) by running the following command:

Check that all applicable Red Hat OpenShift Container Platform Operator namespaces are annotated for workload partitioning. This includes all Operators installed with core Red Hat OpenShift Container Platform and the set of additional Operators included in the reference DU tuning configuration. Run the following command:

Check that the ClusterLogging configuration is correct. Run the following commands:

Check that the web console is disabled (managementState: Removed) by running the following command:

Check that chronyd is disabled on the cluster node by running the following commands:

Check that the PTP interface is successfully synchronized to the primary clock using a remote shell connection to the linuxptp-daemon container and the PTP Management Client (pmc) tool:

Check that the SR-IOV configuration is correct by running the following commands:

Check that the cluster performance profile is correct. The cpu and hugepages sections will vary depending on your hardware configuration. Run the following command:

Check that the PerformanceProfile was successfully applied to the cluster by running the following command:

Check the Tuned performance patch settings by running the following command:

Check that cluster networking diagnostics are disabled by running the following command:

Check that the Kubelet housekeeping interval is tuned to slower rate. This is set in the containerMountNS machine config. Run the following command:

Check that Grafana and alertManagerMain are disabled and that the Prometheus retention period is set to 24h by running the following command:

Check that there is a minimum of 4 CPUs allocated as reserved for each of the PerformanceProfile, Tuned performance-patch, workload partitioning, and kernel command line arguments by running the following command: