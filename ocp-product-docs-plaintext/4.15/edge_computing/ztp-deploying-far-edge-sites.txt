Installing managed clusters with RHACM and SiteConfig resources

You can provision Red Hat OpenShift Container Platform clusters at scale with Red Hat Advanced Cluster Management (RHACM) using the assisted service and the GitOps plugin policy generator with core-reduction technology enabled. The GitOps Zero Touch Provisioning (ZTP) pipeline performs the cluster installations. GitOps ZTP can be used in a disconnected environment.
GitOps ZTP and Topology Aware Lifecycle Manager
GitOps Zero Touch Provisioning (ZTP) generates installation and configuration CRs from manifests stored in Git. These artifacts are applied to a centralized hub cluster where Red Hat Advanced Cluster Management (RHACM), the assisted service, and the Topology Aware Lifecycle Manager (TALM) use the CRs to install and configure the managed cluster. The configuration phase of the GitOps ZTP pipeline uses the TALM to orchestrate the application of the configuration CRs to the cluster. There are several key integration points between GitOps ZTP and the TALM.


Inform policies
By default, GitOps ZTP creates all policies with a remediation action of inform. These policies cause RHACM to report on compliance status of clusters relevant to the policies but does not apply the desired configuration. During the GitOps ZTP process, after OpenShift installation, the TALM steps through the created inform policies and enforces them on the target managed cluster(s). This applies the configuration to the managed cluster. Outside of the GitOps ZTP phase of the cluster lifecycle, this allows you to change policies without the risk of immediately rolling those changes out to affected managed clusters. You can control the timing and the set of remediated clusters by using TALM.
Automatic creation of ClusterGroupUpgrade CRs
To automate the initial configuration of newly deployed clusters, TALM monitors the state of all ManagedCluster CRs on the hub cluster. Any ManagedCluster CR that does not have a ztp-done label applied, including newly created ManagedCluster CRs, causes the TALM to automatically create a ClusterGroupUpgrade CR with the following characteristics:
Waves
Each policy generated from a PolicyGenTemplate CR includes a ztp-deploy-wave annotation. This annotation is based on the same annotation from each CR which is included in that policy. The wave annotation is used to order the policies in the auto-generated ClusterGroupUpgrade CR. The wave annotation is not used other than for the auto-generated ClusterGroupUpgrade CR.


To check the default wave value in each source CR, run the following command against the out/source-crs directory that is extracted from the ztp-site-generate container image:

$ grep -r "ztp-deploy-wave" out/source-crs

Phase labels
The ClusterGroupUpgrade CR is automatically created and includes directives to annotate the ManagedCluster CR with labels at the start and end of the GitOps ZTP process.
Linked CRs
The automatically created ClusterGroupUpgrade CR has the owner reference set as the ManagedCluster from which it was derived. This reference ensures that deleting the ManagedCluster CR causes the instance of the ClusterGroupUpgrade to be deleted along with any supporting resources.
Overview of deploying managed clusters with GitOps ZTP
Red Hat Advanced Cluster Management (RHACM) uses GitOps Zero Touch Provisioning (ZTP) to deploy single-node Red Hat OpenShift Container Platform clusters, three-node clusters, and standard clusters. You manage site configuration data as Red Hat OpenShift Container Platform custom resources (CRs) in a Git repository. GitOps ZTP uses a declarative GitOps approach for a develop once, deploy anywhere model to deploy the managed clusters.

The deployment of the clusters includes:

Installing the host operating system (RHCOS) on a blank server

Deploying Red Hat OpenShift Container Platform

Creating cluster policies and site subscriptions

Making the necessary network configurations to the server operating system

Deploying profile Operators and performing any needed software-related configuration, such as performance profile, PTP, and SR-IOV



After you apply the managed site custom resources (CRs) on the hub cluster, the following actions happen automatically:

A Discovery image ISO file is generated and booted on the target host.

When the ISO file successfully boots on the target host it reports the host hardware information to RHACM.

After all hosts are discovered, Red Hat OpenShift Container Platform is installed.

When Red Hat OpenShift Container Platform finishes installing, the hub installs the klusterlet service on the target cluster.

The requested add-on services are installed on the target cluster.


The Discovery image ISO process is complete when the Agent CR  for the managed cluster is created on the hub cluster.

The target bare-metal host must meet the networking, firmware, and hardware requirements listed in Recommended single-node OpenShift cluster configuration for vDU application workloads.
Creating the managed bare-metal host secrets
Add the required Secret custom resources (CRs) for the managed bare-metal host to the hub cluster. You need a secret for the GitOps Zero Touch Provisioning (ZTP) pipeline to access the Baseboard Management Controller (BMC) and a secret for the assisted installer service to pull cluster installation images from the registry.

The secrets are referenced from the SiteConfig CR by name. The namespace must match the SiteConfig namespace.
Create a YAML secret file containing credentials for the host Baseboard Management Controller (BMC) and a pull secret required for installing OpenShift and all add-on cluster Operators:

Add the relative path to example-sno-secret.yaml to the kustomization.yaml file that you use to install the cluster.
Configuring Discovery ISO kernel arguments for installations using GitOps ZTP
The GitOps Zero Touch Provisioning (ZTP) workflow uses the Discovery ISO as part of the Red Hat OpenShift Container Platform installation process on managed bare-metal hosts. You can edit the InfraEnv resource to specify kernel arguments for the Discovery ISO. This is useful for cluster installations with specific environmental requirements. For example, configure the rd.net.timeout.carrier kernel argument for the Discovery ISO to facilitate static networking for the cluster or to receive a DHCP address before downloading the root file system during installation.

In Red Hat OpenShift Container Platform 4.15, you can only add kernel arguments. You can not replace or delete kernel arguments.
You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Create the InfraEnv CR and edit the spec.kernelArguments specification to configure kernel arguments.

Commit the InfraEnv-example.yaml CR to the same location in your Git repository that has the SiteConfig CR and push your changes. The following example shows a sample Git repository structure:

Edit the spec.clusters.crTemplates specification in the SiteConfig CR to reference the InfraEnv-example.yaml CR in your Git repository:


To verify that the kernel arguments are applied, after the Discovery image verifies that Red Hat OpenShift Container Platform is ready for installation, you can SSH to the target host before the installation process begins. At that point, you can view the kernel arguments for the Discovery ISO in the /proc/cmdline file.

Begin an SSH session with the target host:

View the system's kernel arguments by using the following command:
Deploying a managed cluster with SiteConfig and GitOps ZTP
Use the following procedure to create a SiteConfig custom resource (CR) and related files and initiate the GitOps Zero Touch Provisioning (ZTP) cluster deployment.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.

You configured the hub cluster for generating the required installation and policy CRs.

You created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and you must configure it as a source repository for the ArgoCD application. See "Preparing the GitOps ZTP site configuration repository" for more information.

To be ready for provisioning managed clusters, you require the following for each bare-metal host:


Verify that the custom roles and labels are applied after the node is deployed:


Name:   example-node.example.com
Roles:  control-plane,example-label,master,worker
Labels: beta.kubernetes.io/arch=amd64
        beta.kubernetes.io/os=linux
        custom-label/parameter1=true
        kubernetes.io/arch=amd64
        kubernetes.io/hostname=cnfdf03.telco5gran.eng.rdu2.redhat.com
        kubernetes.io/os=linux
        node-role.kubernetes.io/control-plane=
        node-role.kubernetes.io/example-label= 1
        node-role.kubernetes.io/master=
        node-role.kubernetes.io/worker=
        node.openshift.io/os_id=rhcos
The custom label is applied to the node.


Single-node OpenShift SiteConfig CR installation reference


Single-node OpenShift SiteConfig CR installation reference

Customizing extra installation manifests in the GitOps ZTP pipeline

Preparing the GitOps ZTP site configuration repository

Configuring the hub cluster with ArgoCD

Signalling ZTP cluster deployment completion with validator inform policies

Creating the managed bare-metal host secrets

BMC addressing

About root device hints
Monitoring managed cluster installation progress
The ArgoCD pipeline uses the SiteConfig CR to generate the cluster configuration CRs and syncs it with the hub cluster. You can monitor the progress of the synchronization in the ArgoCD dashboard.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


When the synchronization is complete, the installation generally proceeds as follows:

The Assisted Service Operator installs Red Hat OpenShift Container Platform on the cluster. You can monitor the progress of cluster installation from the RHACM dashboard or from the command line by running the following commands:
Troubleshooting GitOps ZTP by validating the installation CRs
The ArgoCD pipeline uses the SiteConfig and PolicyGenTemplate custom resources (CRs) to generate the cluster configuration CRs and Red Hat Advanced Cluster Management (RHACM) policies. Use the following steps to troubleshoot issues that might occur during this process.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Check that the installation CRs were created by using the following command:

Verify that the ManagedCluster CR was generated using the SiteConfig CR on the hub cluster:

If the ManagedCluster is missing, check if the clusters application failed to synchronize the files from the Git repository to the hub cluster:
Troubleshooting GitOps ZTP virtual media booting on Supermicro servers
SuperMicro X11 servers do not support virtual media installations when the image is served using the https protocol. As a result, single-node OpenShift deployments for this environment fail to boot on the target node. To avoid this issue, log in to the hub cluster and disable Transport Layer Security (TLS) in the Provisioning resource. This ensures the image is not served with TLS even though the image address uses the https scheme.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Disable TLS in the Provisioning resource by running the following command:

Continue the steps to deploy your single-node OpenShift cluster.
Removing a managed cluster site from the GitOps ZTP pipeline
You can remove a managed site and the associated installation and configuration policy CRs from the GitOps Zero Touch Provisioning (ZTP) pipeline.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Remove a site and the associated CRs by removing the associated SiteConfig and PolicyGenTemplate files from the kustomization.yaml file.

Optional: If you want to permanently remove a site, you should also remove the SiteConfig and site-specific PolicyGenTemplate files from the Git repository.

Optional: If you want to remove a site temporarily, for example when redeploying a site, you can leave the SiteConfig and site-specific PolicyGenTemplate CRs in the Git repository.


For information about removing a cluster, see Removing a cluster from management.
Removing obsolete content from the GitOps ZTP pipeline
If a change to the PolicyGenTemplate configuration results in obsolete policies, for example, if you rename policies, use the following procedure to remove the obsolete policies.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Remove the affected PolicyGenTemplate files from the Git repository, commit and push to the remote repository.

Wait for the changes to synchronize through the application and the affected policies to be removed from the hub cluster.

Add the updated PolicyGenTemplate files back to the Git repository, and then commit and push to the remote repository.

Optional: As an alternative, after making changes to PolicyGenTemplate CRs that result in obsolete policies, you can remove these policies from the hub cluster manually. You can delete policies from the RHACM console using the Governance tab or by running the following command:
Tearing down the GitOps ZTP pipeline
You can remove the ArgoCD pipeline and all generated GitOps Zero Touch Provisioning (ZTP) artifacts.

You have installed the OpenShift CLI (oc).

You have logged in to the hub cluster as a user with cluster-admin privileges.


Detach all clusters from Red Hat Advanced Cluster Management (RHACM) on the hub cluster.

Delete the kustomization.yaml file in the deployment directory using the following command:

Commit and push your changes to the site repository.