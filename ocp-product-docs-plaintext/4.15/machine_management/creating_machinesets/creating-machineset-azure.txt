Creating a compute machine set on Azure

You can create a different compute machine set to serve a specific purpose in your Red Hat OpenShift Container Platform cluster on Microsoft Azure. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.

You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.

Clusters with the infrastructure platform type none cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.

To view the platform type for your cluster, run the following command:

$ oc get infrastructure cluster -o jsonpath='{.status.platform}'
Sample YAML for a compute machine set custom resource on Azure
This sample YAML defines a compute machine set that runs in the 1 Microsoft Azure zone in a region and creates nodes that are labeled with node-role.kubernetes.io/<role>: "".

In this sample, <infrastructure_id> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <role> is the node label to add.

apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> 1
    machine.openshift.io/cluster-api-machine-role: <role> 2
    machine.openshift.io/cluster-api-machine-type: <role>
  name: <infrastructure_id>-<role>-<region> 3
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>-<region>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
        machine.openshift.io/cluster-api-machine-role: <role>
        machine.openshift.io/cluster-api-machine-type: <role>
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>-<region>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          machine.openshift.io/cluster-api-machineset: <machineset_name>
          node-role.kubernetes.io/<role>: ""
      providerSpec:
        value:
          apiVersion: azureproviderconfig.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image: 4
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/<infrastructure_id>-rg/providers/Microsoft.Compute/galleries/gallery_<infrastructure_id>/images/<infrastructure_id>-gen2/versions/latest 5
            sku: ""
            version: ""
          internalLoadBalancer: ""
          kind: AzureMachineProviderSpec
          location: <region> 6
          managedIdentity: <infrastructure_id>-identity
          metadata:
            creationTimestamp: null
          natRule: null
          networkResourceGroup: ""
          osDisk:
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: ""
          resourceGroup: <infrastructure_id>-rg
          sshPrivateKey: ""
          sshPublicKey: ""
          tags:
            - name: <custom_tag_name> 7
              value: <custom_tag_value>
          subnet: <infrastructure_id>-<role>-subnet
          userDataSecret:
            name: worker-user-data
          vmSize: Standard_D4s_v3
          vnet: <infrastructure_id>-vnet
          zone: "1" 8
Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:

Specify the node label to add.

Specify the infrastructure ID, node label, and region.

Specify the image details for your compute machine set. If you want to use an Azure Marketplace image, see "Selecting an Azure Marketplace image".

Specify an image that is compatible with your instance type. The Hyper-V generation V2 images created by the installation program have a -gen2 suffix, while V1 images have the same name without the suffix.

Specify the region to place machines on.

Optional: Specify custom tags in your machine set. Provide the tag name in <custom_tag_name> field and the corresponding tag value in <custom_tag_value> field.

Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
Creating a compute machine set
In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.

Deploy an Red Hat OpenShift Container Platform cluster.

Install the OpenShift CLI (oc).

Log in to oc as a user with cluster-admin permission.


Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <file_name>.yaml.

Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.

Create a MachineSet CR by running the following command:


View the list of compute machine sets by running the following command:
Using the Azure Marketplace offering
You can create a machine set running on Azure that deploys machines that use the Azure Marketplace offering. To use this offering, you must first obtain the Azure Marketplace image. When obtaining your image, consider the following:

While the images are the same, the Azure Marketplace publisher is different depending on your region. If you are located in North America, specify redhat as the publisher. If you are located in EMEA, specify redhat-limited as the publisher.

The offer includes a rh-ocp-worker SKU and a rh-ocp-worker-gen1 SKU. The rh-ocp-worker SKU represents a Hyper-V generation version 2 VM image. The default instance types used in Red Hat OpenShift Container Platform are version 2 compatible. If you plan to use an instance type that is only version 1 compatible, use the image associated with the rh-ocp-worker-gen1 SKU. The rh-ocp-worker-gen1 SKU represents a Hyper-V version 1 VM image.


Installing images with the Azure marketplace is not supported on clusters with 64-bit ARM instances.
You have installed the Azure CLI client (az).

Your Azure account is entitled for the offer and you have logged into this account with the Azure CLI client.


Display all of the available Red Hat OpenShift Container Platform images by running one of the following commands:

Inspect the image for your offer by running one of the following commands:

Review the terms of the offer by running one of the following commands:

Accept the terms of the offering by running one of the following commands:

Record the image details of your offer, specifically the values for publisher, offer, sku, and version.

Add the following parameters to the providerSpec section of your machine set YAML file using the image details for your offer:
Enabling Azure boot diagnostics
You can enable boot diagnostics on Azure machines that your machine set creates.

Have an existing Microsoft Azure
cluster.


Add the diagnostics configuration that is applicable to your storage type to the providerSpec field in your machine set YAML file:


On the Microsoft Azure portal, review the Boot diagnostics page for a machine deployed by the machine set, and verify that you can see the serial logs for the machine.
Machine sets that deploy machines as Spot VMs
You can save on costs by creating a compute machine set running on Azure that deploys machines as non-guaranteed Spot VMs. Spot VMs utilize unused Azure capacity and are less expensive than standard VMs. You can use Spot VMs for workloads that can tolerate interruptions, such as batch or stateless, horizontally scalable workloads.

Azure can terminate a Spot VM at any time. Azure gives a 30-second warning to the user when an interruption occurs. Red Hat OpenShift Container Platform begins to remove the workloads from the affected instances when Azure issues the termination warning.

Interruptions can occur when using Spot VMs for the following reasons:

The instance price exceeds your maximum price

The supply of Spot VMs decreases

Azure needs capacity back


When Azure terminates an instance, a termination handler running on the Spot VM node deletes the machine resource. To satisfy the compute machine set replicas quantity, the compute machine set creates a machine that requests a Spot VM.

Creating Spot VMs by using compute machine sets
You can launch a Spot VM on Azure by adding spotVMOptions to your compute machine set YAML file.

Add the following line under the providerSpec field:


It is strongly recommended to use the default standard VM price as the maxPrice value and to not set the maximum price for Spot VMs.
Machine sets that deploy machines on Ephemeral OS disks
You can create a compute machine set running on Azure that deploys machines on Ephemeral OS disks. Ephemeral OS disks use local VM capacity rather than remote Azure Storage. This configuration therefore incurs no additional cost and provides lower latency for reading, writing, and reimaging.

For more information, see the Microsoft Azure documentation about Ephemeral OS disks for Azure VMs.


Creating machines on Ephemeral OS disks by using compute machine sets
You can launch machines on Ephemeral OS disks on Azure by editing your compute machine set YAML file.

Have an existing Microsoft Azure cluster.


Edit the custom resource (CR) by running the following command:

Add the following to the providerSpec field:

Create a compute machine set using the updated configuration:


On the Microsoft Azure portal, review the Overview page for a machine deployed by the compute machine set, and verify that the Ephemeral OS disk field is set to OS cache placement.
Machine sets that deploy machines with ultra disks as data disks
You can create a machine set running on Azure that deploys machines with ultra disks. Ultra disks are high-performance storage that are intended for use with the most demanding data workloads.

You can also create a persistent volume claim (PVC) that dynamically binds to a storage class backed by Azure ultra disks and mounts them to pods.

Data disks do not support the ability to specify disk throughput or disk IOPS. You can configure these properties by using PVCs.
Microsoft Azure ultra disks documentation

Machine sets that deploy machines on ultra disks using CSI PVCs

Machine sets that deploy machines on ultra disks using in-tree PVCs


Creating machines with ultra disks by using machine sets
You can deploy machines with ultra disks on Azure by editing your machine set YAML file.

Have an existing Microsoft Azure cluster.


Create a custom secret in the openshift-machine-api namespace using the worker data secret by running the following command:

In a text editor, open the userData.txt file and locate the final } character in the file.

Extract the disabling template value to a file called disableTemplating.txt by running the following command:

Combine the userData.txt file and disableTemplating.txt file to create a data secret file by running the following command:

Copy an existing Azure MachineSet custom resource (CR) and edit it by running the following command:

Add the following lines in the positions indicated:

Create a machine set using the updated configuration by running the following command:


Validate that the machines are created by running the following command:

For a machine that is running and has a node attached, validate the partition by running the following command:


To use an ultra disk from within a pod, create a workload that uses the mount point. Create a YAML file similar to the following example:
Troubleshooting resources for machine sets that enable ultra disks
Use the information in this section to understand and recover from issues you might encounter.

Incorrect ultra disk configuration
If an incorrect configuration of the ultraSSDCapability parameter is specified in the machine set, the machine provisioning fails.

For example, if the ultraSSDCapability parameter is set to Disabled, but an ultra disk is specified in the dataDisks parameter, the following error message appears:

StorageAccountType UltraSSD_LRS can be used only when additionalCapabilities.ultraSSDEnabled is set.
To resolve this issue, verify that your machine set configuration is correct.
Unsupported disk parameters
If a region, availability zone, or instance size that is not compatible with ultra disks is specified in the machine set, the machine provisioning fails. Check the logs for the following error message:

failed to create vm <machine_name>: failure sending request for machine <machine_name>: cannot create vm: compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code="BadRequest" Message="Storage Account type 'UltraSSD_LRS' is not supported <more_information_about_why>."
To resolve this issue, verify that you are using this feature in a supported environment and that your machine set configuration is correct.
Unable to delete disks
If the deletion of ultra disks as data disks is not working as expected, the machines are deleted and the data disks are orphaned. You must delete the orphaned disks manually if desired.
Enabling customer-managed encryption keys for a machine set
You can supply an encryption key to Azure to encrypt data on managed disks at rest. You can enable server-side encryption with customer-managed keys by using the Machine API.

An Azure Key Vault, a disk encryption set, and an encryption key are required to use a customer-managed key. The disk encryption set must be in a resource group where the Cloud Credential Operator (CCO) has granted permissions. If not, an additional reader role is required to be granted on the disk encryption set.

Create an Azure Key Vault instance.

Create an instance of a disk encryption set.

Grant the disk encryption set access to key vault.


Configure the disk encryption set under the providerSpec field in your machine set YAML file. For example:


Azure documentation about customer-managed keys
Configuring trusted launch for Azure virtual machines by using machine sets
Using trusted launch for Azure virtual machines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
Red Hat OpenShift Container Platform 4.15 supports trusted launch for Azure virtual machines (VMs). By editing the machine set YAML file, you can configure the trusted launch options that a machine set uses for machines that it deploys. For example, you can configure these machines to use UEFI security features such as Secure Boot or a dedicated virtual Trusted Platform Module (vTPM) instance.

Some feature combinations result in an invalid configuration.

Using the secureBoot field.

Using the virtualizedTrustedPlatformModule field.
For more information about related features and functionality, see the Microsoft Azure documentation about Trusted launch for Azure virtual machines.

In a text editor, open the YAML file for an existing machine set or create a new one.

Edit the following section under the providerSpec field to provide a valid configuration:


On the Azure portal, review the details for a machine deployed by the machine set and verify that the trusted launch options match the values that you configured.
Configuring Azure confidential virtual machines by using machine sets
Using Azure confidential virtual machines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
Red Hat OpenShift Container Platform 4.15 supports Azure confidential virtual machines (VMs).

Confidential VMs are currently not supported on 64-bit ARM architectures.
By editing the machine set YAML file, you can configure the confidential VM options that a machine set uses for machines that it deploys. For example, you can configure these machines to use UEFI security features such as Secure Boot or a dedicated virtual Trusted Platform Module (vTPM) instance.

For more information about related features and functionality, see the Microsoft Azure documentation about Confidential virtual machines.

In a text editor, open the YAML file for an existing machine set or create a new one.

Edit the following section under the providerSpec field:


On the Azure portal, review the details for a machine deployed by the machine set and verify that the confidential VM options match the values that you configured.
Accelerated Networking for Microsoft Azure VMs
Accelerated Networking uses single root I/O virtualization (SR-IOV) to provide Microsoft Azure VMs with a more direct path to the switch. This enhances network performance. This feature can be enabled during or after installation.

Limitations
Consider the following limitations when deciding whether to use Accelerated Networking:

Accelerated Networking is only supported on clusters where the Machine API is operational.




When this feature is enabled on an existing Azure cluster, only newly provisioned nodes are affected. Currently running nodes are not reconciled. To enable the feature on all nodes, you must replace each existing machine. This can be done for each machine individually, or by scaling the replicas down to zero, and then scaling back up to your desired number of replicas.
Adding a GPU node to an existing Red Hat OpenShift Container Platform cluster
You can copy and modify a default compute machine set configuration to create a GPU-enabled machine set and machines for the Azure cloud provider.

The following table lists the validated instance types:


By default, Azure subscriptions do not have a quota for the Azure instance types with GPU. Customers have to request a quota increase for the Azure instance families listed above.
View the machines and machine sets that exist in the openshift-machine-api namespace
by running the following command. Each compute machine set is associated with a different availability zone within the Azure region.
The installer automatically load balances compute machines across availability zones.

Make a copy of one of the existing compute MachineSet definitions and output the result to a YAML file by running the following command.
This will be the basis for the GPU-enabled compute machine set definition.

View the content of the machineset:

Make a copy of the machineset-azure.yaml file by running the following command:

Update the following fields in machineset-azure-gpu.yaml:

To verify your changes, perform a diff of the original compute definition and the new GPU-enabled node definition by running the following command:

Create the GPU-enabled compute machine set from the definition file by running the following command:

View the machines and machine sets that exist in the openshift-machine-api namespace
by running the following command. Each compute machine set is associated with a
different availability zone within the Azure region.
The installer automatically load balances compute machines across availability zones.

View the machines that exist in the openshift-machine-api namespace by running the following command. You can only configure one compute machine per set, although you can scale a compute machine set to add a node in a particular region and zone.

View the existing nodes, machines, and machine sets by running the following command. Note that each node is an instance of a machine definition with a specific Azure region and Red Hat OpenShift Container Platform role.

View the list of compute machine sets:

Create the GPU-enabled compute machine set from the definition file by running the following command:

View the list of compute machine sets:


View the machine set you created by running the following command:

View the Machine object that the machine set created by running the following command:


There is no need to specify a namespace for the node. The node definition is cluster scoped.
Deploying the Node Feature Discovery Operator
After the GPU-enabled node is created, you need to discover the GPU-enabled node so it can be scheduled. To do this, install the Node Feature Discovery (NFD) Operator. The NFD Operator identifies hardware device features in nodes. It solves the general problem of identifying and cataloging hardware resources in the infrastructure nodes so they can be made available to Red Hat OpenShift Container Platform.

Install the Node Feature Discovery Operator from OperatorHub in the Red Hat OpenShift Container Platform console.

After installing the NFD Operator into OperatorHub, select Node Feature Discovery from the installed Operators list and select Create instance. This installs the nfd-master and nfd-worker pods, one nfd-worker pod for each compute node, in the openshift-nfd namespace.

Verify that the Operator is installed and running by running the following command:

Browse to the installed Oerator in the console and select Create Node Feature Discovery.

Select Create to build a NFD custom resource. This creates NFD pods in the openshift-nfd namespace that poll the Red Hat OpenShift Container Platform nodes for hardware resources and catalogue them.


After a successful build, verify that a NFD pod is running on each nodes by running the following command:

View the NVIDIA GPU discovered by the NFD Operator by running the following command:


Enabling Accelerated Networking during installation


Enabling Accelerated Networking on an existing Microsoft Azure cluster
You can enable Accelerated Networking on Azure by adding acceleratedNetworking to your machine set YAML file.

Have an existing Microsoft Azure cluster where the Machine API is operational.


Add the following to the providerSpec field:


To enable the feature on currently running nodes, you must replace each existing machine. This can be done for each machine individually, or by scaling the replicas down to zero, and then scaling back up to your desired number of replicas.


On the Microsoft Azure portal, review the Networking settings page for a machine provisioned by the machine set, and verify that the Accelerated networking field is set to Enabled.


Manually scaling a compute machine set