Troubleshooting Operator issues

If you experience Operator issues, verify Operator subscription status. Check Operator pod health across the cluster and gather Operator logs for diagnosis.
Operator subscription condition types
Subscriptions can report the following condition types:


Default Red Hat OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a Subscription object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a Subscription object.
Catalog health requirements
Viewing Operator subscription status by using the CLI
You can view Operator subscription status by using the CLI.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


List Operator subscriptions:

Use the oc describe command to inspect a Subscription resource:

In the command output, find the Conditions section for the status of Operator subscription condition types. In the following example, the CatalogSourcesUnhealthy condition type has a status of false because all available catalog sources are healthy:


Default Red Hat OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a Subscription object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a Subscription object.
Viewing Operator catalog source status by using the CLI
You can view the status of an Operator catalog source by using the CLI.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


List the catalog sources in a namespace. For example, you can check the openshift-marketplace namespace, which is used for cluster-wide catalog sources:

Use the oc describe command to get more details and status about a catalog source:

List the pods in the namespace where your catalog source was created:

Use the oc describe command to inspect a pod for more detailed information:


Operator Lifecycle Manager concepts and resources -> Catalog source

gRPC documentation: States of Connectivity

Accessing images for Operators from private registries
Querying Operator pod status
You can list Operator pods within a cluster and their status. You can also collect a detailed Operator pod summary.

You have access to the cluster as a user with the cluster-admin role.

Your API service is still functional.

You have installed the OpenShift CLI (oc).


List Operators running in the cluster. The output includes Operator version, availability, and up-time information:

List Operator pods running in the Operator's namespace, plus pod status, restarts, and age:

Output a detailed Operator pod summary:

If an Operator issue is node-specific, query Operator container status on that node.
Gathering Operator logs
If you experience Operator issues, you can gather detailed diagnostic information from Operator pod logs.

You have access to the cluster as a user with the cluster-admin role.

Your API service is still functional.

You have installed the OpenShift CLI (oc).

You have the fully qualified domain names of the control plane or control plane machines.


List the Operator pods that are running in the Operator's namespace, plus the pod status, restarts, and age:

Review logs for an Operator pod:

If the API is not functional, review Operator pod and container logs on each control plane node by using SSH instead. Replace <master-node>.<cluster_name>.<base_domain> with appropriate values.
Disabling the Machine Config Operator from automatically rebooting
When configuration changes are made by the Machine Config Operator (MCO), Red Hat Enterprise Linux CoreOS (RHCOS) must reboot for the changes to take effect. Whether the configuration change is automatic or manual, an RHCOS node reboots automatically unless it is paused.

The following modifications do not trigger a node reboot:

When the MCO detects any of the following changes, it applies the update without draining or rebooting the node:

When the MCO detects changes to the /etc/containers/registries.conf file, such as adding or editing an ImageDigestMirrorSet, ImageTagMirrorSet, or ImageContentSourcePolicy object, it drains the corresponding nodes, applies the changes, and uncordons the nodes. The node drain does not happen for the following changes:
To avoid unwanted disruptions, you can modify the machine config pool (MCP) to prevent automatic rebooting after the Operator makes changes to the machine config.

Disabling the Machine Config Operator from automatically rebooting by using the console
To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can use the Red Hat OpenShift Container Platform web console to modify the machine config pool (MCP) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.

See second NOTE in Disabling the Machine Config Operator from automatically rebooting.
You have access to the cluster as a user with the cluster-admin role.


To pause or unpause automatic MCO update rebooting:

Pause the autoreboot process:

Unpause the autoreboot process:
Disabling the Machine Config Operator from automatically rebooting by using the CLI
To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can modify the machine config pool (MCP) using the OpenShift CLI (oc) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.

See second NOTE in Disabling the Machine Config Operator from automatically rebooting.
You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


To pause or unpause automatic MCO update rebooting:

Pause the autoreboot process:

Unpause the autoreboot process:
Refreshing failing subscriptions
In Operator Lifecycle Manager (OLM), if you subscribe to an Operator that references images that are not accessible on your network, you can find jobs in the openshift-marketplace namespace that are failing with the following errors:

ImagePullBackOff for
Back-off pulling image "example.com/openshift4/ose-elasticsearch-operator-bundle@sha256:6d2587129c846ec28d384540322b40b05833e7e00b25cca584e004af9a1d292e"
rpc error: code = Unknown desc = error pinging docker registry example.com: Get "https://example.com/v2/": dial tcp: lookup example.com on 10.0.0.1:53: no such host
As a result, the subscription is stuck in this failing state and the Operator is unable to install or upgrade.

You can refresh a failing subscription by deleting the subscription, cluster service version (CSV), and other related objects. After recreating the subscription, OLM then reinstalls the correct version of the Operator.

You have a failing subscription that is unable to pull an inaccessible bundle image.

You have confirmed that the correct bundle image is accessible.


Get the names of the Subscription and ClusterServiceVersion objects from the namespace where the Operator is installed:

Delete the subscription:

Delete the cluster service version:

Get the names of any failing jobs and related config maps in the openshift-marketplace namespace:

Delete the job:

Delete the config map:

Reinstall the Operator using OperatorHub in the web console.


Check that the Operator has been reinstalled successfully:
Reinstalling Operators after failed uninstallation
You must successfully and completely uninstall an Operator prior to attempting to reinstall the same Operator. Failure to fully uninstall the Operator properly can leave resources, such as a project or namespace, stuck in a "Terminating" state and cause "error resolving resource" messages. For example:

...
    message: 'Failed to delete all resource types, 1 remaining: Internal error occurred:
      error resolving resource'
...
These types of issues can prevent an Operator from being reinstalled successfully.

Forced deletion of a namespace is not likely to resolve "Terminating" state issues and can lead to unstable or unpredictable cluster behavior, so it is better to try to find related resources that might be preventing the namespace from being deleted. For more information, see the Red Hat Knowledgebase Solution #4165791, paying careful attention to the cautions and warnings.
The following procedure shows how to troubleshoot when an Operator cannot be reinstalled because an existing custom resource definition (CRD) from a previous installation of the Operator is preventing a related namespace from deleting successfully.

Check if there are any namespaces related to the Operator that are stuck in "Terminating" state:

Check if there are any CRDs related to the Operator that are still present after the failed uninstallation:

If there are any CRDs that you know were provided or managed by the Operator and that should have been deleted after uninstallation, delete the CRD:

Check if there are any remaining CR instances related to the Operator that are still present after uninstallation, and if so, delete the CRs:

Check that the namespace deletion has successfully resolved:

Reinstall the Operator using OperatorHub in the web console.


Check that the Operator has been reinstalled successfully:


Deleting Operators from a cluster

Adding Operators to a cluster