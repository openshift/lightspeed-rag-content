Operator SDK tutorial for Go-based Operators

Operator developers can take advantage of Go programming language support in the Operator SDK to build an example Go-based Operator for Memcached, a distributed key-value store, and manage its lifecycle.

This process is accomplished using two centerpieces of the Operator Framework:


Operator SDK
The operator-sdk CLI tool and controller-runtime library API
Operator Lifecycle Manager (OLM)
Installation, upgrade, and role-based access control (RBAC) of Operators on a cluster


This tutorial goes into greater detail than Getting started with Operator SDK for Go-based Operators.
Prerequisites
Operator SDK CLI installed

OpenShift CLI (oc) 4.15+ installed

Go 1.19+

Logged into an Red Hat OpenShift Container Platform 4.15 cluster with oc with an account that has cluster-admin permissions

To allow the cluster to pull the image, the repository where you push your image must be set as public, or you must configure an image pull secret


Installing the Operator SDK CLI

Getting started with the OpenShift CLI
Creating a project
Use the Operator SDK CLI to create a project called memcached-operator.

Create a directory for the project:

Change to the directory:

Activate support for Go modules:

Run the operator-sdk init command
to initialize the project:


PROJECT file
Among the files generated by the operator-sdk init command is a Kubebuilder PROJECT file. Subsequent operator-sdk commands, as well as help output, that are run from the project root read this file and are aware that the project type is Go. For example:

domain: example.com
layout:
- go.kubebuilder.io/v3
projectName: memcached-operator
repo: github.com/example-inc/memcached-operator
version: "3"
plugins:
  manifests.sdk.operatorframework.io/v2: {}
  scorecard.sdk.operatorframework.io/v2: {}
  sdk.x-openshift.io/v1: {}
About the Manager
The main program for the Operator is the main.go file, which initializes and runs the Manager. The Manager automatically registers the Scheme for all custom resource (CR) API definitions and sets up and runs controllers and webhooks.

The Manager can restrict the namespace that all controllers watch for resources:

mgr, err := ctrl.NewManager(cfg, manager.Options{Namespace: namespace})
By default, the Manager watches the namespace where the Operator runs. To watch all namespaces, you can leave the namespace option empty:

mgr, err := ctrl.NewManager(cfg, manager.Options{Namespace: ""})
You can also use the MultiNamespacedCacheBuilder function to watch a specific set of namespaces:

var namespaces []string 1
mgr, err := ctrl.NewManager(cfg, manager.Options{ 2
   NewCache: cache.MultiNamespacedCacheBuilder(namespaces),
})
List of namespaces.

Creates a Cmd struct to provide shared dependencies and start components.
About multi-group APIs
Before you create an API and controller, consider whether your Operator requires multiple API groups. This tutorial covers the default case of a single group API, but to change the layout of your project to support multi-group APIs, you can run the following command:

$ operator-sdk edit --multigroup=true
This command updates the PROJECT file, which should look like the following example:

domain: example.com
layout: go.kubebuilder.io/v3
multigroup: true
...
For multi-group projects, the API Go type files are created in the apis/<group>/<version>/ directory, and the controllers are created in the controllers/<group>/ directory. The Dockerfile is then updated accordingly.

For more details on migrating to a multi-group project, see the Kubebuilder documentation.
Creating an API and controller
Use the Operator SDK CLI to create a custom resource definition (CRD) API and controller.

Run the following command to create an API with group cache, version, v1, and kind Memcached:

When prompted, enter y for creating both the resource and controller:


This process generates the Memcached resource API at api/v1/memcached_types.go and the controller at controllers/memcached_controller.go.

Defining the API
Define the API for the Memcached custom resource (CR).

Modify the Go type definitions at api/v1/memcached_types.go to have the following spec and status:

Update the generated code for the resource type:
Generating CRD manifests
After the API is defined with spec and status fields and custom resource definition (CRD) validation markers, you can generate CRD manifests.

Run the following command to generate and update CRD manifests:


About OpenAPI validation
OpenAPIv3 schemas are added to CRD manifests in the spec.validation block when the manifests are generated. This validation block allows Kubernetes to validate the properties in a Memcached custom resource (CR) when it is created or updated.

Markers, or annotations, are available to configure validations for your API. These markers always have a +kubebuilder:validation prefix.

For more details on the usage of markers in API code, see the following Kubebuilder documentation:

For more details about OpenAPIv3 validation schemas in CRDs, see the Kubernetes documentation.
Implementing the controller
After creating a new API and controller, you can implement the controller logic.

For this example, replace the generated controller file controllers/memcached_controller.go with following example implementation:


The next subsections explain how the controller in the example implementation watches resources and how the reconcile loop is triggered. You can skip these subsections to go directly to Running the Operator.

Resources watched by the controller
The SetupWithManager() function in controllers/memcached_controller.go specifies how the controller is built to watch a CR and other resources that are owned and managed by that controller.

import (
	...
	appsv1 "k8s.io/api/apps/v1"
	...
)

func (r *MemcachedReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&cachev1.Memcached{}).
		Owns(&appsv1.Deployment{}).
		Complete(r)
}
NewControllerManagedBy() provides a controller builder that allows various controller configurations.

For(&cachev1.Memcached{}) specifies the Memcached type as the primary resource to watch. For each Add, Update, or Delete event for a Memcached type, the reconcile loop is sent a reconcile Request argument, which consists of a namespace and name key, for that Memcached object.

Owns(&appsv1.Deployment{}) specifies the Deployment type as the secondary resource to watch. For each Deployment type Add, Update, or Delete event, the event handler maps each event to a reconcile request for the owner of the deployment. In this case, the owner is the Memcached object for which the deployment was created.
Controller configurations
You can initialize a controller by using many other useful configurations. For example:

Set the maximum number of concurrent reconciles for the controller by using the MaxConcurrentReconciles option, which defaults to 1:

Filter watch events using predicates.

Choose the type of EventHandler to change how a watch event translates to reconcile requests for the reconcile loop. For Operator relationships that are more complex than primary and secondary resources, you can use the EnqueueRequestsFromMapFunc handler to transform a watch event into an arbitrary set of reconcile requests.


For more details on these and other configurations, see the upstream Builder and Controller GoDocs.
Reconcile loop
Every controller has a reconciler object with a Reconcile() method that implements the reconcile loop. The reconcile loop is passed the Request argument, which is a namespace and name key used to find the primary resource object, Memcached, from the cache:

import (
	ctrl "sigs.k8s.io/controller-runtime"

	cachev1 "github.com/example-inc/memcached-operator/api/v1"
	...
)

func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
  // Lookup the Memcached instance for this reconcile request
  memcached := &cachev1.Memcached{}
  err := r.Get(ctx, req.NamespacedName, memcached)
  ...
}
Based on the return values, result, and error, the request might be requeued and the reconcile loop might be triggered again:

// Reconcile successful - don't requeue
return ctrl.Result{}, nil
// Reconcile failed due to error - requeue
return ctrl.Result{}, err
// Requeue for any reason other than an error
return ctrl.Result{Requeue: true}, nil
You can set the Result.RequeueAfter to requeue the request after a grace period as well:

import "time"

// Reconcile for any reason other than an error after 5 seconds
return ctrl.Result{RequeueAfter: time.Second*5}, nil
You can return Result with RequeueAfter set to periodically reconcile a CR.
For more on reconcilers, clients, and interacting with resource events, see the Controller Runtime Client API documentation.
Permissions and RBAC manifests
The controller requires certain RBAC permissions to interact with the resources it manages. These are specified using RBAC markers, such as the following:

// +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/finalizers,verbs=update
// +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;

func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
  ...
}
The ClusterRole object manifest at config/rbac/role.yaml is generated from the previous markers by using the controller-gen utility whenever the make manifests command is run.
Enabling proxy support
Operator authors can develop Operators that support network proxies. Cluster administrators configure proxy support for the environment variables that are handled by Operator Lifecycle Manager (OLM). To support proxied clusters, your Operator must inspect the environment for the following standard proxy variables and pass the values to Operands:

HTTP_PROXY

HTTPS_PROXY

NO_PROXY


This tutorial uses HTTP_PROXY as an example environment variable.
A cluster with cluster-wide egress proxy enabled.


Edit the controllers/memcached_controller.go file to include the following:

Set the environment variable on the Operator deployment by adding the following to the config/manager/manager.yaml file:
Running the Operator
There are three ways you can use the Operator SDK CLI to build and run your Operator:

Run locally outside the cluster as a Go program.

Run as a deployment on the cluster.

Bundle your Operator and use Operator Lifecycle Manager (OLM) to deploy on the cluster.


Before running your Go-based Operator as either a deployment on Red Hat OpenShift Container Platform or as a bundle that uses OLM, ensure that your project has been updated to use supported images.
Running locally outside the cluster
You can run your Operator project as a Go program outside of the cluster. This is useful for development purposes to speed up deployment and testing.

Run the following command to install the custom resource definitions (CRDs) in the cluster configured in your ~/.kube/config file and run the Operator locally:
Running as a deployment on the cluster
You can run your Operator project as a deployment on your cluster.

Prepared your Go-based Operator to run on Red Hat OpenShift Container Platform by updating the project to use supported images


Run the following make commands to build and push the Operator image. Modify the IMG argument in the following steps to reference a repository that you have access to. You can obtain an account for storing containers at repository sites such as Quay.io.

Run the following command to deploy the Operator:

Run the following command to verify that the Operator is running:
Bundling an Operator and deploying with Operator Lifecycle Manager
Bundling an Operator
The Operator bundle format is the default packaging method for Operator SDK and Operator Lifecycle Manager (OLM). You can get your Operator ready for use on OLM by using the Operator SDK to build and push your Operator project as a bundle image.

Operator SDK CLI installed on a development workstation

OpenShift CLI (oc) v4.15+ installed

Operator project initialized by using the Operator SDK

If your Operator is Go-based, your project must be updated to use supported images for running on Red Hat OpenShift Container Platform


Run the following make commands in your Operator project directory to build and push your Operator image. Modify the IMG argument in the following steps to reference a repository that you have access to. You can obtain an account for storing containers at repository sites such as Quay.io.

Create your Operator bundle manifest by running the make bundle command, which invokes several commands, including the Operator SDK generate bundle and bundle validate subcommands:

Build and push your bundle image by running the following commands. OLM consumes Operator bundles using an index image, which reference one or more bundle images.
Deploying an Operator with Operator Lifecycle Manager
Operator Lifecycle Manager (OLM) helps you to install, update, and manage the lifecycle of Operators and their associated services on a Kubernetes cluster. OLM is installed by default on Red Hat OpenShift Container Platform and runs as a Kubernetes extension so that you can use the web console and the OpenShift CLI (oc) for all Operator lifecycle management functions without any additional tools.

The Operator bundle format is the default packaging method for Operator SDK and OLM. You can use the Operator SDK to quickly run a bundle image on OLM to ensure that it runs properly.

Operator SDK CLI installed on a development workstation

Operator bundle image built and pushed to a registry

OLM installed on a Kubernetes-based cluster (v1.16.0 or later if you use apiextensions.k8s.io/v1 CRDs, for example Red Hat OpenShift Container Platform 4.15)

Logged in to the cluster with oc using an account with cluster-admin permissions

If your Operator is Go-based, your project must be updated to use supported images for running on Red Hat OpenShift Container Platform


Enter the following command to run the Operator on the cluster:
Creating a custom resource
After your Operator is installed, you can test it by creating a custom resource (CR) that is now provided on the cluster by the Operator.

Example Memcached Operator, which provides the Memcached CR, installed on a cluster


Change to the namespace where your Operator is installed. For example, if you deployed the Operator using the make deploy command:

Edit the sample Memcached CR manifest at config/samples/cache_v1_memcached.yaml to contain the following specification:

Create the CR:

Ensure that the Memcached Operator creates the deployment for the sample CR with the correct size:

Check the pods and CR status to confirm the status is updated with the Memcached pod names.

Update the deployment size.

Delete the CR by running the following command:

Clean up the resources that have been created as part of this tutorial.
Additional resources
See Project layout for Go-based Operators to learn about the directory structures created by the Operator SDK.

If a cluster-wide egress proxy is configured, cluster administrators can override the proxy settings or inject a custom CA certificate for specific Operators running on Operator Lifecycle Manager (OLM).