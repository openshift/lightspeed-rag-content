Postinstallation network configuration

After installing Red Hat OpenShift Container Platform, you can further expand and customize your network to your requirements.
Cluster Network Operator configuration
The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named cluster. The CR specifies the fields for the Network API in the operator.openshift.io API group.

The CNO configuration inherits the following fields during cluster installation from the Network API in the Network.config.openshift.io API group:


clusterNetwork
IP address pools from which pod IP addresses are allocated.
serviceNetwork
IP address pool for services.
defaultNetwork.type
Cluster network plugin. OVNKubernetes is the only supported plugin during installation.


After cluster installation, you can only modify the clusterNetwork IP address range. The default network type can only be changed from OpenShift SDN to OVN-Kubernetes through migration.
Enabling the cluster-wide proxy
The Proxy object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a Proxy object is still generated but it will have a nil spec. For example:

apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:
A cluster administrator can configure the proxy for Red Hat OpenShift Container Platform by modifying this cluster Proxy object.

Only the Proxy object named cluster is supported, and no additional proxies can be created.
Cluster administrator permissions

Red Hat OpenShift Container Platform oc CLI tool installed


Create a config map that contains any additional CA certificates required for proxying HTTPS connections.

Use the oc edit command to modify the Proxy object:

Configure the necessary fields for the proxy:

Save the file to apply the changes.
Setting DNS to private
After you deploy a cluster, you can modify its DNS to use only a private zone.

Review the DNS custom resource for your cluster:

Patch the DNS custom resource to remove the public zone:

Optional: Review the DNS custom resource for your cluster and confirm that the public zone was removed:
Configuring ingress cluster traffic
Red Hat OpenShift Container Platform provides the following methods for communicating from outside the cluster with services running in the cluster:

If you have HTTP/HTTPS, use an Ingress Controller.

If you have a TLS-encrypted protocol other than HTTPS, such as TLS with the SNI header, use an Ingress Controller.

Otherwise, use a load balancer, an external IP, or a node port.
Configuring the node port service range
As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.

The default port range is 30000-32767. You can never reduce the port range, even if you first expand it beyond the default range.

Prerequisites
Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to 30000-32900, the inclusive port range of 32768-32900 must be allowed by your firewall or packet filtering configuration.


Expanding the node port range
You can expand the node port range for the cluster.

Install the OpenShift CLI (oc).

Log in to the cluster with a user with cluster-admin privileges.


To expand the node port range, enter the following command. Replace <port> with the largest port number in the new range.

To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.
Configuring IPsec encryption
With IPsec enabled, all network traffic between nodes on the OVN-Kubernetes network plugin travels through an encrypted tunnel.

IPsec is disabled by default.

Prerequisites
Your cluster must use the OVN-Kubernetes network plugin.


Enabling IPsec encryption
As a cluster administrator, you can enable pod-to-pod IPsec encryption and IPsec encryption between the cluster and external IPsec endpoints.

You can configure IPsec in either of the following modes:

Full: Encryption for pod-to-pod and external traffic

External: Encryption for external traffic


If you need to configure encryption for external traffic in addition to pod-to-pod traffic, you must also complete the "Configuring IPsec encryption for external traffic" procedure.

Install the OpenShift CLI (`oc`).

You are logged in to the cluster as a user with cluster-admin privileges.

You have reduced the size of your cluster MTU by 46 bytes to allow for the overhead of the IPsec ESP header.


To enable IPsec encryption, enter the following command:

Optional: If you need to encrypt traffic to external hosts, complete the "Configuring IPsec encryption for external traffic" procedure.


To find the names of the OVN-Kubernetes data plane pods, enter the following command:

Verify that IPsec is enabled on your cluster by running the following command:
Configuring network policy
As a cluster administrator or project administrator, you can configure network policies for a project.

About network policy
In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by NetworkPolicy objects. In Red Hat OpenShift Container Platform 4.15, OpenShift SDN supports using network policy in its default network isolation mode.

Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.

Network policies cannot block traffic from localhost or from their resident nodes.
By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create NetworkPolicy objects in that project to indicate the allowed incoming connections. Project administrators can create and delete NetworkPolicy objects within their own project.

If a pod is matched by selectors in one or more NetworkPolicy objects, then the pod will accept only connections that are allowed by at least one of those NetworkPolicy objects. A pod that is not selected by any NetworkPolicy objects is fully accessible.

A network policy applies to only the TCP, UDP, ICMP, and SCTP protocols. Other protocols are not affected.

The following example NetworkPolicy objects demonstrate supporting different scenarios:

Deny all traffic:

Only allow connections from the Red Hat OpenShift Container Platform Ingress Controller:

Only accept connections from pods within a project:

Only allow HTTP and HTTPS traffic based on pod labels:

Accept connections by using both namespace and pod selectors:


NetworkPolicy objects are additive, which means you can combine multiple NetworkPolicy objects together to satisfy complex network requirements.

For example, for the NetworkPolicy objects defined in previous samples, you can define both allow-same-namespace and allow-http-and-https policies within the same project. Thus allowing the pods with the label role=frontend, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports 80 and 443 from pods in any namespace.

Using the allow-from-router network policy
Use the following NetworkPolicy to allow external traffic regardless of the router configuration:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""1
  podSelector: {}
  policyTypes:
  - Ingress
policy-group.network.openshift.io/ingress:"" label supports both OpenShift-SDN and OVN-Kubernetes.
Using the allow-from-hostnetwork network policy
Add the following allow-from-hostnetwork NetworkPolicy object to direct traffic from the host network pods:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress
Example NetworkPolicy object
The following annotates an example NetworkPolicy object:

kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 1
spec:
  podSelector: 2
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: 3
        matchLabels:
          app: app
    ports: 4
    - protocol: TCP
      port: 27017
The name of the NetworkPolicy object.

A selector that describes the pods to which the policy applies.
The policy object can only select pods in the project that defines the NetworkPolicy object.

A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.

A list of one or more destination ports on which to accept traffic.
Creating a network policy using the CLI
To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a network policy.

If you log in with a user with the cluster-admin role, then you can create a network policy in any namespace in the cluster.
Your cluster uses a network plugin that supports NetworkPolicy objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with mode: NetworkPolicy set. This mode is the default for OpenShift SDN.

You installed the OpenShift CLI (oc).

You are logged in to the cluster with a user with admin privileges.

You are working in the namespace that the network policy applies to.


Create a policy rule:

To create the network policy object, enter the following command:


If you log in to the web console with cluster-admin privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
Configuring multitenant isolation by using network policy
You can configure your project to isolate it from pods and services in other project namespaces.

Your cluster uses a network plugin that supports NetworkPolicy objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with mode: NetworkPolicy set. This mode is the default for OpenShift SDN.

You installed the OpenShift CLI (oc).

You are logged in to the cluster with a user with admin privileges.


Create the following NetworkPolicy objects:

Optional: To confirm that the network policies exist in your current project, enter the following command:
Supported configurations
The following configurations are supported for the current release of Red Hat OpenShift Service Mesh.

Supported platforms
The Red Hat OpenShift Service Mesh Operator supports multiple versions of the ServiceMeshControlPlane resource. Version 2.4 Service Mesh control planes are supported on the following platform versions:

Red Hat Red Hat OpenShift Container Platform version 4.10 or later

Red Hat OpenShift Dedicated version 4

Azure Red Hat OpenShift (ARO) version 4

Red Hat OpenShift Service on AWS (ROSA)
Unsupported configurations
Explicitly unsupported cases include:

OpenShift Online is not supported for Red Hat OpenShift Service Mesh.

Red Hat OpenShift Service Mesh does not support the management of microservices outside the cluster where Service Mesh is running.
Supported network configurations
Red Hat OpenShift Service Mesh supports the following network configurations.

OpenShift-SDN

OVN-Kubernetes is available on all supported versions of Red Hat OpenShift Container Platform.

Third-Party Container Network Interface (CNI) plugins that have been certified on Red Hat OpenShift Container Platform and passed Service Mesh conformance testing. See Certified OpenShift CNI Plug-ins for more information.
Supported configurations for Service Mesh
This release of Red Hat OpenShift Service Mesh is only available on Red Hat OpenShift Container Platform x86_64, IBM Z(R), and IBM Power(R).

Configurations where all Service Mesh components are contained within a single Red Hat OpenShift Container Platform cluster.

Configurations that do not integrate external services such as virtual machines.

Red Hat OpenShift Service Mesh does not support EnvoyFilter configuration except where explicitly documented.
Supported configurations for Kiali
The Kiali console is only supported on the two most recent releases of the Google Chrome, Microsoft Edge, Mozilla Firefox, or Apple Safari browsers.

The openshift authentication strategy is the only supported authentication configuration when Kiali is deployed with Red Hat OpenShift Service Mesh (OSSM). The openshift strategy controls access based on the individual's role-based access control (RBAC) roles of the Red Hat OpenShift Container Platform.
Supported configurations for Distributed Tracing
Jaeger agent as a sidecar is the only supported configuration for Jaeger. Jaeger as a daemonset is not supported for multitenant installations or OpenShift Dedicated.
Supported WebAssembly module
3scale WebAssembly is the only provided WebAssembly module. You can create custom WebAssembly modules.
Operator overview
Red Hat OpenShift Service Mesh requires the following Operators:

OpenShift Elasticsearch - (Optional) Provides database storage for tracing and logging with the distributed tracing platform (Jaeger). It is based on the open source Elasticsearch project.

Red Hat OpenShift distributed tracing platform (Jaeger) - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source Jaeger project.

Kiali Operator (provided by Red Hat) - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source Kiali project.

Red Hat OpenShift Service Mesh - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The Service Mesh Operator defines and monitors the ServiceMeshControlPlane resources that manage the deployment, updating, and deletion of the Service Mesh components. It is based on the open source Istio project.


Install Red Hat OpenShift Service Mesh in your Red Hat OpenShift Container Platform environment.
Optimizing routing
The Red Hat OpenShift Container Platform HAProxy router can be scaled or configured to optimize performance.

Baseline Ingress Controller (router) performance
The Red Hat OpenShift Container Platform Ingress Controller, or router, is the ingress point for ingress traffic for applications and services that are configured using routes and ingresses.

When evaluating a single HAProxy router performance in terms of HTTP requests handled per second, the performance varies depending on many factors. In particular:

HTTP keep-alive/close mode

Route type

TLS session resumption client support

Number of concurrent connections per target route

Number of target routes

Back end server page size

Underlying infrastructure (network/SDN solution, CPU, and so on)


While performance in your specific environment will vary, Red Hat lab tests on a public cloud instance of size 4 vCPU/16GB RAM. A single HAProxy router handling 100 routes terminated by backends serving 1kB static pages is able to handle the following number of transactions per second.

In HTTP keep-alive mode scenarios:


In HTTP close (no keep-alive) scenarios:


The default Ingress Controller configuration was used with the spec.tuningOptions.threadCount field set to 4. Two different endpoint publishing strategies were tested: Load Balancer Service and Host Network. TLS session resumption was used for encrypted routes. With HTTP keep-alive, a single HAProxy router is capable of saturating a 1 Gbit NIC at page sizes as small as 8 kB.

When running on bare metal with modern processors, you can expect roughly twice the performance of the public cloud instance above. This overhead is introduced by the virtualization layer in place on public clouds and holds mostly true for private cloud-based virtualization as well. The following table is a guide to how many applications to use behind the router:


In general, HAProxy can support routes for up to 1000 applications, depending on the technology in use. Ingress Controller performance might be limited by the capabilities and performance of the applications behind it, such as language or static versus dynamic content.

Ingress, or router, sharding should be used to serve more routes towards applications and help horizontally scale the routing tier.
Configuring Ingress Controller liveness, readiness, and startup probes
Cluster administrators can configure the timeout values for the kubelet's liveness, readiness, and startup probes for router deployments that are managed by the Red Hat OpenShift Container Platform Ingress Controller (router). The liveness and readiness probes of the router use the default timeout value of 1 second, which is too brief when networking or runtime performance is severely degraded. Probe timeouts can cause unwanted router restarts that interrupt application connections. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts.

You can update the timeoutSeconds value on the livenessProbe, readinessProbe, and startupProbe parameters of the router container.


The timeout configuration option is an advanced tuning technique that can be used to work around issues. However, these issues should eventually be diagnosed and possibly a support case or Jira issue opened for any issues that causes probes to time out.
The following example demonstrates how you can directly patch the default router deployment to set a 5-second timeout for the liveness and readiness probes:

$ oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","livenessProbe":{"timeoutSeconds":5},"readinessProbe":{"timeoutSeconds":5}}]}}}}'
$ oc -n openshift-ingress describe deploy/router-default | grep -e Liveness: -e Readiness:
    Liveness:   http-get http://:1936/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:1936/healthz/ready delay=0s timeout=5s period=10s #success=1 #failure=3
Configuring HAProxy reload interval
When you update a route or an endpoint associated with a route, Red Hat OpenShift Container Platform router updates the configuration for HAProxy. Then, HAProxy reloads the updated configuration for those changes to take effect. When HAProxy reloads, it generates a new process that handles new connections using the updated configuration.

HAProxy keeps the old process running to handle existing connections until those connections are all closed. When old processes have long-lived connections, these processes can accumulate and consume resources.

The default minimum HAProxy reload interval is five seconds. You can configure an Ingress Controller using its spec.tuningOptions.reloadInterval field to set a longer minimum reload interval.

Setting a large value for the minimum HAProxy reload interval can cause latency in observing updates to routes and their endpoints. To lessen the risk, avoid setting a value larger than the tolerable latency for updates.
Change the minimum HAProxy reload interval of the default Ingress Controller to 15 seconds by running the following command:
Postinstallation OpenStack network configuration
You can configure some aspects of an Red Hat OpenShift Container Platform on OpenStack cluster after installation.

Configuring application access with floating IP addresses
After you install Red Hat OpenShift Container Platform, configure OpenStack to allow application network traffic.

You do not need to perform this procedure if you provided values for platform.openstack.apiFloatingIP and platform.openstack.ingressFloatingIP in the install-config.yaml file, or os_api_fip and os_ingress_fip in the inventory.yaml playbook, during installation. The floating IP addresses are already set.
Red Hat OpenShift Container Platform cluster must be installed

Floating IP addresses are enabled as described in the Red Hat OpenShift Container Platform on OpenStack installation documentation.


After you install the Red Hat OpenShift Container Platform cluster, attach a floating IP address to the ingress port:

Show the port:

Attach the port to the IP address:

Add a wildcard A record for *apps. to your DNS file:


If you do not control the DNS server but want to enable application access for non-production purposes, you can add these hostnames to /etc/hosts:

<apps_FIP> console-openshift-console.apps.<cluster name>.<base domain>
<apps_FIP> integrated-oauth-server-openshift-authentication.apps.<cluster name>.<base domain>
<apps_FIP> oauth-openshift.apps.<cluster name>.<base domain>
<apps_FIP> prometheus-k8s-openshift-monitoring.apps.<cluster name>.<base domain>
<apps_FIP> <app name>.apps.<cluster name>.<base domain>
Enabling OVS hardware offloading
For clusters that run on OpenStack, you can enable Open vSwitch (OVS) hardware offloading.

OVS is a multi-layer virtual switch that enables large-scale, multi-server network virtualization.

You installed a cluster on OpenStack that is configured for single-root input/output virtualization (SR-IOV).

You installed the SR-IOV Network Operator on your cluster.

You created two hw-offload type virtual function (VF) interfaces on your cluster.


Application layer gateway flows are broken in Red Hat OpenShift Container Platform version 4.10, 4.11, and 4.12. Also, you cannot offload the application layer gateway flow for Red Hat OpenShift Container Platform version 4.13.
Create an SriovNetworkNodePolicy policy for the two hw-offload type VF interfaces that are on your cluster:

Create NetworkAttachmentDefinition resources for the two interfaces:

Use the interfaces that you created with a pod. For example:
Attaching an OVS hardware offloading network
You can attach an Open vSwitch (OVS) hardware offloading network to your cluster.

Your cluster is installed and running.

You provisioned an OVS hardware offloading network on OpenStack to use with your cluster.


Create a file named network.yaml from the following template:

From a command line, enter the following command to patch your cluster with the file:
Enabling IPv6 connectivity to pods on OpenStack
To enable IPv6 connectivity between pods that have additional networks that are on different nodes, disable port security for the IPv6 port of the server. Disabling port security obviates the need to create allowed address pairs for each IPv6 address that is assigned to pods and enables traffic on the security group.

Only the following IPv6 additional network configurations are supported:

SLAAC and host-device

SLAAC and MACVLAN

DHCP stateless and host-device

DHCP stateless and MACVLAN
On a command line, enter the following command:


where:


<compute_ipv6_port>
Specifies the IPv6 port of the compute server.
Adding IPv6 connectivity to pods on OpenStack
After you enable IPv6 connectivity in pods, add connectivity to them by using a Container Network Interface (CNI) configuration.

To edit the Cluster Network Operator (CNO), enter the following command:

Specify your CNI configuration under the spec field. For example, the following configuration uses a SLAAC address mode with MACVLAN:

Save your changes and quit the text editor to commit your changes.


On a command line, enter the following command:


You can now create pods that have secondary IPv6 connections.

Configuration for an additional network attachment
Create pods that have IPv6 connectivity on OpenStack
After you enable IPv6 connectivty for pods and add it to them, create pods that have secondary IPv6 connections.

Define pods that use your IPv6 namespace and the annotation k8s.v1.cni.cncf.io/networks: <additional_network_name>, where <additional_network_name is the name of the additional network. For example, as part of a Deployment object:

Create the pod. For example, on a command line, enter the following command:


where:


<ipv6_enabled_resource>
Specifies the file that contains your resource definition.