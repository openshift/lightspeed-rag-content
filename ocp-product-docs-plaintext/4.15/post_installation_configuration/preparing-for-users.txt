Preparing for users

After installing Red Hat OpenShift Container Platform, you can further expand and customize your cluster to your requirements, including taking steps to prepare for users.
Understanding identity provider configuration
The Red Hat OpenShift Container Platform control plane includes a built-in OAuth server. Developers and administrators obtain OAuth access tokens to authenticate themselves to the API.

As an administrator, you can configure OAuth to specify an identity provider after you install your cluster.

About identity providers in Red Hat OpenShift Container Platform
By default, only a kubeadmin user exists on your cluster. To specify an identity provider, you must create a custom resource (CR) that describes that identity provider and add it to the cluster.

Red Hat OpenShift Container Platform user names containing /, :, and % are not supported.
Supported identity providers
You can configure the following types of identity providers:


After you define an identity provider, you can use RBAC to define and apply permissions.
Identity provider parameters
The following parameters are common to all identity providers:


When adding or changing identity providers, you can map identities from the new
provider to existing users by setting the mappingMethod parameter to
add.
Sample identity provider CR
The following custom resource (CR) shows the parameters and default values that you use to configure an identity provider. This example uses the htpasswd identity provider.

apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_identity_provider 1
    mappingMethod: claim 2
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret 3
This provider name is prefixed to provider user names to form an
identity name.

Controls how mappings are established between this provider's
identities and User objects.

An existing secret containing a file generated using
htpasswd.
Using RBAC to define and apply permissions
Understand and apply role-based access control.

RBAC overview
Role-based access control (RBAC) objects determine whether a user is allowed to perform a given action within a project.

can use the cluster roles and bindings to control who has various access levels to the Red Hat OpenShift Container Platform platform itself and all projects.

Developers can use local roles and bindings to control who has access to their projects. Note that authorization is a separate step from authentication, which is more about determining the identity of who is taking the action.

Authorization is managed using:


Be mindful of the difference between local and cluster bindings. For example, if you bind the cluster-admin role to a user by using a local role binding, it might appear that this user has the privileges of a cluster administrator. This is not the case. Binding the cluster-admin to a user in a project grants super administrator privileges for only that project to the user. That user has the permissions of the cluster role admin, plus a few additional permissions like the ability to edit rate limits, for that project. This binding can be confusing via the web console UI, which does not list cluster role bindings that are bound to true cluster administrators. However, it does list local role bindings that you can use to locally bind cluster-admin.

The relationships between cluster roles, local roles, cluster role bindings, local role bindings, users, groups and service accounts are illustrated below.


The get pods/exec, get pods/*, and get * rules grant execution privileges when they are applied to a role. Apply the principle of least privilege and assign only the minimal RBAC rights required for users and agents. For more information, see RBAC rules allow execution privileges.
Evaluating authorization
Red Hat OpenShift Container Platform evaluates authorization by using:


Identity
The user name and list of groups that the user belongs to.
Action
The action you perform. In most cases, this consists of:
Bindings
The full list of bindings, the associations between users or groups
with a role.


Red Hat OpenShift Container Platform evaluates authorization by using the following steps:

The identity and the project-scoped action is used to find all bindings that
apply to the user or their groups.

Bindings are used to locate all the roles that apply.

Roles are used to find all the rules that apply.

The action is checked against each rule to find a match.

If no matching rule is found, the action is then denied by default.


including a matrix of the verbs and resources each are associated with.
Projects and namespaces
A Kubernetes namespace provides a mechanism to scope resources in a cluster. The Kubernetes documentation has more information on namespaces.

Namespaces provide a unique scope for:

Named resources to avoid basic naming collisions.

Delegated management authority to trusted users.

The ability to limit community resource consumption.


Most objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.

A project is a Kubernetes namespace with additional annotations and is the central vehicle by which access to resources for regular users is managed. A project allows a community of users to organize and manage their content in isolation from other communities. Users must be given access to projects by administrators, or if allowed to create projects, automatically have access to their own projects.

Projects can have a separate name, displayName, and description.

The mandatory name is a unique identifier for the project and is most visible when using the CLI tools or API. The maximum name length is 63 characters.

The optional displayName is how the project is displayed in the web console (defaults to name).

The optional description can be a more detailed description of the project and is also visible in the web console.


Each project scopes its own set of:


Cluster administrators can create projects and delegate administrative rights for the project to any member of the user community. Cluster administrators can also allow developers to create their own projects.

Developers and administrators can interact with projects by using the CLI or the web console.
Default projects
Red Hat OpenShift Container Platform comes with a number of default projects, and projects starting with openshift- are the most essential to users. These projects host master components that run as pods and other infrastructure components. The pods created in these namespaces that have a critical pod annotation are considered critical, and the have guaranteed admission by kubelet. Pods created for master components in these namespaces are already marked as critical.

Do not run workloads in or share access to default projects. Default projects are reserved for running core cluster components.

The following default projects are considered highly privileged: default, kube-public, kube-system, openshift, openshift-infra, openshift-node, and other system-created projects that have the openshift.io/run-level label set to 0 or 1. Functionality that relies on admission plugins, such as pod security admission, security context constraints, cluster resource quotas, and image reference resolution, does not work in highly privileged projects.
Viewing cluster roles and bindings
You can use the oc CLI to view cluster roles and bindings by using the oc describe command.

Install the oc CLI.

Obtain permission to view the cluster roles and bindings.


To view the cluster roles and their associated rule sets:

To view the current set of cluster role bindings, which shows the users and
groups that are bound to various roles:
Viewing local roles and bindings
You can use the oc CLI to view local roles and bindings by using the oc describe command.

Install the oc CLI.

Obtain permission to view the local roles and bindings:


To view the current set of local role bindings, which show the users and groups
that are bound to various roles for the current project:

To view the local role bindings for a different project, add the -n flag
to the command:
Adding roles to users
You can use  the oc adm administrator CLI to manage the roles and bindings.

Binding, or adding, a role to users or groups gives the user or group the access that is granted by the role. You can add and remove roles to and from users and groups using oc adm policy commands.

You can bind any of the default cluster roles to local users or groups in your project.

Add a role to a user in a specific project:

View the local role bindings and verify the addition in the output:
Local role binding commands
When you manage a user or group's associated roles for local role bindings using the following operations, a project may be specified with the -n flag. If it is not specified, then the current project is used.

You can use the following commands for local RBAC management.
The kubeadmin user
Red Hat OpenShift Container Platform creates a cluster administrator, kubeadmin, after the installation process completes.

This user has the cluster-admin role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your Red Hat OpenShift Container Platform environment. After installation completes the password is provided in the installation program's output. For example:

INFO Install complete!
INFO Run 'export KUBECONFIG=<your working directory>/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.
INFO The cluster is ready when 'oc login -u kubeadmin -p <provided>' succeeds (wait a few minutes).
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com
INFO Login to the console with user: kubeadmin, password: <provided>
Removing the kubeadmin user
After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security.

If you follow this procedure before another user is a cluster-admin, then Red Hat OpenShift Container Platform must be reinstalled. It is not possible to undo this command.
You must have configured at least one identity provider.

You must have added the cluster-admin role to a user.

You must be logged in as an administrator.


Remove the kubeadmin secrets:
Image configuration
Understand and configure image registry settings.

Image controller configuration parameters
The image.config.openshift.io/cluster resource holds cluster-wide information about how to handle images. The canonical, and only valid name is cluster. Its spec offers the following configuration parameters.

Parameters such as DisableScheduledImport, MaxImagesBulkImportedPerRepository, MaxScheduledImportsPerMinute, ScheduledImageImportMinimumIntervalSeconds, InternalRegistryHostname are not configurable.

When the allowedRegistries parameter is defined, all registries, including registry.redhat.io and quay.io registries and the default OpenShift image registry, are blocked unless explicitly listed. When using the parameter, to prevent pod failure, add all registries including the registry.redhat.io and quay.io registries and the internalRegistryHostname to the allowedRegistries list, as they are required by payload images within your environment. For disconnected clusters, mirror registries should also be added.
The status field of the image.config.openshift.io/cluster resource holds observed values from the cluster.
Configuring image registry settings
You can configure image registry settings by editing the image.config.openshift.io/cluster custom resource (CR). When changes to the registry are applied to the image.config.openshift.io/cluster CR, the Machine Config Operator (MCO) performs the following sequential actions:

Cordons the node

Applies changes by restarting CRI-O

Uncordons the node


Edit the image.config.openshift.io/cluster custom resource:

To check that the changes are applied, list your nodes:


For more information on the allowed, blocked, and insecure registry parameters, see Configuring image registry settings.
Configuring additional trust stores for image registry access
The image.config.openshift.io/cluster custom resource can contain a reference to a config map that contains additional certificate authorities to be trusted during image registry access.

The certificate authorities (CA) must be PEM-encoded.


You can create a config map in the openshift-config namespace and use its name in AdditionalTrustedCA in the image.config.openshift.io custom resource to provide additional CAs that should be trusted when contacting external registries.

The config map key is the hostname of a registry with the port for which this CA is to be trusted, and the PEM certificate content is the value, for each additional registry CA to trust.

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-registry-ca
data:
  registry.example.com: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
  registry-with-port.example.com..5000: | 1
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
If the registry has the port, such as registry-with-port.example.com:5000, : should be replaced with &#8230;&#8203;


You can configure additional CAs with the following procedure.

To configure an additional CA:
Understanding image registry repository mirroring
Setting up container registry repository mirroring enables you to perform the following tasks:

Configure your Red Hat OpenShift Container Platform cluster to redirect requests to pull images from a repository on a source image registry and have it resolved by a repository on a mirrored image registry.

Identify multiple mirrored repositories for each target repository, to make sure that if one mirror is down, another can be used.


Repository mirroring in Red Hat OpenShift Container Platform includes the following attributes:

Image pulls are resilient to registry downtimes.

Clusters in disconnected environments can pull images from critical locations, such as quay.io, and have registries behind a company firewall provide the requested images.

A particular order of registries is tried when an image pull request is made, with the permanent registry typically being the last one tried.

The mirror information you enter is added to the /etc/containers/registries.conf file on every node in the Red Hat OpenShift Container Platform cluster.

When a node makes a request for an image from the source repository, it tries each mirrored repository in turn until it finds the requested content. If all mirrors fail, the cluster tries the source repository. If successful, the image is pulled to the node.


Setting up repository mirroring can be done in the following ways:

At Red Hat OpenShift Container Platform installation:

After Red Hat OpenShift Container Platform installation:


Each of these custom resource objects identify the following information:

The source of the container image repository you want to mirror.

A separate entry for each mirror repository you want to offer the content
requested from the source repository.
For new clusters, you can use IDMS, ITMS, and ICSP CRs objects as desired. However, using IDMS and ITMS is recommended.

If you upgraded a cluster, any existing ICSP objects remain stable, and both IDMS and ICSP objects are supported. Workloads using ICSP objects continue to function as expected. However, if you want to take advantage of the fallback policies introduced in the IDMS CRs, you can migrate current workloads to IDMS objects by using the oc adm migrate icsp command as shown in the Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring section that follows. Migrating to IDMS objects does not require a cluster reboot.

If your cluster uses an ImageDigestMirrorSet, ImageTagMirrorSet, or ImageContentSourcePolicy object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.
Configuring image registry repository mirroring
You can create postinstallation mirror configuration custom resources (CR) to redirect image pull requests from a source image registry to a mirrored image registry.

Access to the cluster as a user with the cluster-admin role.


Configure mirrored repositories, by either:

Log in to your Red Hat OpenShift Container Platform cluster.

Create a postinstallation mirror configuration CR, by using one of the following examples:

Create the new object:

To check that the mirrored configuration settings are applied, do the following on one of the nodes.


If the repository mirroring procedure does not work as described, use the following information about how repository mirroring works to help troubleshoot the problem.

The first working mirror is used to supply the pulled image.

The main registry is only used if no other mirror works.

From the system context, the Insecure flags are used as fallback.

The format of the /etc/containers/registries.conf file has changed recently. It is now version 2 and in TOML format.
Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring
Using an ImageContentSourcePolicy (ICSP) object to configure repository mirroring is a deprecated feature. This functionality is still included in Red Hat OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

ICSP objects are being replaced by ImageDigestMirrorSet and ImageTagMirrorSet objects to configure repository mirroring. If you have existing YAML files that you used to create ImageContentSourcePolicy objects, you can use the oc adm migrate icsp command to convert those files to an ImageDigestMirrorSet YAML file. The command updates the API to the current version, changes the kind value to ImageDigestMirrorSet, and changes spec.repositoryDigestMirrors to spec.imageDigestMirrors. The rest of the file is not changed.

Because the migration does not change the registries.conf file, the cluster does not need to reboot.

For more information about ImageDigestMirrorSet or ImageTagMirrorSet objects, see "Configuring image registry repository mirroring" in the previous section.

Access to the cluster as a user with the cluster-admin role.

Ensure that you have ImageContentSourcePolicy objects on your cluster.


Use the following command to convert one or more ImageContentSourcePolicy YAML files to an ImageDigestMirrorSet YAML file:

Create the CR object by running the following command:

Remove the ICSP objects after the IDMS objects are rolled out.
Populating OperatorHub from mirrored Operator catalogs
If you mirrored Operator catalogs for use with disconnected clusters, you can populate OperatorHub with the Operators from your mirrored catalogs. You can use the generated manifests from the mirroring process to create the required ImageContentSourcePolicy and CatalogSource objects.

Prerequisites
Mirroring Operator catalogs for use with disconnected clusters
Creating the ImageContentSourcePolicy object
After mirroring Operator catalog content to your mirror registry, create the required ImageContentSourcePolicy (ICSP) object. The ICSP object configures nodes to translate between the image references stored in Operator manifests and the mirrored registry.

On a host with access to the disconnected cluster, create the ICSP by running the following command to specify the imageContentSourcePolicy.yaml file in your manifests directory:
Adding a catalog source to a cluster
Adding a catalog source to an Red Hat OpenShift Container Platform cluster enables the discovery and installation of Operators for users. Cluster administrators can create a CatalogSource object that references an index image. OperatorHub uses catalog sources to populate the user interface.

Alternatively, you can use the web console to manage catalog sources. From the Administration -> Cluster Settings -> Configuration -> OperatorHub page, click the Sources tab, where you can create, update, delete, disable, and enable individual sources.
You built and pushed an index image to a registry.

You have access to the cluster as a user with the cluster-admin role.


Create a CatalogSource object that references your index image.
If you used the oc adm catalog mirror command to mirror your catalog to a target registry, you can use the generated catalogSource.yaml file in your manifests directory as a starting point.

Verify the following resources are created successfully.


You can now install the Operators from the OperatorHub page on your Red Hat OpenShift Container Platform web console.

Accessing images for Operators from private registries

Image template for custom catalog sources

Image pull policy
About Operator installation with OperatorHub
OperatorHub is a user interface for discovering Operators; it works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.

As a cluster administrator, you can install an Operator from OperatorHub by using the Red Hat OpenShift Container Platform

During installation, you must determine the following initial settings for the Operator:


Update Channel
If an Operator is available through multiple channels, you can choose which channel you want to subscribe to. For example, to deploy from the stable channel, if available, select it from the list.
Approval Strategy
You can choose automatic or manual updates.


Installing from OperatorHub using the web console
You can install and subscribe to an Operator from OperatorHub by using the Red Hat OpenShift Container Platform web console.

Access to an Red Hat OpenShift Container Platform cluster using an account with


Navigate in the web console to the Operators → OperatorHub page.

Scroll or type a keyword into the Filter by keyword box to find the Operator you want. For example, type jaeger to find the Jaeger Operator.

Select the Operator to display additional information.

Read the information about the Operator and click Install.

On the Install Operator page:

Click Install to make the Operator available to the selected namespaces on this Red Hat OpenShift Container Platform cluster.

After the upgrade status of the subscription is Up to date, select Operators → Installed Operators to verify that the cluster service version (CSV) of the installed Operator eventually shows up. The Status should ultimately resolve to InstallSucceeded in the relevant namespace.