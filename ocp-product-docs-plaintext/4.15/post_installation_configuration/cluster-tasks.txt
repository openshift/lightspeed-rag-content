Postinstallation cluster tasks

After installing "Red Hat OpenShift Container Platform", you can further expand and customize your cluster to your requirements.
Available cluster customizations
You complete most of the cluster configuration and customization after you deploy your "Red Hat OpenShift Container Platform" cluster. A number of configuration resources are available.

If you install your cluster on IBM Z&#174;, not all features and functions are available.
You modify the configuration resources to configure the major features of the cluster, such as the image registry, networking configuration, image build behavior, and the identity provider.

For current documentation of the settings that you control by using these resources, use the oc explain command, for example oc explain builds --api-version=config.openshift.io/v1

Cluster configuration resources
All cluster configuration resources are globally scoped (not namespaced) and named cluster.
Operator configuration resources
These configuration resources are cluster-scoped instances, named cluster, which control the behavior of a specific component as owned by a particular Operator.
Additional configuration resources
These configuration resources represent a single instance of a particular component. In some cases, you can request multiple instances by creating multiple instances of the resource. In other cases, the Operator can use only a specific resource instance name in a specific namespace. Reference the component-specific documentation for details on how and when you can create additional resource instances.
Informational Resources
You use these resources to retrieve information about the cluster. Some configurations might require you to edit these resources directly.
Updating the global cluster pull secret
You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.

The procedure is required when users use a separate registry to store images than the registry used during installation.

You have access to the cluster as a user with the cluster-admin role.


Optional: To append a new pull secret to the existing pull secret, complete the following steps:

Enter the following command to update the global pull secret for your cluster:
Adding worker nodes
After you deploy your "Red Hat OpenShift Container Platform" cluster, you can add worker nodes to scale cluster resources. There are different ways you can add worker nodes depending on the installation method and the environment of your cluster.

Adding worker nodes to installer-provisioned infrastructure clusters
For installer-provisioned infrastructure clusters, you can manually or automatically scale the MachineSet object to match the number of available bare-metal hosts.

To add a bare-metal host, you must configure all network prerequisites, configure an associated baremetalhost object, then provision the worker node to the cluster. You can add a bare-metal host manually or by using the web console.

Adding worker nodes using the web console

Adding worker nodes using YAML in the web console

Manually adding a worker node to an installer-provisioned infrastructure cluster
Adding worker nodes to user-provisioned infrastructure clusters
For user-provisioned infrastructure clusters, you can add worker nodes by using a RHEL or RHCOS ISO image and connecting it to your cluster using cluster Ignition config files. For RHEL worker nodes, the following example uses Ansible playbooks to add worker nodes to the cluster. For RHCOS worker nodes, the following example uses an ISO image and network booting to add worker nodes to the cluster.

Adding RHCOS worker nodes to a user-provisioned infrastructure cluster

Adding RHEL worker nodes to a user-provisioned infrastructure cluster
Adding worker nodes to clusters managed by the Assisted Installer
For clusters managed by the Assisted Installer, you can add worker nodes by using the Red Hat OpenShift Cluster Manager console, the Assisted Installer REST API or you can manually add worker nodes using an ISO image and cluster Ignition config files.

Adding worker nodes using the OpenShift Cluster Manager

Adding worker nodes using the Assisted Installer REST API

Manually adding worker nodes to a SNO cluster
Adding worker nodes to clusters managed by the multicluster engine for Kubernetes
For clusters managed by the multicluster engine for Kubernetes, you can add worker nodes by using the dedicated multicluster engine console.

Creating your cluster with the console
Adjust worker nodes
If you incorrectly sized the worker nodes during deployment, adjust them by creating one or more new compute machine sets, scale them up, then scale the original compute machine set down before removing them.

Understanding the difference between compute machine sets and the machine config pool
MachineSet objects describe "Red Hat OpenShift Container Platform" nodes with respect to the cloud or machine provider.

The MachineConfigPool object allows MachineConfigController components to define and provide the status of machines in the context of upgrades.

The MachineConfigPool object allows users to configure how upgrades are rolled out to the "Red Hat OpenShift Container Platform" nodes in the machine config pool.

The NodeSelector object can be replaced with a reference to the MachineSet object.
Scaling a compute machine set manually
To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.

This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.

Install an "Red Hat OpenShift Container Platform" cluster and the oc command line.

Log in to  oc as a user with cluster-admin permission.


View the compute machine sets that are in the cluster by running the following command:

View the compute machines that are in the cluster by running the following command:

Set the annotation on the compute machine that you want to delete by running the following command:

Scale the compute machine set by running one of the following commands:


Verify the deletion of the intended machine by running the following command:
The compute machine set deletion policy
Random, Newest, and Oldest are the three supported deletion options. The default is Random, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:

spec:
  deletePolicy: <delete_policy>
  replicas: <desired_replica_count>
Specific machines can also be prioritized for deletion by adding the annotation machine.openshift.io/delete-machine=true to the machine of interest, regardless of the deletion policy.

By default, the "Red Hat OpenShift Container Platform" router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to 0 unless you first relocate the router pods.
Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.
Creating default cluster-wide node selectors
You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.

With cluster-wide node selectors, when you create a pod in that cluster, "Red Hat OpenShift Container Platform" adds the default node selectors to the pod and schedules the pod on nodes with matching labels.

You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.

You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.
To add a default cluster-wide node selector:

Edit the Scheduler Operator CR to add the default cluster-wide node selectors:

Add labels to a node by using a compute machine set or editing the node directly:
Improving cluster stability in high latency environments using worker latency profiles
If the cluster administrator has performed latency tests for platform verification, they can discover the need to adjust the operation of the cluster to ensure stability in cases of high latency. The cluster administrator need change only one parameter, recorded in a file, which controls four parameters affecting how supervisory processes read status and interpret the health of the cluster. Changing only the one parameter provides cluster tuning in an easy, supportable manner.

The Kubelet process provides the starting point for monitoring cluster health. The Kubelet sets status values for all nodes in the "Red Hat OpenShift Container Platform" cluster. The Kubernetes Controller Manager (kube controller) reads the status values every 10 seconds, by default. If the kube controller cannot read a node status value, it loses contact with that node after a configured period. The default behavior is:

The node controller on the control plane updates the node health to Unhealthy and marks the node Ready condition`Unknown`.

In response, the scheduler stops scheduling pods to that node.

The Node Lifecycle Controller adds a node.kubernetes.io/unreachable taint with a NoExecute effect to the node and schedules any pods on the node for eviction after five minutes, by default.


This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager might not receive an update from a healthy node due to network latency. The Kubelet evicts pods from the node even though the node is healthy.

To avoid this problem, you can use worker latency profiles to adjust the frequency that the Kubelet and the Kubernetes Controller Manager wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly if network latency between the control plane and the worker nodes is not optimal.

These worker latency profiles contain three sets of parameters that are pre-defined with carefully tuned values to control the reaction of the cluster to increased latency. No need to experimentally find the best values manually.

You can configure worker latency profiles when installing a cluster or at any time you notice increased latency in your cluster network.

Understanding worker latency profiles
Worker latency profiles are four different categories of carefully-tuned parameters. The four parameters which implement these values are node-status-update-frequency, node-monitor-grace-period, default-not-ready-toleration-seconds and default-unreachable-toleration-seconds. These parameters can use values which allow you control the reaction of the cluster to latency issues without needing to determine the best values using manual methods.

Setting these parameters manually is not supported. Incorrect parameter settings adversely affect cluster stability.
All worker latency profiles configure the following parameters:


node-status-update-frequency
Specifies how often the kubelet posts node status to the API server.
node-monitor-grace-period
Specifies the amount of time in seconds that the Kubernetes Controller Manager waits for an update from a kubelet before marking the node unhealthy and adding the node.kubernetes.io/not-ready or node.kubernetes.io/unreachable taint to the node.
default-not-ready-toleration-seconds
Specifies the amount of time in seconds after marking a node unhealthy that the Kube API Server Operator waits before evicting pods from that node.
default-unreachable-toleration-seconds
Specifies the amount of time in seconds after marking a node unreachable that the Kube API Server Operator waits before evicting pods from that node.
The following Operators monitor the changes to the worker latency profiles and respond accordingly:

The Machine Config Operator (MCO) updates the node-status-update-frequency parameter on the worker nodes.

The Kubernetes Controller Manager updates the node-monitor-grace-period parameter on the control plane nodes.

The Kubernetes API Server Operator updates the default-not-ready-toleration-seconds and default-unreachable-toleration-seconds parameters on the control plane nodes.


Although the default configuration works in most cases, "Red Hat OpenShift Container Platform" offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:


Default worker latency profile
With the Default profile, each Kubelet updates it's status every 10 seconds (node-status-update-frequency). The Kube Controller Manager checks the statuses of Kubelet every 5 seconds (node-monitor-grace-period).
Medium worker latency profile
Use the MediumUpdateAverageReaction profile if the network latency is slightly higher than usual.
Low worker latency profile
Use the LowUpdateSlowReaction profile if the network latency is extremely high.
Using and changing worker latency profiles
To change a worker latency profile to deal with network latency, edit the node.config object to add the name of the profile. You can change the profile at any time as latency increases or decreases.

You must move one worker latency profile at a time. For example, you cannot move directly from the Default profile to the LowUpdateSlowReaction worker latency profile. You must move from the Default worker latency profile to the MediumUpdateAverageReaction profile first, then to LowUpdateSlowReaction. Similarly, when returning to the Default profile, you must move from the low profile to the medium profile first, then to Default.

You can also configure worker latency profiles upon installing an "Red Hat OpenShift Container Platform" cluster.
To move from the default worker latency profile:

Move to the medium worker latency profile:

Optional: Move to the low worker latency profile:


Scheduling on each worker node is disabled as the change is being applied.

When all nodes return to the Ready condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:


To change the medium profile to default or change the default to medium, edit the node.config object and set the spec.workerLatencyProfile parameter to the appropriate value.
Managing control plane machines
Control plane machine sets provide management capabilities for control plane machines that are similar to what compute machine sets provide for compute machines. The availability and initial status of control plane machine sets on your cluster depend on your cloud provider and the version of "Red Hat OpenShift Container Platform" that you installed. For more information, see Getting started with control plane machine sets.
Creating infrastructure machine sets for production environments
You can create a compute machine set to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.

In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.

For information on infrastructure nodes and which components can run on infrastructure nodes, see Creating infrastructure machine sets.

To create an infrastructure node, you can use a machine set, assign a label to the nodes, or use a machine config pool.

For sample machine sets that you can use with these procedures, see Creating machine sets for different clouds.

Applying a specific node selector to all infrastructure components causes "Red Hat OpenShift Container Platform" to schedule those workloads on nodes with that label.

Creating a compute machine set
In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.

Deploy an "Red Hat OpenShift Container Platform" cluster.

Install the OpenShift CLI (oc).

Log in to oc as a user with cluster-admin permission.


Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <file_name>.yaml.

Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.

Create a MachineSet CR by running the following command:


View the list of compute machine sets by running the following command:
Creating an infrastructure node
See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
Requirements of the cluster dictate that infrastructure, also called infra nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called app, nodes through labeling.

Add a label to the worker node that you want to act as application node:

Add a label to the worker nodes that you want to act as infrastructure nodes:

Check to see if applicable nodes now have the infra role and app roles:

Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod's selector.


You can now move infrastructure resources to the newly labeled infra nodes.

For information on how to configure project node selectors to avoid cluster-wide node selector key conflicts, see Project node selectors.
Creating a machine config pool for infrastructure machines
If you need infrastructure machines to have dedicated configurations, you must create an infra pool.

Creating a custom machine configuration pool overrides default worker pool configurations if they refer to the same file or unit.
Add a label to the node you want to assign as the infra node with a specific label:

Create a machine config pool that contains both the worker role and your custom role as machine config selector:

After you have the YAML file, you can create the machine config pool:

Check the machine configs to ensure that the infrastructure configuration rendered successfully:

Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as infra. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.

Confirm that your new machine config pool is available:


See Node configuration management with machine config pools for more information on grouping infra machines in a custom pool.
Assigning machine set resources to infrastructure nodes
After creating an infrastructure machine set, the worker and infra roles are applied to new infra nodes. Nodes with the infra role are not counted toward the total number of subscriptions that are required to run the environment, even when the worker role is also applied.

However, when an infra node is assigned the worker role, there is a chance that user workloads can get assigned inadvertently to the infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods that you want to control.

Binding infrastructure node workloads using taints and tolerations
If you have an infra node that has the infra and worker roles assigned, you must configure the node so that user workloads are not assigned to it.

It is recommended that you preserve the dual infra,worker label that is created for infra nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the worker label from the node, you must create a custom pool to manage it. A node with a label other than master or worker is not recognized by the MCO without a custom pool. Maintaining the worker label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The infra label communicates to the cluster that it does not count toward the total number of subscriptions.
Configure additional MachineSet objects in your "Red Hat OpenShift Container Platform" cluster.


Add a taint to the infra node to prevent scheduling user workloads on it:

Add tolerations for the pod configurations you want to schedule on the infra node, like router, registry, and monitoring workloads. Add the following code to the Pod object specification:

Schedule the pod to the infra node using a scheduler. See the documentation for Controlling pod placement onto nodes for details.


See Controlling pod placement using the scheduler for general information on scheduling a pod to a node.
Moving resources to infrastructure machine sets
Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created.

Moving the router
You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.

Configure additional compute machine sets in your "Red Hat OpenShift Container Platform" cluster.


View the IngressController custom resource for the router Operator:

Edit the ingresscontroller resource and change the nodeSelector to use the infra label:

Confirm that the router pod is running on the infra node.
Moving the default registry
You configure the registry Operator to deploy its pods to different nodes.

Configure additional compute machine sets in your "Red Hat OpenShift Container Platform" cluster.


View the config/instance object:

Edit the config/instance object:

Verify the registry pod has been moved to the infrastructure node.
Moving the monitoring solution
The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager. The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.

Edit the cluster-monitoring-config config map and change the nodeSelector to use the infra label:

Watch the monitoring pods move to the new machines:

If a component has not moved to the infra node, delete the pod with this component:
Moving logging resources
You can configure the Red Hat OpenShift Logging Operator to deploy the pods for logging components, such as Elasticsearch and Kibana, to different nodes. You cannot move the Red Hat OpenShift Logging Operator pod from its installed location.

For example, you can move the Elasticsearch pods to a separate node because of high CPU, memory, and disk requirements.

You have installed the Red Hat OpenShift Logging Operator and the OpenShift Elasticsearch Operator.


Edit the ClusterLogging custom resource (CR) in the openshift-logging project:


To verify that a component has moved, you can use the oc get pod -o wide command.

For example:

You want to move the Kibana pod from the ip-10-0-147-79.us-east-2.compute.internal node:

You want to move the Kibana pod to the ip-10-0-139-48.us-east-2.compute.internal node, a dedicated infrastructure node:

To move the Kibana pod, edit the ClusterLogging CR to add a node selector:

After you save the CR, the current Kibana pod is terminated and new pod is deployed:

The new pod is on the ip-10-0-139-48.us-east-2.compute.internal node:

After a few moments, the original Kibana pod is removed.
About the cluster autoscaler
The cluster autoscaler adjusts the size of an "Red Hat OpenShift Container Platform" cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.

The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.

The cluster autoscaler computes the total memory, CPU, and GPU on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.

Ensure that the maxNodesTotal value in the ClusterAutoscaler resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.
Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:

The node utilization is less than the node utilization level threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the ClusterAutoscaler custom resource, the cluster autoscaler uses a default value of 0.5, which corresponds to 50% utilization.

The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.

The cluster autoscaler does not have scale down disabled annotation.


If the following types of pods are present on a node, the cluster autoscaler will not remove the node:

Pods with restrictive pod disruption budgets (PDBs).

Kube-system pods that do not run on the node by default.

Kube-system pods that do not have a PDB or have a PDB that is too restrictive.

Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.

Pods with local storage.

Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.

Unless they also have a "cluster-autoscaler.kubernetes.io/safe-to-evict": "true" annotation, pods that have a "cluster-autoscaler.kubernetes.io/safe-to-evict": "false" annotation.


For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.

If you configure the cluster autoscaler, additional usage restrictions apply:

Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.

Specify requests for your pods.

If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.

Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.

Do not run additional node group autoscalers, especially the ones offered by your cloud provider.


The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment's or replica set's number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.

The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.

Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.

Cluster autoscaling is supported for the platforms that have machine API available on it.

Cluster autoscaler resource definition
This ClusterAutoscaler resource definition shows the parameters and sample values for the cluster autoscaler.

apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 1
  resourceLimits:
    maxNodesTotal: 24 2
    cores:
      min: 8 3
      max: 128 4
    memory:
      min: 4 5
      max: 256 6
    gpus:
      - type: nvidia.com/gpu 7
        min: 0 8
        max: 16 9
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 10
  scaleDown: 11
    enabled: true 12
    delayAfterAdd: 10m 13
    delayAfterDelete: 5m 14
    delayAfterFailure: 30s 15
    unneededTime: 5m 16
    utilizationThreshold: "0.4" 17
Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The podPriorityThreshold value is compared to the value of the PriorityClass that you assign to each pod.

Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your MachineAutoscaler resources.

Specify the minimum number of cores to deploy in the cluster.

Specify the maximum number of cores to deploy in the cluster.

Specify the minimum amount of memory, in GiB, in the cluster.

Specify the maximum amount of memory, in GiB, in the cluster.

Optional: Specify the type of GPU node to deploy. Only nvidia.com/gpu and amd.com/gpu are valid types.

Specify the minimum number of GPUs to deploy in the cluster.

Specify the maximum number of GPUs to deploy in the cluster.

Specify the logging verbosity level between 0 and 10. The following log level thresholds are provided for guidance:

In this section, you can specify the period to wait for each action by using any valid ParseDuration interval, including ns, us, ms, s, m, and h.

Specify whether the cluster autoscaler can remove unnecessary nodes.

Optional: Specify the period to wait before deleting a node after a node has recently been added. If you do not specify a value, the default value of 10m is used.

Optional: Specify the period to wait before deleting a node after a node has recently been deleted. If you do not specify a value, the default value of 0s is used.

Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of 3m is used.

Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of 10m is used.

Optional:  Specify the node utilization level. Nodes below this utilization level are eligible for deletion. If you do not specify a value, the default value of 10m is used.. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than "0" but less than "1". If you do not specify a value, the cluster autoscaler uses a default value of "0.5", which corresponds to 50% utilization. This value must be expressed as a string.


When performing a scaling operation, the cluster autoscaler remains within the ranges set in the ClusterAutoscaler resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.

The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
Deploying a cluster autoscaler
To deploy a cluster autoscaler, you create an instance of the ClusterAutoscaler resource.

Create a YAML file for a ClusterAutoscaler resource that contains the custom resource definition.

Create the custom resource in the cluster by running the following command:
About the machine autoscaler
The machine autoscaler adjusts the number of Machines in the compute machine sets that you deploy in an "Red Hat OpenShift Container Platform" cluster. You can scale both the default worker compute machine set and any other compute machine sets that you create. The machine autoscaler makes more Machines when the cluster runs out of resources to support more deployments. Any changes to the values in MachineAutoscaler resources, such as the minimum or maximum number of instances, are immediately applied to the compute machine set they target.

You must deploy a machine autoscaler for the cluster autoscaler to scale your machines. The cluster autoscaler uses the annotations on compute machine sets that the machine autoscaler sets to determine the resources that it can scale. If you define a cluster autoscaler without also defining machine autoscalers, the cluster autoscaler will never scale your cluster.
Machine autoscaler resource definition
This MachineAutoscaler resource definition shows the parameters and sample values for the machine autoscaler.

apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
  name: "worker-us-east-1a" 1
  namespace: "openshift-machine-api"
spec:
  minReplicas: 1 2
  maxReplicas: 12 3
  scaleTargetRef: 4
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet 5
    name: worker-us-east-1a 6
Specify the machine autoscaler name. To make it easier to identify which compute machine set this machine autoscaler scales, specify or include the name of the compute machine set to scale. The compute machine set name takes the following form: <clusterid>-<machineset>-<region>.

Specify the minimum number machines of the specified type that must remain in the specified zone after the cluster autoscaler initiates cluster scaling. If running in AWS, GCP, Azure, RHOSP, or vSphere, this value can be set to 0. For other providers, do not set this value to 0.

Specify the maximum number machines of the specified type that the cluster autoscaler can deploy in the specified zone after it initiates cluster scaling. Ensure that the maxNodesTotal value in the ClusterAutoscaler resource definition is large enough to allow the machine autoscaler to deploy this number of machines.

In this section, provide values that describe the existing compute machine set to scale.

The kind parameter value is always MachineSet.

The name value must match the name of an existing compute machine set, as shown in the metadata.name parameter value.
Deploying a machine autoscaler
To deploy a machine autoscaler, you create an instance of the MachineAutoscaler resource.

Create a YAML file for a MachineAutoscaler resource that contains the custom resource definition.

Create the custom resource in the cluster by running the following command:
Configuring Linux cgroup
As of "Red Hat OpenShift Container Platform" 4.14, "Red Hat OpenShift Container Platform" uses Linux control group version 2 (cgroup v2) in your cluster. If you are using cgroup v1 on "Red Hat OpenShift Container Platform" 4.13 or earlier, migrating to "Red Hat OpenShift Container Platform" 4.14 or later will not automatically update your cgroup configuration to version 2. A fresh installation of "Red Hat OpenShift Container Platform" 4.14 or later will use cgroup v2 by default. However, you can enable Linux control group version 1 (cgroup v1) upon installation.

cgroup v2 is the current version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as Pressure Stall Information, and enhanced resource management and isolation. However, cgroup v2 has different CPU, memory, and I/O management characteristics than cgroup v1. Therefore, some workloads might experience slight differences in memory or CPU usage on clusters that run cgroup v2.

You can change between cgroup v1 and cgroup v2, as needed. Enabling cgroup v1 in "Red Hat OpenShift Container Platform" disables all cgroup v2 controllers and hierarchies in your cluster.

Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
You have a running "Red Hat OpenShift Container Platform" cluster that uses version 4.12 or later.

You are logged in to the cluster as a user with administrative privileges.


Enable cgroup v1 on nodes:


Check the machine configs to see that the new machine configs were added:

Check that the new kernelArguments were added to the new machine configs:

Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:

After a node returns to the Ready state, start a debug session for that node:

Set /host as the root directory within the debug shell:

Check that the sys/fs/cgroup/cgroup2fs file is present on your nodes. This file is created by cgroup v1:


Configuring the Linux cgroup version on your nodes
Enabling Technology Preview features using FeatureGates
You can turn on a subset of the current Technology Preview features on for all nodes in the cluster by editing the FeatureGate custom resource (CR).

Understanding feature gates
You can use the FeatureGate custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of "Red Hat OpenShift Container Platform" features that are not enabled by default.

You can activate the following feature set by using the FeatureGate CR:

TechPreviewNoUpgrade. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.
Enabling feature sets using the web console
You can use the "Red Hat OpenShift Container Platform" web console to enable feature sets for all of the nodes in a cluster by editing the FeatureGate custom resource (CR).

To enable feature sets:

In the "Red Hat OpenShift Container Platform" web console, switch to the Administration -> Custom Resource Definitions page.

On the Custom Resource Definitions page, click FeatureGate.

On the Custom Resource Definition Details page, click the Instances tab.

Click the cluster feature gate, then click the YAML tab.

Edit the cluster instance to add specific feature sets:


You can verify that the feature gates are enabled by looking at the kubelet.conf file on a node after the nodes return to the ready state.

From the Administrator perspective in the web console, navigate to Compute -> Nodes.

Select a node.

In the Node details page, click Terminal.

In the terminal window, change your root directory to /host:

View the kubelet.conf file:
Enabling feature sets using the CLI
You can use the OpenShift CLI (oc) to enable feature sets for all of the nodes in a cluster by editing the FeatureGate custom resource (CR).

You have installed the OpenShift CLI (oc).


To enable feature sets:

Edit the FeatureGate CR named cluster:


You can verify that the feature gates are enabled by looking at the kubelet.conf file on a node after the nodes return to the ready state.

From the Administrator perspective in the web console, navigate to Compute -> Nodes.

Select a node.

In the Node details page, click Terminal.

In the terminal window, change your root directory to /host:

View the kubelet.conf file:
etcd tasks
Back up etcd, enable or disable etcd encryption, or defragment etcd data.

About etcd encryption
By default, etcd data is not encrypted in "Red Hat OpenShift Container Platform". You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.

When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:

Secrets

Config maps

Routes

OAuth access tokens

OAuth authorize tokens


When you enable etcd encryption, encryption keys are created. You must have these keys to restore from an etcd backup.

Etcd encryption only encrypts values, not keys. Resource types, namespaces, and object names are unencrypted.

If etcd encryption is enabled during a backup, the static_kuberesources_<datetimestamp>.tar.gz file contains the encryption keys for the etcd snapshot. For security reasons, store this file separately from the etcd snapshot. However, this file is required to restore a previous state of etcd from the respective etcd snapshot.
Supported encryption types
The following encryption types are supported for encrypting etcd data in "Red Hat OpenShift Container Platform":


AES-CBC
Uses AES-CBC with PKCS#7 padding and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.
AES-GCM
Uses AES-GCM with a random nonce and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.
Enabling etcd encryption
You can enable etcd encryption to encrypt sensitive resources in your cluster.

Do not back up etcd resources until the initial encryption process is completed. If the encryption process is not completed, the backup might be only partially encrypted.

After you enable etcd encryption, several changes can occur:

The etcd encryption might affect the memory consumption of a few resources.

You might notice a transient affect on backup performance because the leader must serve the backup.

A disk I/O can affect the node that receives the backup state.
You can encrypt the etcd database in either AES-GCM or AES-CBC encryption.

To migrate your etcd database from one encryption type to the other, you can modify the API server's spec.encryption.type field. Migration of the etcd data to the new encryption type occurs automatically.
Access to the cluster as a user with the cluster-admin role.


Modify the APIServer object:

Set the spec.encryption.type field to aesgcm or aescbc:

Save the file to apply the changes.

Verify that etcd encryption was successful.
Disabling etcd encryption
You can disable encryption of etcd data in your cluster.

Access to the cluster as a user with the cluster-admin role.


Modify the APIServer object:

Set the encryption field type to identity:

Save the file to apply the changes.

Verify that etcd decryption was successful.
Backing up etcd data
Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.

Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.
You have access to the cluster as a user with the cluster-admin role.

You have checked whether the cluster-wide proxy is enabled.


Start a debug session as root for a control plane node:

Change your root directory to /host in the debug shell:

If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables.

Run the cluster-backup.sh script in the debug shell and pass in the location to save the backup to.
Defragmenting etcd data
For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.

Monitor these key metrics:

etcd_server_quota_backend_bytes, which is the current quota limit

etcd_mvcc_db_total_size_in_use_in_bytes, which indicates the actual database usage after a history compaction

etcd_mvcc_db_total_size_in_bytes, which shows the database size, including free space waiting for defragmentation


Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.

History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.

Defragmentation occurs automatically, but you can also trigger it manually.

Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.
Automatic defragmentation
The etcd Operator automatically defragments disks. No manual intervention is needed.

Verify that the defragmentation process is successful by viewing one of these logs:

etcd logs

cluster-etcd-operator pod

operator status error log


Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.
etcd member has been defragmented: <member_name>, memberID: <member_id>
failed defrag on member: <member_name>, memberID: <member_id>: <error_message>
Manual defragmentation
A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:

When etcd uses more than 50% of its available space for more than 10 minutes

When etcd is actively using less than 50% of its total database size for more than 10 minutes


You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: (etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024

Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.
Follow this procedure to defragment etcd data on each etcd member.

You have access to the cluster as a user with the cluster-admin role.


Determine which etcd member is the leader, because the leader should be defragmented last.

Defragment an etcd member.

If any NOSPACE alarms were triggered due to the space quota being exceeded, clear them.
Restoring to a previous cluster state
You can use a saved etcd backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.

If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure.
When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an "Red Hat OpenShift Container Platform" 4.7.2 cluster must use an etcd backup that was taken from 4.7.2.
Access to the cluster as a user with the cluster-admin role through a certificate-based kubeconfig file, like the one that was used during installation.

A healthy control plane host to use as the recovery host.

SSH access to control plane hosts.

A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: snapshot_<datetimestamp>.db and static_kuberesources_<datetimestamp>.tar.gz.


For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.
Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.

Establish SSH connectivity to each of the control plane nodes, including the recovery host.

Copy the etcd backup directory to the recovery control plane host.

Stop the static pods on any other control plane nodes.

Access the recovery control plane host.

If the keepalived daemon is in use, verify that the recovery control plane node owns the VIP:

If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables.

Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory:

Check the nodes to ensure they are in the Ready state.

Restart the kubelet service on all control plane hosts.

Approve the pending Certificate Signing Requests (CSRs):


CSR (for user-provisioned installations). <2> A pending node-bootstrapper CSR.

Review the details of a CSR to verify that it is valid by running:

Approve each valid node-bootstrapper CSR by running:

For user-provisioned installations, approve each valid kubelet service CSR by running:

From the recovery host, verify that the etcd container is running by using:

From the recovery host, verify that the etcd pod is running by using:

Delete all of the ovnkube-controlplane pods by running:

Verify that all of the ovnkube-controlplane pods were redeployed by using:

Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run:

Restart the OpenVSwitch services. Access the node by using Secure Shell (SSH) and run the following command:

Delete the ovnkube-node pod on the node by running the following command, replacing <node> with the name of the node that you are restarting:

Verify that the ovnkube-node pod is running again with:

Obtain the machine for one of the lost control plane hosts.

Save the machine configuration to a file on your file system by running:

Edit the new-master-machine.yaml file that was created in the previous step to assign a new name and remove unnecessary fields.

Delete the machine of the lost control plane host by running:

Verify that the machine was deleted by running:

Create a machine by using the new-master-machine.yaml file by running:

Verify that the new machine has been created by running:

Repeat these steps for each lost control plane host that is not the recovery host.

Force a new rollout for kube-apiserver:

Force a new rollout for the Kubernetes controller manager by running the following command:

Force a new rollout for the kube-scheduler by running:


To ensure that all workloads return to normal operation following a recovery procedure, restart each pod that stores kube-apiserver information. This includes "Red Hat OpenShift Container Platform" components such as routers, Operators, and third-party components.

On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using oc login might not immediately work until the OAuth server pods are restarted.

Consider using the system:admin kubeconfig file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:

$ export KUBECONFIG=<installation_directory>/auth/kubeconfig
Issue the following command to display your authenticated user name:

$ oc whoami
Installing a user-provisioned cluster on bare metal

Replacing a bare-metal control plane node
Issues and workarounds for restoring a persistent storage state
If your "Red Hat OpenShift Container Platform" cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a StatefulSet object. When you restore from an etcd backup, the status of the workloads in "Red Hat OpenShift Container Platform" is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.

The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an "Red Hat OpenShift Container Platform" cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.
The following are some example scenarios that produce an out-of-date status:

MySQL database is running in a pod backed up by a PV object. Restoring "Red Hat OpenShift Container Platform" from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.

Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. "Red Hat OpenShift Container Platform" is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.

Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.

A device is removed or renamed from "Red Hat OpenShift Container Platform" nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from /dev/disk/by-id or /dev directories. This situation might cause the local PVs to refer to devices that no longer exist.
Pod disruption budgets
Understand and configure pod disruption budgets.

Understanding how to use pod disruption budgets to specify the number of pods that must be up
A pod disruption budget allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.

PodDisruptionBudget is an API object that specifies the minimum number or percentage of replicas that must be up at a time. Setting these in projects can be helpful during node maintenance (such as scaling a cluster down or a cluster upgrade) and is only honored on voluntary evictions (not on node failures).

A PodDisruptionBudget object's configuration consists of the following key parts:

A label selector, which is a label query over a set of pods.

An availability level, which specifies the minimum number of pods that must be
available simultaneously, either:


Available refers to the number of pods that has condition Ready=True. Ready=True refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.

A maxUnavailable of 0% or 0 or a minAvailable of 100% or equal to the number of replicas is permitted but can block nodes from being drained.
The default setting for maxUnavailable is 1 for all the machine config pools in "Red Hat OpenShift Container Platform". It is recommended to not change this value and update one control plane node at a time. Do not change this value to 3 for the control plane pool.
You can check for pod disruption budgets across all projects with the following:

$ oc get poddisruptionbudget --all-namespaces
NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...
The PodDisruptionBudget is considered healthy when there are at least minAvailable pods running in the system. Every pod above that limit can be evicted.

Depending on your pod priority and preemption settings, lower-priority pods might be removed despite their pod disruption budget requirements.
Specifying the number of pods that must be up with pod disruption budgets
You can use a PodDisruptionBudget object to specify the minimum number or percentage of replicas that must be up at a time.

To configure a pod disruption budget:

Create a YAML file with the an object definition similar to the following:

Run the following command to add the object to project:
Specifying the eviction policy for unhealthy pods
When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.

You can choose one of the following policies:


IfHealthyBudget
Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.
AlwaysAllow
Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the CrashLoopBackOff state or failing to report the Ready status.


Create a YAML file that defines a PodDisruptionBudget object and specify the unhealthy pod eviction policy:

Create the PodDisruptionBudget object by running the following command:


With a PDB that has the AlwaysAllow unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.

Enabling features using feature gates

Unhealthy Pod Eviction Policy in the Kubernetes documentation
Rotating or removing cloud provider credentials
After installing "Red Hat OpenShift Container Platform", some organizations require the rotation or removal of the cloud provider credentials that were used during the initial installation.

To allow the cluster to use the new credentials, you must update the secrets that the Cloud Credential Operator (CCO) uses to manage cloud provider credentials.

Rotating cloud provider credentials with the Cloud Credential Operator utility
The Cloud Credential Operator (CCO) utility ccoctl supports updating secrets for clusters installed on IBM Cloud&#174;.

Rotating API keys
You can rotate API keys for your existing service IDs and update the corresponding secrets.

You have configured the ccoctl binary.

You have existing service IDs in a live "Red Hat OpenShift Container Platform" cluster installed.


Use the ccoctl utility to rotate your API keys for the service IDs and update the secrets:
Rotating cloud provider credentials manually
If your cloud provider credentials are changed for any reason, you must manually update the secret that the Cloud Credential Operator (CCO) uses to manage cloud provider credentials.

The process for rotating cloud credentials depends on the mode that the CCO is configured to use. After you rotate credentials for a cluster that is using mint mode, you must manually remove the component credentials that were created by the removed credential.

Your cluster is installed on a platform that supports rotating cloud credentials manually with the CCO mode that you are using:

You have changed the credentials that are used to interface with your cloud provider.

The new credentials have sufficient permissions for the mode CCO is configured to use in your cluster.


In the Administrator perspective of the web console, navigate to Workloads -> Secrets.

In the table on the Secrets page, find the root secret for your cloud provider.

Click the Options menu  in the same row as the secret and select Edit Secret.

Record the contents of the Value field or fields. You can use this information to verify that the value is different after updating the credentials.

Update the text in the Value field or fields with the new authentication information for your cloud provider, and then click Save.

If you are updating the credentials for a vSphere cluster that does not have the vSphere CSI Driver Operator enabled, you must force a rollout of the Kubernetes controller manager to apply the updated credentials.

If the CCO for your cluster is configured to use mint mode, delete each component secret that is referenced by the individual CredentialsRequest objects.


To verify that the credentials have changed:

In the Administrator perspective of the web console, navigate to Workloads -> Secrets.

Verify that the contents of the Value field or fields have changed.


vSphere CSI Driver Operator
Removing cloud provider credentials
After installing an "Red Hat OpenShift Container Platform" cluster with the Cloud Credential Operator (CCO) in mint mode, you can remove the administrator-level credential secret from the kube-system namespace in the cluster. The administrator-level credential is required only during changes that require its elevated permissions, such as upgrades.

Prior to a non z-stream upgrade, you must reinstate the credential secret with the administrator-level credential. If the credential is not present, the upgrade might be blocked.
Your cluster is installed on a platform that supports removing cloud credentials from the CCO. Supported platforms are AWS and GCP.


In the Administrator perspective of the web console, navigate to Workloads -> Secrets.

In the table on the Secrets page, find the root secret for your cloud provider.

Click the Options menu  in the same row as the secret and select Delete Secret.


About the Cloud Credential Operator

Admin credentials root secret format
Configuring image streams for a disconnected cluster
After installing "Red Hat OpenShift Container Platform" in a disconnected environment, configure the image streams for the Cluster Samples Operator and the must-gather image stream.

Cluster Samples Operator assistance for mirroring
During installation, "Red Hat OpenShift Container Platform" creates a config map named imagestreamtag-to-image in the openshift-cluster-samples-operator namespace. The imagestreamtag-to-image config map contains an entry, the populating image, for each image stream tag.

The format of the key for each entry in the data field in the config map is <image_stream_name>_<image_stream_tag_name>.

During a disconnected installation of "Red Hat OpenShift Container Platform", the status of the Cluster Samples Operator is set to Removed. If you choose to change it to Managed, it installs samples.

The use of samples in a network-restricted or discontinued environment may require access to services external to your network. Some example services include: Github, Maven Central, npm, RubyGems, PyPi and others. There might be additional steps to take that allow the cluster samples operators's objects to reach the services they require.
You can use this config map as a reference for which images need to be mirrored for your image streams to import.

While the Cluster Samples Operator is set to Removed, you can create your mirrored registry, or determine which existing mirrored registry you want to use.

Mirror the samples you want to the mirrored registry using the new config map as your guide.

Add any of the image streams you did not mirror to the skippedImagestreams list of the Cluster Samples Operator configuration object.

Set samplesRegistry of the Cluster Samples Operator configuration object to the mirrored registry.

Then set the Cluster Samples Operator to Managed to install the image streams you have mirrored.
Using Cluster Samples Operator image streams with alternate or mirrored registries
Most image streams in the openshift namespace managed by the Cluster Samples Operator point to images located in the Red Hat registry at registry.redhat.io. Mirroring will not apply to these image streams.

The cli, installer, must-gather, and tests image streams, while part of the install payload, are not managed by the Cluster Samples Operator. These are not addressed in this procedure.
The Cluster Samples Operator must be set to Managed in a disconnected environment. To install the image streams, you have a mirrored registry.
Access to the cluster as a user with the cluster-admin role.

Create a pull secret for your mirror registry.


Access the images of a specific image stream to mirror, for example:

Mirror images from registry.redhat.io associated with any image streams you need
in the restricted network environment into one of the defined mirrors, for example:

Create the cluster's image configuration object:

Add the required trusted CAs for the mirror in the cluster's image
configuration object:

Update the samplesRegistry field in the Cluster Samples Operator configuration object
to contain the hostname portion of the mirror location defined in the mirror
configuration:

Add any image streams that are not mirrored into the skippedImagestreams field
of the Cluster Samples Operator configuration object. Or if you do not want to support
any of the sample image streams, set the Cluster Samples Operator to Removed in the
Cluster Samples Operator configuration object.
Preparing your cluster to gather support data
Clusters using a restricted network must import the default must-gather image to gather debugging data for Red Hat support. The must-gather image is not imported by default, and clusters on a restricted network do not have access to the internet to pull the latest image from a remote repository.

If you have not added your mirror registry's trusted CA to your cluster's image configuration object as part of the Cluster Samples Operator configuration, perform the following steps:

Import the default must-gather image from your installation payload:


When running the oc adm must-gather command, use the --image flag and point to the payload image, as in the following example:

$ oc adm must-gather --image=$(oc adm release info --image-for must-gather)
Configuring periodic importing of Cluster Sample Operator image stream tags
You can ensure that you always have access to the latest versions of the Cluster Sample Operator images by periodically importing the image stream tags when new versions become available.

Fetch all the imagestreams in the openshift namespace by running the following command:

Fetch the tags for every imagestream in the openshift namespace by running the following command:

Schedule periodic importing of images for each tag present in the image stream by running the following command:

Verify the scheduling status of the periodic import by running the following command: