Backup, restore, and disaster recovery for hosted control planes

If you need to back up and restore etcd on a hosted cluster or provide disaster recovery for a hosted cluster, see the following procedures.
Recovering etcd pods for hosted clusters
In hosted clusters, etcd pods run as part of a stateful set. The stateful set relies on persistent storage to store etcd data for each member. In a highly available control plane, the size of the stateful set is three pods, and each member has its own persistent volume claim.

Checking the status of a hosted cluster
To check the status of your hosted cluster, complete the following steps.

Enter the running etcd pod that you want to check by entering the following command:

Set up the etcdctl environment by entering the following commands:

Print the endpoint status for each cluster member by entering the following command:
Recovering an etcd member for a hosted cluster
An etcd member of a 3-node cluster might fail because of corrupted or missing data. To recover the etcd member, complete the following steps.

If you need to confirm that the etcd member is failing, enter the following command:

Delete the persistent volume claim of the failing etcd member and the pod by entering the following command:

When the pod restarts, verify that the etcd member is added back to the etcd cluster and is correctly functioning by entering the following command:
Backing up and restoring etcd on a hosted cluster on AWS
If you use hosted control planes for Red Hat OpenShift Container Platform, the process to back up and restore etcd is different from the usual etcd backup process.

The following procedures are specific to hosted control planes on AWS.

Hosted control planes on the AWS platform is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
Taking a snapshot of etcd on a hosted cluster
As part of the process to back up etcd for a hosted cluster, you take a snapshot of etcd. After you take the snapshot, you can restore it, for example, as part of a disaster recovery operation.

This procedure requires API downtime.
Pause reconciliation of the hosted cluster by entering this command:

Stop all etcd-writer deployments by entering this command:

To take an etcd snapshot, use the exec command in each etcd container by running the following command:

To check the snapshot status, use the exec command in each etcd container by running the following command:

Copy the snapshot data to a location where you can retrieve it later, such as an S3 bucket, as shown in the following example.

If you want to be able to restore the snapshot on a new cluster later, save the encryption secret that the hosted cluster references, as shown in this example:


Restore the etcd snapshot.
Restoring an etcd snapshot on a hosted cluster
If you have a snapshot of etcd from your hosted cluster, you can restore it. Currently, you can restore an etcd snapshot only during cluster creation.

To restore an etcd snapshot, you modify the output from the create cluster --render command and define a restoreSnapshotURL value in the etcd section of the HostedCluster specification.

You took an etcd snapshot on a hosted cluster.

On the aws command-line interface (CLI), create a pre-signed URL so that you can download your etcd snapshot from S3 without passing credentials to the etcd deployment:

Modify the HostedCluster specification to refer to the URL:

Ensure that the secret that you referenced from the spec.secretEncryption.aescbc value contains the same AES key that you saved in the previous steps.
Backing up and restoring etcd on a hosted cluster in an on-premise environment
By backing up and restoring etcd on a hosted cluster, you can fix failures, such as corrupted or missing data in an etcd member of a three node cluster. If multiple members of the etcd cluster encounter data loss or have a CrashLoopBackOff status, this approach helps prevent an etcd quorum loss.

This procedure requires API downtime.
The oc and jq binaries have been installed.


First, set up your environment variables and scale down the API servers:

Next, take a snapshot of etcd by using one of the following methods:

Next, scale down the etcd statefulset by entering the following command:

Restore reconciliation of the hosted cluster by entering the following command:
Disaster recovery for a hosted cluster within an AWS region
In a situation where you need disaster recovery (DR) for a hosted cluster, you can recover a hosted cluster to the same region within AWS. For example, you need DR when the upgrade of a management cluster fails and the hosted cluster is in a read-only state.

Hosted control planes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
The DR process involves three main steps:

Backing up the hosted cluster on the source management cluster

Restoring the hosted cluster on a destination management cluster

Deleting the hosted cluster from the source management cluster


Your workloads remain running during the process. The Cluster API might be unavailable for a period, but that will not affect the services that are running on the worker nodes.

Both the source management cluster and the destination management cluster must have the --external-dns flags to maintain the API server URL, as shown in this example:

--external-dns-provider=aws \
--external-dns-credentials=<AWS Credentials location> \
--external-dns-domain-filter=<DNS Base Domain>
That way, the server URL ends with https://api-sample-hosted.sample-hosted.aws.openshift.com.

If you do not include the --external-dns flags to maintain the API server URL, the hosted cluster cannot be migrated.
Example environment and context
Consider an scenario where you have three clusters to restore. Two are management clusters, and one is a hosted cluster. You can restore either the control plane only or the control plane and the nodes. Before you begin, you need the following information:

Source MGMT Namespace: The source management namespace

Source MGMT ClusterName: The source management cluster name

Source MGMT Kubeconfig: The source management kubeconfig file

Destination MGMT Kubeconfig: The destination management kubeconfig file

HC Kubeconfig: The hosted cluster kubeconfig file

SSH key file: The SSH public key

Pull secret: The pull secret file to access the release images

AWS credentials

AWS region

Base domain: The DNS base domain to use as an external DNS

S3 bucket name: The bucket in the AWS region where you plan to upload the etcd backup


This information is shown in the following example environment variables.

SSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub
BASE_PATH=${HOME}/hypershift
BASE_DOMAIN="aws.sample.com"
PULL_SECRET_FILE="${HOME}/pull_secret.json"
AWS_CREDS="${HOME}/.aws/credentials"
AWS_ZONE_ID="Z02718293M33QHDEQBROL"

CONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica
HYPERSHIFT_PATH=${BASE_PATH}/src/hypershift
HYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift
HYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-"quay.io/${USER}/hypershift:latest"}
NODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}

# MGMT Context
MGMT_REGION=us-west-1
MGMT_CLUSTER_NAME="${USER}-dev"
MGMT_CLUSTER_NS=${USER}
MGMT_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}"
MGMT_KUBECONFIG="${MGMT_CLUSTER_DIR}/kubeconfig"

# MGMT2 Context
MGMT2_CLUSTER_NAME="${USER}-dest"
MGMT2_CLUSTER_NS=${USER}
MGMT2_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}"
MGMT2_KUBECONFIG="${MGMT2_CLUSTER_DIR}/kubeconfig"

# Hosted Cluster Context
HC_CLUSTER_NS=clusters
HC_REGION=us-west-1
HC_CLUSTER_NAME="${USER}-hosted"
HC_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}"
HC_KUBECONFIG="${HC_CLUSTER_DIR}/kubeconfig"
BACKUP_DIR=${HC_CLUSTER_DIR}/backup

BUCKET_NAME="${USER}-hosted-${MGMT_REGION}"

# DNS
AWS_ZONE_ID="Z07342811SH9AA102K1AC"
EXTERNAL_DNS_DOMAIN="hc.jpdv.aws.kerbeross.com"
Overview of the backup and restore process
The backup and restore process works as follows:

On management cluster 1, which you can think of as the source management cluster, the control plane and workers interact by using the external DNS API. The external DNS API is accessible, and a load balancer sits between the management clusters.

You take a snapshot of the hosted cluster, which includes etcd, the control plane, and the worker nodes. During this process, the worker nodes continue to try to access the external DNS API even if it is not accessible, the workloads are running, the control plane is saved in a local manifest file, and etcd is backed up to an S3 bucket. The data plane is active and the control plane is paused.

On management cluster 2, which you can think of as the destination management cluster, you restore etcd from the S3 bucket and restore the control plane from the local manifest file. During this process, the external DNS API is stopped, the hosted cluster API becomes inaccessible, and any workers that use the API are unable to update their manifest files, but the workloads are still running.

The external DNS API is accessible again, and the worker nodes use it to move to management cluster 2. The external DNS API can access the load balancer that points to the control plane.

On management cluster 2, the control plane and worker nodes interact by using the external DNS API. The resources are deleted from management cluster 1, except for the S3 backup of etcd. If you try to set up the hosted cluster again on mangagement cluster 1, it will not work.


You can manually back up and restore your hosted cluster, or you can run a script to complete the process. For more information about the script, see "Running a script to back up and restore a hosted cluster".
Backing up a hosted cluster
To recover your hosted cluster in your target management cluster, you first need to back up all of the relevant data.

Create a configmap file to declare the source management cluster by entering this command:

Shut down the reconciliation in the hosted cluster and in the node pools by entering these commands:

Back up etcd and upload the data to an S3 bucket by running this bash script:

Back up Kubernetes and Red Hat OpenShift Container Platform objects by entering the following commands. You need to back up the following objects:

Clean up the ControlPlane routes by entering this command:

Verify that the Route53 entries are clean by running this script:


Check all of the Red Hat OpenShift Container Platform objects and the S3 bucket to verify that everything looks as expected.

Restore your hosted cluster.
Restoring a hosted cluster
Gather all of the objects that you backed up and restore them in your destination management cluster.

You backed up the data from your source management cluster.

Ensure that the kubeconfig file of the destination management cluster is placed as it is set in the KUBECONFIG variable or, if you use the script, in the MGMT2_KUBECONFIG variable. Use export KUBECONFIG=<Kubeconfig FilePath> or, if you use the script, use export KUBECONFIG=${MGMT2_KUBECONFIG}.
Verify that the new management cluster does not contain any namespaces from the cluster that you are restoring by entering these commands:

Re-create the deleted namespaces by entering these commands:

Restore the secrets in the HC namespace by entering this command:

Restore the objects in the HostedCluster control plane namespace by entering these commands:

If you are recovering the nodes and the node pool to reuse AWS instances, restore the objects in the HC control plane namespace by entering these commands:

Restore the etcd data and the hosted cluster by running this bash script:

If you are recovering the nodes and the node pool to reuse AWS instances, restore the node pool by entering this command:


To verify that the nodes are fully restored, use this function:


Shut down and delete your cluster.
Deleting a hosted cluster from your source management cluster
After you back up your hosted cluster and restore it to your destination management cluster, you shut down and delete the hosted cluster on your source management cluster.

You backed up your data and restored it to your source management cluster.

Ensure that the kubeconfig file of the destination management cluster is placed as it is set in the KUBECONFIG variable or, if you use the script, in the MGMT_KUBECONFIG variable. Use export KUBECONFIG=<Kubeconfig FilePath> or, if you use the script, use export KUBECONFIG=${MGMT_KUBECONFIG}.
Scale the deployment and statefulset objects by entering these commands:

Delete the NodePool objects by entering these commands:

Delete the machine and machineset objects by entering these commands:

Delete the cluster object by entering these commands:

Delete the AWS machines (Kubernetes objects) by entering these commands. Do not worry about deleting the real AWS machines. The cloud instances will not be affected.

Delete the HostedControlPlane and ControlPlane HC namespace objects by entering these commands:

Delete the HostedCluster and HC namespace objects by entering these commands:


To verify that everything works, enter these commands:


Delete the OVN pods in the hosted cluster so that you can connect to the new OVN control plane that runs in the new management cluster:

Load the KUBECONFIG environment variable with the hosted cluster's kubeconfig path.

Enter this command:
Running a script to back up and restore a hosted cluster
To expedite the process to back up a hosted cluster and restore it within the same region on AWS, you can modify and run a script.

Replace the variables in the following script with your information:

Save the script to your local file system.

Run the script by entering the following command: