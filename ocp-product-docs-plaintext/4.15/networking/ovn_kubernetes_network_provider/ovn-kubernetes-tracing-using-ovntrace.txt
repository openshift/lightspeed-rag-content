Tracing Openflow with ovnkube-trace

OVN and OVS traffic flows can be simulated in a single utility called ovnkube-trace. The ovnkube-trace utility runs ovn-trace, ovs-appctl ofproto/trace and ovn-detrace and correlates that information in a single output.

You can execute the ovnkube-trace binary from a dedicated container. For releases after Red Hat OpenShift Container Platform 4.7, you can also copy the binary to a local host and execute it from that host.
Installing the ovnkube-trace on local host
The ovnkube-trace tool traces packet simulations for arbitrary UDP or TCP traffic between points in an OVN-Kubernetes driven Red Hat OpenShift Container Platform cluster. Copy the ovnkube-trace binary to your local host making it available to run against the cluster.

You installed the OpenShift CLI (oc).

You are logged in to the cluster with a user with cluster-admin privileges.


Create a pod variable by using the following command:

Run the following command on your local host to copy the binary from the ovnkube-control-plane pods:

Make ovnkube-trace executable by running the following command:

Display the options available with ovnkube-trace by running the following command:
Running ovnkube-trace
Run ovn-trace to simulate packet forwarding within an OVN logical network.

You installed the OpenShift CLI (oc).

You are logged in to the cluster with a user with cluster-admin privileges.

You have installed ovnkube-trace on local host


This example illustrates how to test the DNS resolution from a deployed pod to the core DNS pod that runs in the cluster.

Start a web service in the default namespace by entering the following command:

List the pods running in the openshift-dns namespace:

Run the following ovnkube-trace command to verify DNS resolution is working:


If for example that did not work and you wanted to get the ovn-trace, the ovs-appctl of proto/trace and ovn-detrace, and more debug type information increase the log level to 2 and run the command again as follows:

$ ./ovnkube-trace \
  -src-namespace default \
  -src web \
  -dst-namespace openshift-dns \
  -dst dns-default-467qw \
  -udp -dst-port 53 \
  -loglevel 2
The output from this increased log level is too much to list here. In a failure situation the output of this command shows which flow is dropping that traffic. For example an egress or ingress network policy may be configured on the cluster that does not allow that traffic.

This example illustrates how to identify by using the debug output that an ingress default deny policy blocks traffic.

Create the following YAML that defines a deny-by-default policy to deny ingress from all pods in all namespaces. Save the YAML in the deny-by-default.yaml file:

Apply the policy by entering the following command:

Start a web service in the default namespace by entering the following command:

Run the following command to create the prod namespace:

Run the following command to label the prod namespace:

Run the following command to deploy an alpine image in the prod namespace and start a shell:

Open another terminal session.

In this new terminal session run ovn-trace to verify the failure in communication between the source pod test-6459 running in namespace prod and destination pod running in the default namespace:

Increase the log level to 2 to expose the reason for the failure by running the following command:

Create a policy that allows traffic from all pods in a particular namespaces with a label purpose=production. Save the YAML in the web-allow-prod.yaml file:

Apply the policy by entering the following command:

Run ovnkube-trace to verify that traffic is now allowed by entering the following command:

Run the following command in the shell that was opened in step six to connect nginx to the web-server:
Additional resources
Tracing Openflow with ovnkube-trace utility

ovnkube-trace