Migrating from the OpenShift SDN network plugin

As a cluster administrator, you can migrate to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin.

To learn more about OVN-Kubernetes, read About the OVN-Kubernetes network plugin.
Migration to the OVN-Kubernetes network plugin
Migrating to the OVN-Kubernetes network plugin is a manual process that includes some downtime during which your cluster is unreachable. Although a rollback procedure is provided, the migration is intended to be a one-way process.

A migration to the OVN-Kubernetes network plugin is supported on the following platforms:

Bare metal hardware

Amazon Web Services (AWS)

Google Cloud Platform (GCP)

IBM Cloud&#174;

Microsoft Azure

Red Hat OpenStack Platform (RHOSP)

VMware vSphere


Migrating to or from the OVN-Kubernetes network plugin is not supported for managed OpenShift cloud services such as Red Hat OpenShift Dedicated, Azure Red Hat OpenShift(ARO), and Red Hat OpenShift Service on AWS (ROSA).

Migrating from OpenShift SDN network plugin to OVN-Kubernetes network plugin is not supported on Nutanix.
OpenShift SDN CNI is deprecated as of "Red Hat OpenShift Container Platform" 4.14. As of "Red Hat OpenShift Container Platform" 4.15, the network plugin is not an option for new installations. In a subsequent future release, the OpenShift SDN network plugin is planned to be removed and no longer supported. RedÂ Hat will provide bug fixes and support for this feature until it is removed, but this feature will no longer receive enhancements. As an alternative to OpenShift SDN CNI, you can use OVN Kubernetes CNI instead.
Considerations for migrating to the OVN-Kubernetes network plugin
If you have more than 150 nodes in your "Red Hat OpenShift Container Platform" cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.

The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.

While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.

If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:

If your cluster or surrounding network uses any part of the 100.64.0.0/16 address range, you must choose another unused IP range by specifying the v4InternalSubnet spec under the spec.defaultNetwork.ovnKubernetesConfig object definition. OVN-Kubernetes uses the IP range 100.64.0.0/16 internally by default.


The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.


OVN-Kubernetes supports only the network policy isolation mode.

If your cluster uses OpenShift SDN configured in either the multitenant or subnet isolation modes, you cannot migrate to the OVN-Kubernetes network plugin.

OpenShift SDN supports two different Egress IP modes:

In the automatically assigned approach, an egress IP address range is assigned to a node.

In the manually assigned approach, a list of one or more egress IP addresses is assigned to a node.


The migration process supports migrating Egress IP configurations that use the automatically assigned mode.

The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:


For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".


The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:


Because the name of an EgressFirewall object can only be set to default, after the migration all migrated EgressNetworkPolicy objects are named default, regardless of what the name was under OpenShift SDN.

If you subsequently rollback to OpenShift SDN, all EgressNetworkPolicy objects are named default as the prior name is lost.

For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".

OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.

When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.


The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:


For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".


OVN-Kubernetes fully supports the Kubernetes NetworkPolicy API in the networking.k8s.io/v1 API group. No changes are necessary in your network policies when migrating from OpenShift SDN.
How the migration process works
The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.


If a rollback to OpenShift SDN is required, the following table describes the process.
Migrating to the OVN-Kubernetes network plugin
As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes. During the migration, you must reboot every node in your cluster.

While performing the migration, your cluster is unavailable and workloads might be interrupted. Perform the migration only when an interruption in service is acceptable.
A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.

Install the OpenShift CLI (oc).

Access to the cluster as a user with the cluster-admin role.

A recent backup of the etcd database is available.

A reboot can be triggered manually for each node.

The cluster is in a known good state, without any errors.

On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port 6081 for all nodes.


To backup the configuration for the cluster network, enter the following command:

To prepare all the nodes for the migration, set the migration field on the Cluster Network Operator configuration object by entering the following command:

Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:

Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:

As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:

Confirm the status of the new machine configuration on the hosts:

To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:

Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:

To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:

Confirm that the migration succeeded:

Complete the following steps only if the migration succeeds and your cluster is in a good state:
Additional resources
Red Hat OpenShift Network Calculator

Configuration parameters for the OVN-Kubernetes network plugin

Backing up etcd

About network policy

Changing the cluster MTU

MTU value selection

OVN-Kubernetes capabilities

OpenShift SDN capabilities

Network [operator.openshift.io/v1]