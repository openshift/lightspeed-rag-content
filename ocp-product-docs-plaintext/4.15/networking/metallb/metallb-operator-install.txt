Installing the MetalLB Operator

As a cluster administrator, you can add the MetallB Operator so that the Operator can manage the lifecycle for an instance of MetalLB on your cluster.

MetalLB and IP failover are incompatible. If you configured IP failover for your cluster, perform the steps to remove IP failover before you install the Operator.
Installing the MetalLB Operator from the OperatorHub using the web console
As a cluster administrator, you can install the MetalLB Operator by using the Red Hat OpenShift Container Platform web console.

Log in as a user with cluster-admin privileges.


In the Red Hat OpenShift Container Platform web console, navigate to Operators -> OperatorHub.

Type a keyword into the Filter by keyword box or scroll to find the Operator you want. For example, type metallb to find the MetalLB Operator.

On the Install Operator page, accept the defaults and click Install.


To confirm that the installation is successful:

If the Operator is not installed successfully, check the status of the Operator and review the logs:
Installing from OperatorHub using the CLI
Instead of using the Red Hat OpenShift Container Platform web console, you can install an Operator from OperatorHub using the CLI. You can use the OpenShift CLI (oc) to install the MetalLB Operator.

It is recommended that when using the CLI you install the Operator in the metallb-system namespace.

A cluster installed on bare-metal hardware.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Create a namespace for the MetalLB Operator by entering the following command:

Create an Operator group custom resource (CR) in the namespace:

Confirm the Operator group is installed in the namespace:

Create a Subscription CR:

Optional: To ensure BGP and BFD metrics appear in Prometheus, you can label the namespace as in the following command:


The verification steps assume the MetalLB Operator is installed in the metallb-system namespace.

Confirm the install plan is in the namespace:

To verify that the Operator is installed, enter the following command:
Starting MetalLB on your cluster
After you install the Operator, you need to configure a single instance of a MetalLB custom resource. After you configure the custom resource, the Operator starts MetalLB on your cluster.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.

Install the MetalLB Operator.


This procedure assumes the MetalLB Operator is installed in the metallb-system namespace. If you installed using the web console substitute openshift-operators for the namespace.

Create a single instance of a MetalLB custom resource:


Confirm that the deployment for the MetalLB controller and the daemon set for the MetalLB speaker are running.

Verify that the deployment for the controller is running:

Verify that the daemon set for the speaker is running:
Deployment specifications for MetalLB
When you start an instance of MetalLB using the MetalLB custom resource, you can configure deployment specifications in the MetalLB custom resource to manage how the controller or speaker pods deploy and run in your cluster. Use these deployment specifications to manage the following tasks:

Select nodes for MetalLB pod deployment.

Manage scheduling by using pod priority and pod affinity.

Assign CPU limits for MetalLB pods.

Assign a container RuntimeClass for MetalLB pods.

Assign metadata for MetalLB pods.


Limit speaker pods to specific nodes
By default, when you start MetalLB with the MetalLB Operator, the Operator starts an instance of a speaker pod on each node in the cluster. Only the nodes with a speaker pod can advertise a load balancer IP address. You can configure the MetalLB custom resource with a node selector to specify which nodes run the speaker pods.

The most common reason to limit the speaker pods to specific nodes is to ensure that only nodes with network interfaces on specific networks advertise load balancer IP addresses. Only the nodes with a running speaker pod are advertised as destinations of the load balancer IP address.

If you limit the speaker pods to specific nodes and specify local for the external traffic policy of a service, then you must ensure that the application pods for the service are deployed to the same nodes.

apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:  1
    node-role.kubernetes.io/worker: ""
  speakerTolerations:   2
  - key: "Example"
    operator: "Exists"
    effect: "NoExecute"
The example configuration specifies to assign the speaker pods to worker nodes, but you can specify labels that you assigned to nodes or any valid node selector.

In this example configuration, the pod that this toleration is attached to tolerates any taint that matches the key value and effect value using the operator.


After you apply a manifest with the spec.nodeSelector field, you can check the number of pods that the Operator deployed with the oc get daemonset -n metallb-system speaker command. Similarly, you can display the nodes that match your labels with a command like oc get nodes -l node-role.kubernetes.io/worker=.

You can optionally allow the node to control which speaker pods should, or should not, be scheduled on them by using affinity rules. You can also limit these pods by applying a list of tolerations. For more information about affinity rules, taints, and tolerations, see the additional resources.
Configuring a container runtime class in a MetalLB deployment
You can optionally assign a container runtime class to controller and speaker pods by configuring the MetalLB custom resource. For example, for Windows workloads, you can assign a Windows runtime class to the pod, which uses this runtime class for all containers in the pod.

You are logged in as a user with cluster-admin privileges.

You have installed the MetalLB Operator.


Create a RuntimeClass custom resource, such as myRuntimeClass.yaml, to define your runtime class:

Apply the RuntimeClass custom resource configuration:

Create a MetalLB custom resource, such as MetalLBRuntime.yaml, to specify the runtimeClassName value:

Apply the MetalLB custom resource configuration:


To view the container runtime for a pod, run the following command:
Configuring pod priority and pod affinity in a MetalLB deployment
You can optionally assign pod priority and pod affinity rules to controller and speaker pods by configuring the MetalLB custom resource. The pod priority indicates the relative importance of a pod on a node and schedules the pod based on this priority. Set a high priority on your controller or speaker pod to ensure scheduling priority over other pods on the node.

Pod affinity manages relationships among pods. Assign pod affinity to the controller or speaker pods to control on what node the scheduler places the pod in the context of pod relationships. For example, you can use pod affinity rules to ensure that certain pods are located on the same node or nodes, which can help improve network communication and reduce latency between those components.

You are logged in as a user with cluster-admin privileges.

You have installed the MetalLB Operator.

You have started the MetalLB Operator on your cluster.


Create a PriorityClass custom resource, such as myPriorityClass.yaml, to configure the priority level. This example defines a PriorityClass named high-priority with a value of 1000000. Pods that are assigned this priority class are considered higher priority during scheduling compared to pods with lower priority classes:

Apply the PriorityClass custom resource configuration:

Create a MetalLB custom resource, such as MetalLBPodConfig.yaml, to specify the priorityClassName and podAffinity values:

Apply the MetalLB custom resource configuration:


To view the priority class that you assigned to pods in the metallb-system namespace, run the following command:

To verify that the scheduler placed pods according to pod affinity rules, view the metadata for the pod's node or nodes by running the following command:
Configuring pod CPU limits in a MetalLB deployment
You can optionally assign pod CPU limits to controller and speaker pods by configuring the MetalLB custom resource. Defining CPU limits for the controller or speaker pods helps you to manage compute resources on the node. This ensures all pods on the node have the necessary compute resources to manage workloads and cluster housekeeping.

You are logged in as a user with cluster-admin privileges.

You have installed the MetalLB Operator.


Create a MetalLB custom resource file, such as CPULimits.yaml, to specify the cpu value for the controller and speaker pods:

Apply the MetalLB custom resource configuration:


To view compute resources for a pod, run the following command, replacing <pod_name> with your target pod:
Additional resources
Placing pods on specific nodes using node selectors

Understanding taints and tolerations

Understanding pod priority

Understanding pod affinity
Next steps
Configuring MetalLB address pools