Troubleshooting CRI-O container runtime issues

About CRI-O container runtime engine
CRI-O is a Kubernetes-native container engine implementation that integrates closely with the operating system to deliver an efficient and optimized Kubernetes experience. The CRI-O container engine runs as a systemd service on each Red Hat OpenShift Container Platform cluster node.

When container runtime issues occur, verify the status of the crio systemd service on each node. Gather CRI-O journald unit logs from nodes that have container runtime issues.
Verifying CRI-O runtime engine status
You can verify CRI-O container runtime engine status on each cluster node.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


Review CRI-O status by querying the crio systemd service on a node, within a debug pod.
Gathering CRI-O journald unit logs
If you experience CRI-O issues, you can obtain CRI-O journald unit logs from a node.

You have access to the cluster as a user with the cluster-admin role.

Your API service is still functional.

You have installed the OpenShift CLI (oc).

You have the fully qualified domain names of the control plane or control plane machines.


Gather CRI-O journald unit logs. The following example collects logs from all control plane nodes (within the cluster:

Gather CRI-O journald unit logs from a specific node:

If the API is not functional, review the logs using SSH instead. Replace <node>.<cluster_name>.<base_domain> with appropriate values:
Cleaning CRI-O storage
You can manually clear the CRI-O ephemeral storage if you experience the following issues:

A node cannot run on any pods and this error appears:

You cannot create a new container on a working node and the  “can’t stat lower layer” error appears:

Your node is in the NotReady state after a cluster upgrade or if you attempt to reboot it.

The container runtime implementation (crio) is not working properly.

You are unable to start a debug shell on the node using oc debug node/<node_name> because the container runtime instance (crio) is not working.


Follow this process to completely wipe the CRI-O storage and resolve the errors.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


Use cordon on the node. This is to avoid any workload getting scheduled if the node gets into the Ready status. You will know that scheduling is disabled when SchedulingDisabled is in your Status section:

Drain the node as the cluster-admin user:

When the node returns, connect back to the node via SSH or Console. Then connect to the root user:

Manually stop the kubelet:

Stop the containers and pods:

Manually stop the crio services:

After you run those commands, you can completely wipe the ephemeral storage:

Start the crio and kubelet service:

You will know if the clean up worked if the crio and kubelet services are started, and the node is in the Ready status:

Mark the node schedulable. You will know that the scheduling is enabled when SchedulingDisabled is no longer in status: