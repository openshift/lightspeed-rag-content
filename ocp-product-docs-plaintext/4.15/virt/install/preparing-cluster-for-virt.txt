Preparing your cluster for OpenShift Virtualization

Review this section before you install OpenShift Virtualization to ensure that your cluster meets the requirements.


Installation method considerations
You can use any installation method, including user-provisioned, installer-provisioned, or assisted installer, to deploy Red Hat OpenShift Container Platform. However, the installation method and the cluster topology might affect OpenShift Virtualization functionality, such as snapshots or live migration.
Red Hat OpenShift Data Foundation
If you deploy OpenShift Virtualization with Red Hat OpenShift Data Foundation, you must create a dedicated storage class for Windows virtual machine disks. See Optimizing ODF PersistentVolumes for Windows VMs for details.
IPv6
You cannot run OpenShift Virtualization on a single-stack IPv6 cluster.
If you install your cluster in FIPS mode, no additional setup is required for OpenShift Virtualization.
Supported platforms
You can use the following platforms with OpenShift Virtualization:

On-premise bare metal servers. See Planning a bare metal cluster for OpenShift Virtualization.

IBM Cloud(R) Bare Metal Servers. See Deploy OpenShift Virtualization on IBM Cloud(R) Bare Metal nodes.


Bare metal instances or servers offered by other cloud providers are not supported.

OpenShift Virtualization on AWS bare metal
You can run OpenShift Virtualization on an Amazon Web Services (AWS) bare-metal Red Hat OpenShift Container Platform cluster.

OpenShift Virtualization is also supported on Red Hat OpenShift Service on AWS (ROSA) Classic clusters, which have the same configuration requirements as AWS bare-metal clusters.
Before you set up your cluster, review the following summary of supported features and limitations:


Installing


You can install the cluster by using installer-provisioned infrastructure, ensuring that you specify bare-metal instance types for the worker nodes by editing the install-config.yaml file. For example, you can use the c5n.metal type value for a machine based on x86_64 architecture.

Accessing virtual machines (VMs)


There is no change to how you access VMs by using the virtctl CLI tool or the Red Hat OpenShift Container Platform web console.

You can expose VMs by using a NodePort or LoadBalancer service.

Networking


You cannot use Single Root I/O Virtualization (SR-IOV) or bridge Container Network Interface (CNI) networks, including virtual LAN (VLAN). If your application requires a flat layer 2 network or control over the IP pool, consider using OVN-Kubernetes secondary overlay networks.

Storage


You can use any storage solution that is certified by the storage vendor to work with the underlying platform.


Amazon Elastic File System (EFS) and Amazon Elastic Block Store (EBS) are not recommended for use with OpenShift Virtualization due to performance and functionality limitations. Use shared storage instead.
Connecting a virtual machine to an OVN-Kubernetes secondary network

Exposing a virtual machine by using a service
Hardware and operating system requirements
Review the following hardware and operating system requirements for OpenShift Virtualization.

CPU requirements
Supported by Red Hat Enterprise Linux (RHEL) 9.

Support for AMD and Intel 64-bit architectures (x86-64-v2).

Support for Intel 64 or AMD64 CPU extensions.

Intel VT or AMD-V hardware virtualization extensions enabled.

NX (no execute) flag enabled.
Operating system requirements
Red Hat Enterprise Linux CoreOS (RHCOS) installed on worker nodes.
Storage requirements
Supported by Red Hat OpenShift Container Platform. See Optimizing storage.

You must create a default OpenShift Virtualization or Red Hat OpenShift Container Platform storage class. The purpose of this is to address the unique storage needs of VM workloads and offer optimized performance, reliability, and user experience. If both OpenShift Virtualization and Red Hat OpenShift Container Platform default storage classes exist, the OpenShift Virtualization class takes precedence when creating VM disks.


To mark a storage class as the default for virtualization workloads, set the annotation storageclass.kubevirt.io/is-default-virt-class to "true".
If the storage provisioner supports snapshots, you must associate a VolumeSnapshotClass object with the default storage class.


About volume and access modes for virtual machine disks
If you use the storage API with known storage providers, the volume and access modes are selected automatically. However, if you use a storage class that does not have a storage profile, you must configure the volume and access mode.

For best results, use the ReadWriteMany (RWX) access mode and the Block volume mode. This is important for the following reasons:

ReadWriteMany (RWX) access mode is required for live migration.

The Block volume mode performs significantly better than the Filesystem volume mode. This is because the Filesystem volume mode uses more storage layers, including a file system layer and a disk image file. These layers are not necessary for VM disk storage.


You cannot live migrate virtual machines with the following configurations:

Storage volume with ReadWriteOnce (RWO) access mode

Passthrough features such as GPUs


Do not set the evictionStrategy field to LiveMigrate for these virtual machines.
Live migration requirements
Shared storage with ReadWriteMany (RWX) access mode.

Sufficient RAM and network bandwidth.

If the virtual machine uses a host model CPU, the nodes must support the virtual machine's host model CPU.

A dedicated Multus network for live migration is highly recommended. A dedicated network minimizes the effects of network saturation on tenant workloads during migration.
Physical resource overhead requirements
OpenShift Virtualization is an add-on to Red Hat OpenShift Container Platform and imposes additional overhead that you must account for when planning a cluster. Each cluster machine must accommodate the following overhead requirements in addition to the Red Hat OpenShift Container Platform requirements. Oversubscribing the physical resources in a cluster can affect performance.

The numbers noted in this documentation are based on Red Hat's test methodology and setup. These numbers can vary based on your own individual setup and environments.

Calculate the memory overhead values for OpenShift Virtualization by using the equations below.

Memory overhead per infrastructure node ≈ 150 MiB
Memory overhead per worker node ≈ 360 MiB
Additionally, OpenShift Virtualization environment resources require a total of 2179 MiB of RAM that is spread across all infrastructure nodes.

Memory overhead per virtual machine ≈ (1.002 × requested memory) \
              + 218 MiB \ 1
              + 8 MiB × (number of vCPUs) \ 2
              + 16 MiB × (number of graphics devices) \ 3
              + (additional memory overhead) 4
Required for the processes that run in the virt-launcher pod.

Number of virtual CPUs requested by the virtual machine.

Number of virtual graphics cards requested by the virtual machine.

Additional memory overhead:



Calculate the cluster processor overhead requirements for OpenShift Virtualization by using the equation below. The CPU overhead per virtual machine depends on your individual setup.

CPU overhead for infrastructure nodes ≈ 4 cores
OpenShift Virtualization increases the overall utilization of cluster level services such as logging, routing, and monitoring. To account for this workload, ensure that nodes that host infrastructure components have capacity allocated for 4 additional cores (4000 millicores) distributed across those nodes.

CPU overhead for worker nodes ≈ 2 cores + CPU overhead per virtual machine
Each worker node that hosts virtual machines must have capacity for 2 additional cores (2000 millicores) for OpenShift Virtualization management workloads in addition to the CPUs required for virtual machine workloads.

If dedicated CPUs are requested, there is a 1:1 impact on the cluster CPU overhead requirement. Otherwise, there are no specific rules about how many CPUs a virtual machine requires.


Use the guidelines below to estimate storage overhead requirements for your OpenShift Virtualization environment.

Aggregated storage overhead per node ≈ 10 GiB
10 GiB is the estimated on-disk storage impact for each node in the cluster when you install OpenShift Virtualization.

Storage overhead per virtual machine depends on specific requests for resource allocation within the virtual machine. The request could be for ephemeral storage on the node or storage resources hosted elsewhere in the cluster. OpenShift Virtualization does not currently allocate any additional ephemeral storage for the running container itself.

As a cluster administrator, if you plan to host 10 virtual machines in the cluster, each with 1 GiB of RAM and 2 vCPUs, the memory impact across the cluster is 11.68 GiB. The estimated on-disk storage impact for each node in the cluster is 10 GiB and the CPU impact for worker nodes that host virtual machine workloads is a minimum of 2 cores.
Single-node OpenShift differences
You can install OpenShift Virtualization on single-node OpenShift.

However, you should be aware that Single-node OpenShift does not support the following features:

High availability

Pod disruption

Live migration

Virtual machines or templates that have an eviction strategy configured


Glossary of common terms for Red Hat OpenShift Container Platform storage
Object maximums
You must consider the following tested object maximums when planning your cluster:

Red Hat OpenShift Container Platform object maximums.

OpenShift Virtualization object maximums.
Cluster high-availability options
You can configure one of the following high-availability (HA) options for your cluster:

Automatic high availability for installer-provisioned infrastructure (IPI) is available by deploying machine health checks.

Automatic high availability for both IPI and non-IPI is available by using the Node Health Check Operator on the Red Hat OpenShift Container Platform cluster to deploy the NodeHealthCheck controller. The controller identifies unhealthy nodes and uses a remediation provider, such as the Self Node Remediation Operator or Fence Agents Remediation  Operator, to remediate the unhealthy nodes. For more information on remediation, fencing, and maintaining nodes, see the Workload Availability for Red Hat OpenShift documentation.

High availability for any platform is available by using either a monitoring system or a qualified human to monitor node availability. When a node is lost, shut it down and run oc delete node <lost_node>.