OpenShift Virtualization runbooks

You can use the procedures in these runbooks to diagnose and resolve issues that trigger OpenShift Virtualization alerts.

OpenShift Virtualization alerts are displayed on the Virtualization -> Overview -> Overview tab in the web console.
CDIDataImportCronOutdated

This alert fires when DataImportCron cannot poll or import the latest disk image versions.

DataImportCron polls disk images, checking for the latest versions, and imports the images as persistent volume claims (PVCs). This process ensures that PVCs are updated to the latest version so that they can be used as reliable clone sources or golden images for virtual machines (VMs).

For golden images, latest refers to the latest operating system of the distribution. For other disk images, latest refers to the latest hash of the image that is available.


VMs might be created from outdated disk images.

VMs might fail to start because no source PVC is available for cloning.


Check the cluster for a default storage class:

Obtain the DataImportCron namespace and name:

If a default storage class is not defined on the cluster, check the
DataImportCron specification for a default storage class:

Obtain the name of the DataVolume associated with the DataImportCron
object:

Check the DataVolume log for error messages:

Set the CDI_NAMESPACE environment variable:

Check the cdi-deployment log for error messages:



Set a default storage class, either on the cluster or in the DataImportCron
specification, to poll and import golden images. The updated Containerized Data
Importer (CDI) will resolve the issue within a few seconds.

If the issue does not resolve itself, delete the data volumes associated
with the affected DataImportCron objects. The CDI will recreate the data
volumes with the default storage class.

If your cluster is installed in a restricted network environment, disable
the enableCommonBootImageImport feature gate in order to opt out of automatic
updates:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDIDataVolumeUnusualRestartCount

This alert fires when a DataVolume object restarts more than three times.


Data volumes are responsible for importing and creating a virtual machine disk on a persistent volume claim. If a data volume restarts more than three times, these operations are unlikely to succeed. You must diagnose and resolve the issue.


Find Containerized Data Importer (CDI) pods with more than three restarts:

Obtain the details of the pods:

Check the pod logs for error messages:



Delete the data volume, resolve the issue, and create a new data volume.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the Diagnosis procedure.
CDIDefaultStorageClassDegraded

This alert fires when there is no default storage class that supports smart cloning (CSI or snapshot-based) or the ReadWriteMany access mode.


If the default storage class does not support smart cloning, the default cloning method is host-assisted cloning, which is much less efficient.

If the default storage class does not support ReadWriteMany, virtual machines (VMs) cannot be live migrated.

A default OpenShift Virtualization storage class has precedence over a
default "Red Hat OpenShift Container Platform" storage class when creating a
VM disk.

Get the default OpenShift Virtualization storage class by running the following
command:

If a default OpenShift Virtualization storage class exists, check that it
supports ReadWriteMany by running the following command:

If there is no default OpenShift Virtualization storage class, get the
default "Red Hat OpenShift Container Platform" storage class by running the following
command:

If a default "Red Hat OpenShift Container Platform" storage class exists, check that it
supports ReadWriteMany by running the following command:



Ensure that you have a default storage class, either "Red Hat OpenShift Container Platform" or OpenShift Virtualization, and that the default storage class supports smart cloning and ReadWriteMany.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDIMultipleDefaultVirtStorageClasses

This alert fires when more than one storage class has the annotation storageclass.kubevirt.io/is-default-virt-class: "true".


The storageclass.kubevirt.io/is-default-virt-class: "true" annotation defines a default OpenShift Virtualization storage class.

If more than one default OpenShift Virtualization storage class is defined, a data volume with no storage class specified receives the most recently created default storage class.


Obtain a list of default OpenShift Virtualization storage classes by running the following command:

$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-virt-class=="true")].metadata.name}'

Ensure that only one default OpenShift Virtualization storage class is defined by removing the annotation from the other storage classes.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDINoDefaultStorageClass

This alert fires when no default "Red Hat OpenShift Container Platform" or OpenShift Virtualization storage class is defined.


If no default "Red Hat OpenShift Container Platform" or OpenShift Virtualization storage class is defined, a data volume requesting a default storage class (the storage class is not specified), remains in a "pending" state.


Check for a default "Red Hat OpenShift Container Platform" storage class by running
the following command:

Check for a default OpenShift Virtualization storage class by running
the following command:



Create a default storage class for either "Red Hat OpenShift Container Platform" or OpenShift Virtualization or for both.

A default OpenShift Virtualization storage class has precedence over a default "Red Hat OpenShift Container Platform" storage class for creating a virtual machine disk image.

Create a default "Red Hat OpenShift Container Platform" storage class by running
the following command:

Create a default OpenShift Virtualization storage class by running
the following command:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDINotReady

This alert fires when the Containerized Data Importer (CDI) is in a degraded state:

Not progressing

Not available to use



CDI is not usable, so users cannot build virtual machine disks on persistent volume claims (PVCs) using CDI's data volumes. CDI components are not ready and they stopped progressing towards a ready state.


Set the CDI_NAMESPACE environment variable:

Check the CDI deployment for components that are not ready:

Check the details of the failing pod:

Check the logs of the failing pod:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDIOperatorDown

This alert fires when the Containerized Data Importer (CDI) Operator is down. The CDI Operator deploys and manages the CDI infrastructure components, such as data volume and persistent volume claim (PVC) controllers. These controllers help users build virtual machine disks on PVCs.


The CDI components might fail to deploy or to stay in a required state. The CDI installation might not function correctly.


Set the CDI_NAMESPACE environment variable:

Check whether the cdi-operator pod is currently running:

Obtain the details of the cdi-operator pod:

Check the log of the cdi-operator pod for errors:



If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CDIStorageProfilesIncomplete

This alert fires when a Containerized Data Importer (CDI) storage profile is incomplete.

If a storage profile is incomplete, the CDI cannot infer persistent volume claim (PVC) fields, such as volumeMode and  accessModes, which are required to create a virtual machine (VM) disk.


The CDI cannot create a VM disk on the PVC.


Identify the incomplete storage profile:



Add the missing storage profile information as in the following
example:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
CnaoDown

This alert fires when the Cluster Network Addons Operator (CNAO) is down. The CNAO deploys additional networking components on top of the cluster.


If the CNAO is not running, the cluster cannot reconcile changes to virtual machine components. As a result, the changes might fail to take effect.


Set the NAMESPACE environment variable:

Check the status of the cluster-network-addons-operator pod:

Check the cluster-network-addons-operator logs for error messages:

Obtain the details of the cluster-network-addons-operator pods:



If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
HCOInstallationIncomplete

This alert fires when the HyperConverged Cluster Operator (HCO) runs for more than an hour without a HyperConverged custom resource (CR).

This alert has the following causes:

During the installation process, you installed the HCO but you did not
create the HyperConverged CR.

During the uninstall process, you removed the HyperConverged CR before
uninstalling the HCO and the HCO is still running.



The mitigation depends on whether you are installing or uninstalling the HCO:

Complete the installation by creating a HyperConverged CR with its
default values:

Uninstall the HCO. If the uninstall process continues to run, you must
resolve that issue in order to cancel the alert.
HPPNotReady

This alert fires when a hostpath provisioner (HPP) installation is in a degraded state.

The HPP dynamically provisions hostpath volumes to provide storage for persistent volume claims (PVCs).


HPP is not usable. Its components are not ready and they are not progressing towards a ready state.


Set the HPP_NAMESPACE environment variable:

Check for HPP components that are currently not ready:

Obtain the details of the failing pod:

Check the logs of the failing pod:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
HPPOperatorDown

This alert fires when the hostpath provisioner (HPP) Operator is down.

The HPP Operator deploys and manages the HPP infrastructure components, such as the daemon set that provisions hostpath volumes.


The HPP components might fail to deploy or to remain in the required state. As a result, the HPP installation might not work correctly in the cluster.


Configure the HPP_NAMESPACE environment variable:

Check whether the hostpath-provisioner-operator pod is currently running:

Obtain the details of the hostpath-provisioner-operator pod:

Check the log of the hostpath-provisioner-operator pod for errors:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
HPPSharingPoolPathWithOS

This alert fires when the hostpath provisioner (HPP) shares a file system with other critical components, such as kubelet or the operating system (OS).

HPP dynamically provisions hostpath volumes to provide storage for persistent volume claims (PVCs).


A shared hostpath pool puts pressure on the node's disks. The node might have degraded performance and stability.


Configure the HPP_NAMESPACE environment variable:

Obtain the status of the hostpath-provisioner-csi daemon set
pods:

Check the hostpath-provisioner-csi logs to identify the shared
pool and path:



Using the data obtained in the Diagnosis section, try to prevent the pool path from being shared with the OS. The specific steps vary based on the node and other circumstances.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
KubemacpoolDown

KubeMacPool is down. KubeMacPool is responsible for allocating MAC addresses and preventing MAC address conflicts.


If KubeMacPool is down, VirtualMachine objects cannot be created.


Set the KMP_NAMESPACE environment variable:

Set the KMP_NAME environment variable:

Obtain the KubeMacPool-manager pod details:

Check the KubeMacPool-manager logs for error messages:



If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
KubeMacPoolDuplicateMacsFound

This alert fires when KubeMacPool detects duplicate MAC addresses.

KubeMacPool is responsible for allocating MAC addresses and preventing MAC address conflicts. When KubeMacPool starts, it scans the cluster for the MAC addresses of virtual machines (VMs) in managed namespaces.


Duplicate MAC addresses on the same LAN might cause network issues.


Obtain the namespace and the name of the kubemacpool-mac-controller pod:

Obtain the duplicate MAC addresses from the kubemacpool-mac-controller
logs:



Update the VMs to remove the duplicate MAC addresses.

Restart the kubemacpool-mac-controller pod:
KubeVirtComponentExceedsRequestedCPU

This alert fires when a component's CPU usage exceeds the requested limit.


Usage of CPU resources is not optimal and the node might be overloaded.


Set the NAMESPACE environment variable:

Check the component's CPU request limit:

Check the actual CPU usage by using a PromQL query:


See the Prometheus documentation for more information.


Update the CPU request limit in the HCO custom resource.
KubeVirtComponentExceedsRequestedMemory

This alert fires when a component's memory usage exceeds the requested limit.


Usage of memory resources is not optimal and the node might be overloaded.


Set the NAMESPACE environment variable:

Check the component's memory request limit:

Check the actual memory usage by using a PromQL query:


See the Prometheus documentation for more information.


Update the memory request limit in the HCO custom resource.
KubeVirtCRModified

This alert fires when an operand of the HyperConverged Cluster Operator (HCO) is changed by someone or something other than HCO.

HCO configures OpenShift Virtualization and its supporting operators in an opinionated way and overwrites its operands when there is an unexpected change to them. Users must not modify the operands directly. The HyperConverged custom resource is the source of truth for the configuration.


Changing the operands manually causes the cluster configuration to fluctuate and might lead to instability.


Check the component_name value in the alert details to determine the operand
kind (kubevirt) and the operand name (kubevirt-kubevirt-hyperconverged)
that are being changed:



Do not change the HCO operands directly. Use HyperConverged objects to configure the cluster.

The alert resolves itself after 10 minutes if the operands are not changed manually.
KubeVirtDeprecatedAPIRequested

This alert fires when a deprecated KubeVirt API is used.


Using a deprecated API is not recommended because the request will fail when the API is removed in a future release.


Check the Description and Summary sections of the alert to identify the
deprecated API as in the following example:



Use fully supported APIs. The alert resolves itself after 10 minutes if the deprecated API is not used.
KubeVirtNoAvailableNodesToRunVMs

This alert fires when the node CPUs in the cluster do not support virtualization or the virtualization extensions are not enabled.


The nodes must support virtualization and the virtualization features must be enabled in the BIOS to run virtual machines (VMs).


Check the nodes for hardware virtualization support:



Ensure that hardware and CPU virtualization extensions are enabled on all nodes and that the nodes are correctly labeled.

See OpenShift Virtualization reports no nodes are available, cannot start VMs for details.

If you cannot resolve the issue, log in to the Customer Portal and open a support case.
KubevirtVmHighMemoryUsage

This alert fires when a container hosting a virtual machine (VM) has less than 20 MB free memory.


The virtual machine running inside the container is terminated by the runtime if the container's memory limit is exceeded.


Obtain the virt-launcher pod details:

Identify compute container processes with high memory usage in the
virt-launcher pod:



Increase the memory limit in the VirtualMachine specification as in
the following example:
KubeVirtVMIExcessiveMigrations

This alert fires when a virtual machine instance (VMI) live migrates more than 12 times over a period of 24 hours.

This migration rate is abnormally high, even during an upgrade. This alert might indicate a problem in the cluster infrastructure, such as network disruptions or insufficient resources.


A virtual machine (VM) that migrates too frequently might experience degraded performance because memory page faults occur during the transition.


Verify that the worker node has sufficient resources:

Check the status of the worker node:

Log in to the worker node and verify that the kubelet service is running:

Check the kubelet journal log for error messages:



Ensure that the worker nodes have sufficient resources (CPU, memory, disk) to run VM workloads without interruption.

If the problem persists, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
LowKVMNodesCount

This alert fires when fewer than two nodes in the cluster have KVM resources.


The cluster must have at least two nodes with KVM resources for live migration.

Virtual machines cannot be scheduled or run if no nodes have KVM resources.


Identify the nodes with KVM resources:



Install KVM on the nodes without KVM resources.
LowReadyVirtControllersCount

This alert fires when one or more virt-controller pods are running, but none of these pods has been in the Ready state for the past 5 minutes.

A virt-controller device monitors the custom resource definitions (CRDs) of a virtual machine instance (VMI) and manages the associated pods. The device creates pods for VMIs and manages their lifecycle. The device is critical for cluster-wide virtualization functionality.


This alert indicates that a cluster-level failure might occur. Actions related to VM lifecycle management, such as launching a new VMI or shutting down an existing VMI, will fail.


Set the NAMESPACE environment variable:

Verify a virt-controller device is available:

Check the status of the virt-controller deployment:

Obtain the details of the virt-controller deployment to check for
status conditions, such as crashing pods or failures to pull images:

Check if any problems occurred with the nodes. For example, they might
be in a NotReady state:



This alert can have multiple causes, including the following:

The cluster has insufficient memory.

The nodes are down.

The API server is overloaded. For example, the scheduler might be under
a heavy load and therefore not completely available.

There are network issues.


Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
LowReadyVirtOperatorsCount

This alert fires when one or more virt-operator pods are running, but none of these pods has been in a Ready state for the last 10 minutes.

The virt-operator is the first Operator to start in a cluster. The virt-operator deployment has a default replica of two virt-operator pods.

Its primary responsibilities include the following:

Installing, live-updating, and live-upgrading a cluster

Monitoring the lifecycle of top-level controllers, such as virt-controller,
virt-handler, virt-launcher, and managing their reconciliation

Certain cluster-wide tasks, such as certificate rotation and infrastructure
management



A cluster-level failure might occur. Critical cluster-wide management functionalities, such as certification rotation, upgrade, and reconciliation of controllers, might become unavailable. Such a state also triggers the NoReadyVirtOperator alert.

The virt-operator is not directly responsible for virtual machines (VMs) in the cluster. Therefore, its temporary unavailability does not significantly affect VM workloads.


Set the NAMESPACE environment variable:

Obtain the name of the virt-operator deployment:

Obtain the details of the virt-operator deployment:

Check for node issues, such as a NotReady state:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
LowVirtAPICount

This alert fires when only one available virt-api pod is detected during a 60-minute period, although at least two nodes are available for scheduling.


An API call outage might occur during node eviction because the virt-api pod becomes a single point of failure.


Set the NAMESPACE environment variable:

Check the number of available virt-api pods:

Check the status of the virt-api deployment for error conditions:

Check the nodes for issues such as nodes in a NotReady state:



Try to identify the root cause and to resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
LowVirtControllersCount

This alert fires when a low number of virt-controller pods is detected. At least one virt-controller pod must be available in order to ensure high availability. The default number of replicas is 2.

A virt-controller device monitors the custom resource definitions (CRDs) of a virtual machine instance (VMI) and manages the associated pods. The device create pods for VMIs and manages the lifecycle of the pods. The device is critical for cluster-wide virtualization functionality.


The responsiveness of OpenShift Virtualization might become negatively affected. For example, certain requests might be missed.

In addition, if another virt-launcher instance terminates unexpectedly, OpenShift Virtualization might become completely unresponsive.


Set the NAMESPACE environment variable:

Verify that running virt-controller pods are available:

Check the virt-launcher logs for error messages:

Obtain the details of the virt-launcher pod to check for status conditions
such as unexpected termination or a NotReady state.



This alert can have a variety of causes, including:

Not enough memory on the cluster

Nodes are down

The API server is overloaded. For example, the scheduler might be under a
heavy load and therefore not completely available.

Networking issues


Identify the root cause and fix it, if possible.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
LowVirtOperatorCount

This alert fires when only one virt-operator pod in a Ready state has been running for the last 60 minutes.

The virt-operator is the first Operator to start in a cluster. Its primary responsibilities include the following:

Installing, live-updating, and live-upgrading a cluster

Monitoring the lifecycle of top-level controllers, such as virt-controller,
virt-handler, virt-launcher, and managing their reconciliation

Certain cluster-wide tasks, such as certificate rotation and infrastructure
management



The virt-operator cannot provide high availability (HA) for the deployment. HA requires two or more virt-operator pods in a Ready state. The default deployment is two pods.

The virt-operator is not directly responsible for virtual machines (VMs) in the cluster. Therefore, its decreased availability does not significantly affect VM workloads.


Set the NAMESPACE environment variable:

Check the states of the virt-operator pods:

Review the logs of the affected virt-operator pods:

Obtain the details of the affected virt-operator pods:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the Diagnosis procedure.
NetworkAddonsConfigNotReady

This alert fires when the NetworkAddonsConfig custom resource (CR) of the Cluster Network Addons Operator (CNAO) is not ready.

CNAO deploys additional networking components on the cluster. This alert indicates that one of the deployed components is not ready.


Network functionality is affected.


Check the status conditions of the NetworkAddonsConfig CR to identify the
deployment or daemon set that is not ready:

Check the component's pod for errors:

Check the component's logs:

Check the component's details for error conditions:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
NoLeadingVirtOperator

This alert fires when no virt-operator pod with a leader lease has been detected for 10 minutes, although the virt-operator pods are in a Ready state. The alert indicates that no leader pod is available.

The virt-operator is the first Operator to start in a cluster. Its primary responsibilities include the following:

Installing, live updating, and live upgrading a cluster

Monitoring the lifecycle of top-level controllers, such as virt-controller,
virt-handler, virt-launcher, and managing their reconciliation

Certain cluster-wide tasks, such as certificate rotation and infrastructure
management


The virt-operator deployment has a default replica of 2 pods, with one pod holding a leader lease.


This alert indicates a failure at the level of the cluster. As a result, critical cluster-wide management functionalities, such as certification rotation, upgrade, and reconciliation of controllers, might not be available.


Set the NAMESPACE environment variable:

Obtain the status of the virt-operator pods:

Check the virt-operator pod logs to determine the leader status:

Obtain the details of the affected virt-operator pods:



Based on the information obtained during the diagnosis procedure, try to find the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
NoReadyVirtController

This alert fires when no available virt-controller devices have been detected for 5 minutes.

The virt-controller devices monitor the custom resource definitions of virtual machine instances (VMIs) and manage the associated pods. The devices create pods for VMIs and manage the lifecycle of the pods.

Therefore, virt-controller devices are critical for all cluster-wide virtualization functionality.


Any actions related to VM lifecycle management fail. This notably includes launching a new VMI or shutting down an existing VMI.


Set the NAMESPACE environment variable:

Verify the number of virt-controller devices:

Check the status of the virt-controller deployment:

Obtain the details of the virt-controller deployment to check for
status conditions such as crashing pods or failure to pull images:

Obtain the details of the virt-controller pods:

Check the logs of the virt-controller pods for error messages:

Check the nodes for problems, such as a NotReady state:



Based on the information obtained during the diagnosis procedure, try to find the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
NoReadyVirtOperator

This alert fires when no virt-operator pod in a Ready state has been detected for 10 minutes.

The virt-operator is the first Operator to start in a cluster. Its primary responsibilities include the following:

Installing, live-updating, and live-upgrading a cluster

Monitoring the life cycle of top-level controllers, such as virt-controller,
virt-handler, virt-launcher, and managing their reconciliation

Certain cluster-wide tasks, such as certificate rotation and infrastructure
management


The default deployment is two virt-operator pods.


This alert indicates a cluster-level failure. Critical cluster management functionalities, such as certification rotation, upgrade, and reconciliation of controllers, might not be not available.

The virt-operator is not directly responsible for virtual machines in the cluster. Therefore, its temporary unavailability does not significantly affect workloads.


Set the NAMESPACE environment variable:

Obtain the name of the virt-operator deployment:

Generate the description of the virt-operator deployment:

Check for node issues, such as a NotReady state:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the Diagnosis procedure.
OrphanedVirtualMachineInstances

This alert fires when a virtual machine instance (VMI), or virt-launcher pod, runs on a node that does not have a running virt-handler pod. Such a VMI is called orphaned.


Orphaned VMIs cannot be managed.


Check the status of the virt-handler pods to view the nodes on
which they are running:

Check the status of the VMIs to identify VMIs running on nodes
that do not have a running virt-handler pod:

Check the status of the virt-handler daemon:

If the virt-handler daemon set is not healthy, check the virt-handler
daemon set for pod deployment issues:

Check the nodes for issues such as a NotReady status:

Check the spec.workloads stanza of the KubeVirt custom resource
(CR) for a workloads placement policy:



If a workloads placement policy is configured, add the node with the VMI to the policy.

Possible causes for the removal of a virt-handler pod from a node include changes to the node's taints and tolerations or to a pod's scheduling rules.

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
OutdatedVirtualMachineInstanceWorkloads

This alert fires when running virtual machine instances (VMIs) in outdated virt-launcher pods are detected 24 hours after the OpenShift Virtualization control plane has been updated.


Outdated VMIs might not have access to new OpenShift Virtualization features.

Outdated VMIs will not receive the security fixes associated with the virt-launcher pod update.


Identify the outdated VMIs:

Check the KubeVirt custom resource (CR) to determine whether
workloadUpdateMethods is configured in the workloadUpdateStrategy
stanza:

Check each outdated VMI to determine whether it is live-migratable:




Update the HyperConverged CR to enable automatic workload updates.


If a VMI is not live-migratable and if runStrategy: always is
set in the corresponding VirtualMachine object, you can update the
VMI by manually stopping the virtual machine (VM):


A new VMI spins up immediately in an updated virt-launcher pod to replace the stopped VMI. This is the equivalent of a restart action.

Manually stopping a live-migratable VM is destructive and
not recommended because it interrupts the workload.

If a VMI is live-migratable, you can update it by creating a VirtualMachineInstanceMigration object that targets a specific running VMI. The VMI is migrated into an updated virt-launcher pod.

Create a VirtualMachineInstanceMigration manifest and save it
as migration.yaml:

Create a VirtualMachineInstanceMigration object to trigger the
migration:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
SingleStackIPv6Unsupported

This alert fires when you install OpenShift Virtualization on a single stack IPv6 cluster.


You cannot create virtual machines.


Check the cluster network configuration by running the following command:



Install OpenShift Virtualization on a single stack IPv4 cluster or on a dual stack IPv4/IPv6 cluster.
SSPCommonTemplatesModificationReverted

This alert fires when the Scheduling, Scale, and Performance (SSP) Operator reverts changes to common templates as part of its reconciliation procedure.

The SSP Operator deploys and reconciles the common templates and the Template Validator. If a user or script changes a common template, the changes are reverted by the SSP Operator.


Changes to common templates are overwritten.


Set the NAMESPACE environment variable:

Check the ssp-operator logs for templates with reverted changes:



Try to identify and resolve the cause of the changes.

Ensure that changes are made only to copies of templates, and not to the templates themselves.
SSPDown

This alert fires when all the Scheduling, Scale and Performance (SSP) Operator pods are down.

The SSP Operator is responsible for deploying and reconciling the common templates and the Template Validator.


Dependent components might not be deployed. Changes in the components might not be reconciled. As a result, the common templates and/or the Template Validator might not be updated or reset if they fail.


Set the NAMESPACE environment variable:

Check the status of the ssp-operator pods.

Obtain the details of the ssp-operator pods:

Check the ssp-operator logs for error messages:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
SSPFailingToReconcile

This alert fires when the reconcile cycle of the Scheduling, Scale and Performance (SSP) Operator fails repeatedly, although the SSP Operator is running.

The SSP Operator is responsible for deploying and reconciling the common templates and the Template Validator.


Dependent components might not be deployed. Changes in the components might not be reconciled. As a result, the common templates or the Template Validator might not be updated or reset if they fail.


Export the NAMESPACE environment variable:

Obtain the details of the ssp-operator pods:

Check the ssp-operator logs for errors:

Obtain the status of the virt-template-validator pods:

Obtain the details of the virt-template-validator pods:

Check the virt-template-validator logs for errors:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
SSPHighRateRejectedVms

This alert fires when a user or script attempts to create or modify a large number of virtual machines (VMs), using an invalid configuration.


The VMs are not created or modified. As a result, the environment might not behave as expected.


Export the NAMESPACE environment variable:

Check the virt-template-validator logs for errors that might indicate the
cause:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
SSPTemplateValidatorDown

This alert fires when all the Template Validator pods are down.

The Template Validator checks virtual machines (VMs) to ensure that they do not violate their templates.


VMs are not validated against their templates. As a result, VMs might be created with specifications that do not match their respective workloads.


Set the NAMESPACE environment variable:

Obtain the status of the virt-template-validator pods:

Obtain the details of the virt-template-validator pods:

Check the  virt-template-validator logs for error messages:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
UnsupportedHCOModification

This alert fires when a JSON Patch annotation is used to change an operand of the HyperConverged Cluster Operator (HCO).

HCO configures OpenShift Virtualization and its supporting operators in an opinionated way and overwrites its operands when there is an unexpected change to them. Users must not modify the operands directly.

However, if a change is required and it is not supported by the HCO API, you can force HCO to set a change in an operator by using JSON Patch annotations. These changes are not reverted by HCO during its reconciliation process.


Incorrect use of JSON Patch annotations might lead to unexpected results or an unstable environment.

Upgrading a system with JSON Patch annotations is dangerous because the structure of the component custom resources might change.


Check the annotation_name in the alert details to identify the JSON
Patch annotation:



It is best to use the HCO API to change an operand. However, if the change can only be done with a JSON Patch annotation, proceed with caution.

Remove JSON Patch annotations before upgrade to avoid potential issues.
VirtAPIDown

This alert fires when all the API Server pods are down.


OpenShift Virtualization objects cannot send API calls.


Set the NAMESPACE environment variable:

Check the status of the virt-api pods:

Check the status of the virt-api deployment:

Check the virt-api deployment details for issues such as crashing pods or
image pull failures:

Check for issues such as nodes in a NotReady state:



Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtApiRESTErrorsBurst

More than 80% of REST calls have failed in the virt-api pods in the last 5 minutes.


A very high rate of failed REST calls to virt-api might lead to slow response and execution of API calls, and potentially to API calls being completely dismissed.

However, currently running virtual machine workloads are not likely to be affected.


Set the NAMESPACE environment variable:

Obtain the list of virt-api pods on your deployment:

Check the virt-api logs for error messages:

Obtain the details of the virt-api pods:

Check if any problems occurred with the nodes. For example, they might
be in a NotReady state:

Check the status of the virt-api deployment:

Obtain the details of the virt-api deployment:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtApiRESTErrorsHigh

More than 5% of REST calls have failed in the virt-api pods in the last 60 minutes.


A high rate of failed REST calls to virt-api might lead to slow response and execution of API calls.

However, currently running virtual machine workloads are not likely to be affected.


Set the NAMESPACE environment variable as follows:

Check the status of the virt-api pods:

Check the virt-api logs:

Obtain the details of the virt-api pods:

Check if any problems occurred with the nodes. For example, they might be in
a NotReady state:

Check the status of the virt-api deployment:

Obtain the details of the virt-api deployment:



Based on the information obtained during the diagnosis procedure, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtControllerDown

No running virt-controller pod has been detected for 5 minutes.


Any actions related to virtual machine (VM) lifecycle management fail. This notably includes launching a new virtual machine instance (VMI) or shutting down an existing VMI.


Set the NAMESPACE environment variable:

Check the status of the virt-controller deployment:

Review the logs of the virt-controller pod:



This alert can have a variety of causes, including the following:

Node resource exhaustion

Not enough memory on the cluster

Nodes are down

The API server is overloaded. For example, the scheduler might be
under a heavy load and therefore not completely available.

Networking issues


Identify the root cause and fix it, if possible.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtControllerRESTErrorsBurst

More than 80% of REST calls in virt-controller pods failed in the last 5 minutes.

The virt-controller has likely fully lost the connection to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.

The virt-controller pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.



Status updates are not propagated and actions like migrations cannot take place. However, running workloads are not impacted.


Set the NAMESPACE environment variable:

List the available virt-controller pods:

Check the virt-controller logs for error messages when connecting to the
API server:



If the virt-controller pod cannot connect to the API server, delete the
pod to force a restart:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtControllerRESTErrorsHigh

More than 5% of REST calls failed in virt-controller in the last 60 minutes.

This is most likely because virt-controller has partially lost connection to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.

The virt-controller pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.



Node-related actions, such as starting and migrating, and scheduling virtual machines, are delayed. Running workloads are not affected, but reporting their current status might be delayed.


Set the NAMESPACE environment variable:

List the available virt-controller pods:

Check the virt-controller logs for error messages when connecting
to the API server:



If the virt-controller pod cannot connect to the API server, delete
the pod to force a restart:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtHandlerDaemonSetRolloutFailing

The virt-handler daemon set has failed to deploy on one or more worker nodes after 15 minutes.


This alert is a warning. It does not indicate that all virt-handler daemon sets have failed to deploy. Therefore, the normal lifecycle of virtual machines is not affected unless the cluster is overloaded.


Identify worker nodes that do not have a running virt-handler pod:

Export the NAMESPACE environment variable:

Check the status of the virt-handler pods to identify pods that have
not deployed:

Obtain the name of the worker node of the virt-handler pod:



If the virt-handler pods failed to deploy because of insufficient resources, you can delete other pods on the affected worker node.
VirtHandlerRESTErrorsBurst

More than 80% of REST calls failed in virt-handler in the last 5 minutes. This alert usually indicates that the virt-handler pods cannot connect to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.

The virt-handler pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.



Status updates are not propagated and node-related actions, such as migrations, fail. However, running workloads on the affected node are not impacted.


Set the NAMESPACE environment variable:

Check the status of the virt-handler pod:

Check the virt-handler logs for error messages when connecting to
the API server:



If the virt-handler cannot connect to the API server, delete the pod
to force a restart:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtHandlerRESTErrorsHigh

More than 5% of REST calls failed in virt-handler in the last 60 minutes. This alert usually indicates that the virt-handler pods have partially lost connection to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.

The virt-handler pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.



Node-related actions, such as starting and migrating workloads, are delayed on the node that virt-handler is running on. Running workloads are not affected, but reporting their current status might be delayed.


Set the NAMESPACE environment variable:

List the available virt-handler pods to identify the failing
virt-handler pod:

Check the failing virt-handler pod log for API server
connectivity errors:



Delete the pod to force a restart:

$ oc delete -n $NAMESPACE <virt-handler>
If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtOperatorDown

This alert fires when no virt-operator pod in the Running state has been detected for 10 minutes.

The virt-operator is the first Operator to start in a cluster. Its primary responsibilities include the following:

Installing, live-updating, and live-upgrading a cluster

Monitoring the life cycle of top-level controllers, such as virt-controller,
virt-handler, virt-launcher, and managing their reconciliation

Certain cluster-wide tasks, such as certificate rotation and infrastructure
management


The virt-operator deployment has a default replica of 2 pods.


This alert indicates a failure at the level of the cluster. Critical cluster-wide management functionalities, such as certification rotation, upgrade, and reconciliation of controllers, might not be available.

The virt-operator is not directly responsible for virtual machines (VMs) in the cluster. Therefore, its temporary unavailability does not significantly affect VM workloads.


Set the NAMESPACE environment variable:

Check the status of the virt-operator deployment:

Obtain the details of the virt-operator deployment:

Check the status of the virt-operator pods:

Check for node issues, such as a NotReady state:



Based on the information obtained during the diagnosis procedure, try to find the root cause and resolve the issue.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtOperatorRESTErrorsBurst

This alert fires when more than 80% of the REST calls in the virt-operator pods failed in the last 5 minutes. This usually indicates that the virt-operator pods cannot connect to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.

The virt-operator pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.



Cluster-level actions, such as upgrading and controller reconciliation, might not be available.

However, workloads such as virtual machines (VMs) and VM instances (VMIs) are not likely to be affected.


Set the NAMESPACE environment variable:

Check the status of the virt-operator pods:

Check the virt-operator logs for error messages when connecting to the
API server:

Obtain the details of the virt-operator pod:



If the virt-operator pod cannot connect to the API server, delete the pod
to force a restart:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VirtOperatorRESTErrorsHigh

This alert fires when more than 5% of the REST calls in virt-operator pods failed in the last 60 minutes. This usually indicates the virt-operator pods cannot connect to the API server.

This error is frequently caused by one of the following problems:

The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.

The virt-operator pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.



Cluster-level actions, such as upgrading and controller reconciliation, might be delayed.

However, workloads such as virtual machines (VMs) and VM instances (VMIs) are not likely to be affected.


Set the NAMESPACE environment variable:

Check the status of the virt-operator pods:

Check the virt-operator logs for error messages when connecting to the
API server:

Obtain the details of the virt-operator pod:



If the virt-operator pod cannot connect to the API server, delete the pod
to force a restart:


If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.
VMCannotBeEvicted

This alert fires when the eviction strategy of a virtual machine (VM) is set to LiveMigration but the VM is not migratable.


Non-migratable VMs prevent node eviction. This condition affects operations such as node drain and updates.


Check the VMI configuration to determine whether the value of
evictionStrategy is LiveMigrate:

Check for a False status in the LIVE-MIGRATABLE column to identify VMIs
that are not migratable:

Obtain the details of the VMI and check spec.conditions to identify the
issue:



Set the evictionStrategy of the VMI to shutdown or resolve the issue that prevents the VMI from migrating.
VMStorageClassWarning

This alert fires when the storage class is incorrectly configured. A system-wide, shared dummy page causes CRC errors when data is written and read across different processes or threads.


A large number of CRC errors might cause the cluster to display severe performance degradation.


Navigate to Observe -> Metrics in the web console.

Obtain a list of virtual machines with incorrectly configured storage classes
by running the following PromQL query:

Obtain the storage class name by running the following command:



Create a default OpenShift Virtualization storage class with the krbd:rxbounce map option. See  Optimizing ODF PersistentVolumes for Windows VMs for details.

If you cannot resolve the issue, log in to the Customer Portal and open a support case, attaching the artifacts gathered during the diagnosis procedure.