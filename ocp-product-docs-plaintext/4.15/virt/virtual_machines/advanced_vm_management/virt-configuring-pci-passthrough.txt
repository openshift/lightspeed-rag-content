Configuring PCI passthrough

The Peripheral Component Interconnect (PCI) passthrough feature enables you to access and manage hardware devices from a virtual machine (VM). When PCI passthrough is configured, the PCI devices function as if they were physically attached to the guest operating system.

Cluster administrators can expose and manage host devices that are permitted to be used in the cluster by using the oc command-line interface (CLI).
Preparing nodes for GPU passthrough
You can prevent GPU operands from deploying on worker nodes that you designated for GPU passthrough.

Preventing NVIDIA GPU operands from deploying on nodes
If you use the NVIDIA GPU Operator in your cluster, you can apply the nvidia.com/gpu.deploy.operands=false label to nodes that you do not want to configure for GPU or vGPU operands. This label prevents the creation of the pods that configure GPU or vGPU operands and terminates the pods if they already exist.

The OpenShift CLI (oc) is installed.


Label the node by running the following command:


Verify that the label was added to the node by running the following command:

Optional: If GPU operands were previously deployed on the node, verify their removal.
Preparing host devices for PCI passthrough
About preparing a host device for PCI passthrough
To prepare a host device for PCI passthrough by using the CLI, create a MachineConfig object and add kernel arguments to enable the Input-Output Memory Management Unit (IOMMU). Bind the PCI device to the Virtual Function I/O (VFIO) driver and then expose it in the cluster by editing the permittedHostDevices field of the HyperConverged custom resource (CR). The permittedHostDevices list is empty when you first install the OpenShift Virtualization Operator.

To remove a PCI host device from the cluster by using the CLI, delete the PCI device information from the HyperConverged CR.
Adding kernel arguments to enable the IOMMU driver
To enable the IOMMU driver in the kernel, create the MachineConfig object and add the kernel arguments.

You have cluster administrator permissions.

Your CPU hardware is Intel or AMD.

You enabled Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS.


Create a MachineConfig object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.

Create the new MachineConfig object:


Verify that the new MachineConfig object was added.
Binding PCI devices to the VFIO driver
To bind PCI devices to the VFIO (Virtual Function I/O) driver, obtain the values for vendor-ID and device-ID from each device and create a list with the values. Add this list to the MachineConfig object. The MachineConfig Operator generates the /etc/modprobe.d/vfio.conf on the nodes with the PCI devices, and binds the PCI devices to the VFIO driver.

You added kernel arguments to enable IOMMU for the CPU.


Run the lspci command to obtain the vendor-ID and the device-ID for the PCI device.

Create a Butane config file, 100-worker-vfiopci.bu, binding the PCI device to the VFIO driver.

Use Butane to generate a MachineConfig object file, 100-worker-vfiopci.yaml, containing the configuration to be delivered to the worker nodes:

Apply the MachineConfig object to the worker nodes:

Verify that the MachineConfig object was added.


Verify that the VFIO driver is loaded.
Exposing PCI host devices in the cluster using the CLI
To expose PCI host devices in the cluster, add details about the PCI devices to the spec.permittedHostDevices.pciHostDevices array of the HyperConverged custom resource (CR).

Edit the HyperConverged CR in your default editor by running the following command:

Add the PCI device information to the spec.permittedHostDevices.pciHostDevices array. For example:

Save your changes and exit the editor.


Verify that the PCI host devices were added to the node by running the following command. The example output shows that there is one device each associated with the nvidia.com/GV100GL_Tesla_V100, nvidia.com/TU104GL_Tesla_T4, and intel.com/qat resource names.
Removing PCI host devices from the cluster using the CLI
To remove a PCI host device from the cluster, delete the information for that device from the HyperConverged custom resource (CR).

Edit the HyperConverged CR in your default editor by running the following command:

Remove the PCI device information from the spec.permittedHostDevices.pciHostDevices array by deleting the pciDeviceSelector, resourceName and externalResourceProvider (if applicable) fields for the appropriate device. In this example, the intel.com/qat resource has been deleted.

Save your changes and exit the editor.


Verify that the PCI host device was removed from the node by running the following command. The example output shows that there are zero devices associated with the intel.com/qat resource name.
Configuring virtual machines for PCI passthrough
After the PCI devices have been added to the cluster, you can assign them to virtual machines. The PCI devices are now available as if they are physically connected to the virtual machines.

Assigning a PCI device to a virtual machine
When a PCI device is available in a cluster, you can assign it to a virtual machine and enable PCI passthrough.

Assign the PCI device to a virtual machine as a host device.


Use the following command to verify that the host device is available from the virtual machine.
Additional resources
Enabling Intel VT-X and AMD-V Virtualization Hardware Extensions in BIOS

Managing file permissions

Postinstallation machine configuration tasks