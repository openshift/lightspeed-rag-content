Installing log storage

You can use the OpenShift CLI (`oc`) or the Red Hat OpenShift Container Platform web console to deploy a log store on your Red Hat OpenShift Container Platform cluster.

The Logging subsystem 5.9 release does not contain an updated version of the Elasticsearch Operator. If you currently use the Elasticsearch Operator released with Logging subsystem 5.8, it will continue to work with Logging subsystem until the EOL of Logging subsystem 5.8. As an alternative to using the Elasticsearch Operator to manage the default log storage, you can use the Loki Operator. For more information on the Logging subsystem lifecycle dates, see Platform Agnostic Operators.
Deploying a Loki log store
You can use the Loki Operator to deploy an internal Loki log store on your Red Hat OpenShift Container Platform cluster. After install the Loki Operator, you must configure Loki object storage by creating a secret, and create a LokiStack custom resource (CR).

Loki deployment sizing
Sizing for Loki follows the format of 1x.<size> where the value 1x is number of instances and <size> specifies performance capabilities.

It is not possible to change the number 1x for the deployment size.
Installing the Loki Operator by using the Red Hat OpenShift Container Platform web console
To install and configure logging on your Red Hat OpenShift Container Platform cluster, additional Operators must be installed. This can be done from the Operator Hub within the web console.

Red Hat OpenShift Container Platform Operators use custom resources (CR) to manage applications and their components. High-level configuration and settings are provided by the user within a CR. The Operator translates high-level directives into low-level actions, based on best practices embedded within the Operator’s logic. A custom resource definition (CRD) defines a CR and lists all the configurations available to users of the Operator. Installing an Operator creates the CRDs, which are then used to generate CRs.

You have access to a supported object store (AWS S3, Google Cloud Storage, Azure, Swift, Minio, OpenShift Data Foundation).

You have administrator permissions.

You have access to the Red Hat OpenShift Container Platform web console.


In the Red Hat OpenShift Container Platform web console Administrator perspective, go to Operators -> OperatorHub.

Type Loki Operator in the Filter by keyword field. Click Loki Operator in the list of available Operators, and then click Install.

Select stable or stable-x.y as the Update channel.

Select Enable operator-recommended cluster monitoring on this namespace.

For Update approval select Automatic, then click Install.


Go to Operators -> Installed Operators.

Make sure the openshift-logging project is selected.

In the Status column, verify that you see green checkmarks with InstallSucceeded and the text Up to date.


An Operator might display a Failed status before the installation finishes. If the Operator install completes with an InstallSucceeded message, refresh the page.
Creating a secret for Loki object storage by using the web console
To configure Loki object storage, you must create a secret. You can create a secret by using the Red Hat OpenShift Container Platform web console.

You have administrator permissions.

You have access to the Red Hat OpenShift Container Platform web console.

You installed the Loki Operator.


Go to Workloads -> Secrets in the Administrator perspective of the Red Hat OpenShift Container Platform web console.

From the Create drop-down list, select From YAML.

Create a secret that uses the access_key_id and access_key_secret fields to specify your credentials and the bucketnames, endpoint, and region fields to define the object storage location. AWS is used in the following example:


Loki object storage
Workload identity federation
Workload identity federation enables authentication to cloud-based log stores using short-lived tokens.

Red Hat OpenShift Container Platform 4.14 and later

Logging subsystem 5.9 and later


If you use the Red Hat OpenShift Container Platform web console to install the Loki Operator, clusters that use short-lived tokens are automatically detected. You are prompted to create roles and supply the data required for the Loki Operator to create a CredentialsRequest object, which populates a secret.

If you use the OpenShift CLI (`oc`) to install the Loki Operator, you must manually create a subscription object using the appropriate template for your storage provider, as shown in the following examples. This authentication strategy is only supported for the storage providers indicated.


apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat
spec:
  channel: "stable-5.9"
  installPlanApproval: Manual
  name: loki-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  config:
    env:
      - name: CLIENTID
        value: <your_client_id>
      - name: TENANTID
        value: <your_tenant_id>
      - name: SUBSCRIPTIONID
        value: <your_subscription_id>
      - name: REGION
        value: <your_region>
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat
spec:
  channel: "stable-5.9"
  installPlanApproval: Manual
  name: loki-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  config:
    env:
    - name: ROLEARN
      value: <role_ARN>
Creating a LokiStack custom resource by using the web console
You can create a LokiStack custom resource (CR) by using the Red Hat OpenShift Container Platform web console.

You have administrator permissions.

You have access to the Red Hat OpenShift Container Platform web console.

You installed the Loki Operator.


Go to the Operators -> Installed Operators page. Click the All instances tab.

From the Create new drop-down list, select LokiStack.

Select YAML view, and then use the following template to create a LokiStack CR:
Installing Loki Operator by using the CLI
To install and configure logging on your Red Hat OpenShift Container Platform cluster, additional Operators must be installed. This can be done from the Red Hat OpenShift Container Platform CLI.

Red Hat OpenShift Container Platform Operators use custom resources (CR) to manage applications and their components. High-level configuration and settings are provided by the user within a CR. The Operator translates high-level directives into low-level actions, based on best practices embedded within the Operator’s logic. A custom resource definition (CRD) defines a CR and lists all the configurations available to users of the Operator. Installing an Operator creates the CRDs, which are then used to generate CRs.

You have administrator permissions.

You installed the OpenShift CLI (`oc`).

You have access to a supported object store. For example: AWS S3, Google Cloud Storage, Azure, Swift, Minio, or OpenShift Data Foundation.


Create a Subscription object:

Apply the Subscription object:
Creating a secret for Loki object storage by using the CLI
To configure Loki object storage, you must create a secret. You can do this by using the OpenShift CLI (`oc`).

You have administrator permissions.

You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).


Create a secret in the directory that contains your certificate and key files by running the following command:


Use generic or opaque secrets for best results.
Verify that a secret was created by running the following command:


Loki object storage
Creating a LokiStack custom resource by using the CLI
You can create a LokiStack custom resource (CR) by using the OpenShift CLI (`oc`).

You have administrator permissions.

You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).


Create a LokiStack CR:

Apply the LokiStack CR by running the following command:


Verify the installation by listing the pods in the openshift-logging project by running the following command and observing the output:
Loki object storage
The Loki Operator supports AWS S3, as well as other S3 compatible object stores such as Minio and OpenShift Data Foundation. Azure, GCS, and Swift are also supported.

The recommended nomenclature for Loki storage is logging-loki-<your_storage_provider>.

The following table shows the type values within the LokiStack custom resource (CR) for each storage provider. For more information, see the section on your storage provider.


AWS storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You created a bucket on AWS.

You created an AWS IAM Policy and IAM User.


Create an object storage secret with the name logging-loki-aws by running the following command:


AWS storage for STS enabled clusters
If your cluster has STS enabled, the Cloud Credential Operator (CCO) supports short-term authentication using AWS tokens.

You can create the Loki object storage secret manually by running the following command:

$ oc -n openshift-logging create secret generic "logging-loki-aws" \
--from-literal=bucketnames="<s3_bucket_name>" \
--from-literal=region="<bucket_region>" \
--from-literal=audience="<oidc_audience>" 1
Optional annotation, default value is openshift.
Azure storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You created a bucket on Azure.


Create an object storage secret with the name logging-loki-azure by running the following command:


Azure storage for Microsoft Entra Workload ID enabled clusters
If your cluster has Microsoft Entra Workload ID enabled, the Cloud Credential Operator (CCO) supports short-term authentication using Workload ID.

You can create the Loki object storage secret manually by running the following command:

$ oc -n openshift-logging create secret generic logging-loki-azure \
--from-literal=environment="<azure_environment>" \
--from-literal=account_name="<storage_account_name>" \
--from-literal=container="<container_name>"
Google Cloud Platform storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You created a project on Google Cloud Platform (GCP).

You created a bucket in the same project.

You created a service account in the same project for GCP authentication.


Copy the service account credentials received from GCP into a file called key.json.

Create an object storage secret with the name logging-loki-gcs by running the following command:
Minio storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You have Minio deployed on your cluster.

You created a bucket on Minio.


Create an object storage secret with the name logging-loki-minio by running the following command:
OpenShift Data Foundation storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You deployed OpenShift Data Foundation.

You configured your OpenShift Data Foundation cluster for object storage.


Create an ObjectBucketClaim custom resource in the openshift-logging namespace:

Get bucket properties from the associated ConfigMap object by running the following command:

Get bucket access key from the associated secret by running the following command:

Create an object storage secret with the name logging-loki-odf by running the following command:
Swift storage
You installed the Loki Operator.

You installed the OpenShift CLI (`oc`).

You created a bucket on Swift.


Create an object storage secret with the name logging-loki-swift by running the following command:

You can optionally provide project-specific data, region, or both by running the following command:
Deploying an Elasticsearch log store
You can use the Elasticsearch Operator to deploy an internal Elasticsearch log store on your Red Hat OpenShift Container Platform cluster.

The Logging subsystem 5.9 release does not contain an updated version of the Elasticsearch Operator. If you currently use the Elasticsearch Operator released with Logging subsystem 5.8, it will continue to work with Logging subsystem until the EOL of Logging subsystem 5.8. As an alternative to using the Elasticsearch Operator to manage the default log storage, you can use the Loki Operator. For more information on the Logging subsystem lifecycle dates, see Platform Agnostic Operators.
Storage considerations for Elasticsearch
A persistent volume is required for each Elasticsearch deployment configuration. On Red Hat OpenShift Container Platform this is achieved using persistent volume claims (PVCs).

If you use a local volume for persistent storage, do not use a raw block volume, which is described with volumeMode: block in the LocalVolume object. Elasticsearch cannot use raw block volumes.
The OpenShift Elasticsearch Operator names the PVCs using the Elasticsearch resource name.

Fluentd ships any logs from systemd journal and /var/log/containers/*.log to Elasticsearch.

Elasticsearch requires sufficient memory to perform large merge operations. If it does not have enough memory, it becomes unresponsive. To avoid this problem, evaluate how much application log data you need, and allocate approximately double that amount of free storage capacity.

By default, when storage capacity is 85% full, Elasticsearch stops allocating new data to the node. At 90%, Elasticsearch attempts to relocate existing shards from that node to other nodes if possible. But if no nodes have a free capacity below 85%, Elasticsearch effectively rejects creating new indices and becomes RED.

These low and high watermark values are Elasticsearch defaults in the current release. You can modify these default values. Although the alerts use the same default values, you cannot change these values in the alerts.
Installing the OpenShift Elasticsearch Operator by using the web console
The OpenShift Elasticsearch Operator creates and manages the Elasticsearch cluster used by OpenShift Logging.

Elasticsearch is a memory-intensive application. Each Elasticsearch node needs at least 16GB of memory for both memory requests and limits, unless you specify otherwise in the ClusterLogging custom resource.

Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node
requires its own storage volume.


In the Red Hat OpenShift Container Platform web console, click Operators -> OperatorHub.

Click OpenShift Elasticsearch Operator from the list of available Operators, and click Install.

Ensure that the All namespaces on the cluster is selected under Installation mode.

Ensure that openshift-operators-redhat is selected under Installed Namespace.

Select Enable operator recommended cluster monitoring on this namespace.

Select stable-5.x as the Update channel.

Select an Update approval strategy:

Click Install.


Verify that the OpenShift Elasticsearch Operator installed by switching to the Operators → Installed Operators page.

Ensure that OpenShift Elasticsearch Operator is listed in all projects with a Status of Succeeded.
Installing the Elasticsearch Operator by using the CLI
You can use the OpenShift CLI (`oc`) to install the Elasticsearch Operator.

Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node requires its own storage volume.

You have administrator permissions.

You have installed the OpenShift CLI (`oc`).


Create a Namespace object as a YAML file:

Apply the Namespace object by running the following command:

Create an OperatorGroup object  as a YAML file:

Apply the OperatorGroup object by running the following command:

Create a Subscription object to subscribe the namespace to the Elasticsearch Operator:

Apply the subscription by running the following command:


Run the following command:

Observe the output and confirm that pods for the Elasticsearch Operator exist in each namespace
Configuring log storage
You can configure which log storage type your logging subsystem uses by modifying the ClusterLogging custom resource (CR).

You have administrator permissions.

You have installed the OpenShift CLI (`oc`).

You have installed the Red Hat OpenShift Logging Operator and an internal log store that is either the LokiStack or Elasticsearch.

You have created a ClusterLogging CR.


The Logging subsystem 5.9 release does not contain an updated version of the Elasticsearch Operator. If you currently use the Elasticsearch Operator released with Logging subsystem 5.8, it will continue to work with Logging subsystem until the EOL of Logging subsystem 5.8. As an alternative to using the Elasticsearch Operator to manage the default log storage, you can use the Loki Operator. For more information on the Logging subsystem lifecycle dates, see Platform Agnostic Operators.
Modify the ClusterLogging CR logStore spec:

Apply the ClusterLogging CR by running the following command: