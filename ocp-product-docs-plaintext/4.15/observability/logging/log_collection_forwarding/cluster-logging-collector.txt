Configuring the logging collector

Logging subsystem for Red Hat OpenShift collects operations and application logs from your cluster and enriches the data with Kubernetes pod and project metadata. All supported modifications to the log collector can be performed though the spec.collection stanza in the ClusterLogging custom resource (CR).
Configuring the log collector
You can configure which log collector type your logging subsystem uses by modifying the ClusterLogging custom resource (CR).

Fluentd is deprecated and is planned to be removed in a future release. Red Hat provides bug fixes and support for this feature during the current release lifecycle, but this feature no longer receives enhancements. As an alternative to Fluentd, you can use Vector instead.
You have administrator permissions.

You have installed the OpenShift CLI (`oc`).

You have installed the Red Hat OpenShift Logging Operator.

You have created a ClusterLogging CR.


Modify the ClusterLogging CR collection spec:

Apply the ClusterLogging CR by running the following command:
Creating a LogFileMetricExporter resource
In logging subsystem version 5.8 and newer versions, the LogFileMetricExporter is no longer deployed with the collector by default. You must manually create a LogFileMetricExporter custom resource (CR) to generate metrics from the logs produced by running containers.

If you do not create the LogFileMetricExporter CR, you may see a No datapoints found message in the Red Hat OpenShift Container Platform web console dashboard for Produced Logs.

You have administrator permissions.

You have installed the Red Hat OpenShift Logging Operator.

You have installed the OpenShift CLI (`oc`).


Create a LogFileMetricExporter CR as a YAML file:

Apply the LogFileMetricExporter CR by running the following command:


A logfilesmetricexporter pod runs concurrently with a collector pod on each node.

Verify that the logfilesmetricexporter pods are running in the namespace where you have created the LogFileMetricExporter CR, by running the following command and observing the output:
Configure log collector CPU and memory limits
The log collector allows for adjustments to both the CPU and memory limits.

Edit the ClusterLogging custom resource (CR) in the openshift-logging project:
Configuring input receivers
The Red Hat OpenShift Logging Operator deploys a service for each configured input receiver so that clients can write to the collector. This service exposes the port specified for the input receiver. The service name is generated based on the following:

For multi log forwarder ClusterLogForwarder CR deployments, the service name is in the format <ClusterLogForwarder_CR_name>-<input_name>. For example, example-http-receiver.

For legacy ClusterLogForwarder CR deployments, meaning those named instance and located in the openshift-logging namespace, the service name is in the format collector-<input_name>. For example, collector-http-receiver.


Configuring the collector to receive audit logs as an HTTP server
You can configure your log collector to listen for HTTP connections and receive audit logs as an HTTP server by specifying http as a receiver input in the ClusterLogForwarder custom resource (CR). This enables you to use a common log store for audit logs that are collected from both inside and outside of your Red Hat OpenShift Container Platform cluster.

You have administrator permissions.

You have installed the OpenShift CLI (`oc`).

You have installed the Red Hat OpenShift Logging Operator.

You have created a ClusterLogForwarder CR.


Modify the ClusterLogForwarder CR to add configuration for the http receiver input:

Apply the changes to the ClusterLogForwarder CR by running the following command:


Overview of API audit filter
Advanced configuration for the Fluentd log forwarder
Fluentd is deprecated and is planned to be removed in a future release. Red Hat provides bug fixes and support for this feature during the current release lifecycle, but this feature no longer receives enhancements. As an alternative to Fluentd, you can use Vector instead.
Logging subsystem includes multiple Fluentd parameters that you can use for tuning the performance of the Fluentd log forwarder. With these parameters, you can change the following Fluentd behaviors:

Chunk and chunk buffer sizes

Chunk flushing behavior

Chunk forwarding retry behavior


Fluentd collects log data in a single blob called a chunk. When Fluentd creates a chunk, the chunk is considered to be in the stage, where the chunk gets filled with data. When the chunk is full, Fluentd moves the chunk to the queue, where chunks are held before being flushed, or written out to their destination. Fluentd can fail to flush a chunk for a number of reasons, such as network issues or capacity issues at the destination. If a chunk cannot be flushed, Fluentd retries flushing as configured.

By default in Red Hat OpenShift Container Platform, Fluentd uses the exponential backoff method to retry flushing, where Fluentd doubles the time it waits between attempts to retry flushing again, which helps reduce connection requests to the destination. You can disable exponential backoff and use the periodic retry method instead, which retries flushing the chunks at a specified interval.

These parameters can help you determine the trade-offs between latency and throughput.

To optimize Fluentd for throughput, you could use these parameters to reduce network packet count by configuring larger buffers and queues, delaying flushes, and setting longer times between retries. Be aware that larger buffers require more space on the node file system.

To optimize for low latency, you could use the parameters to send data as soon as possible, avoid the build-up of batches, have shorter queues and buffers, and use more frequent flush and retries.


You can configure the chunking and flushing behavior using the following parameters in the ClusterLogging custom resource (CR). The parameters are then automatically added to the Fluentd config map for use by Fluentd.

These parameters are:

Not relevant to most users. The default settings should give good general performance.

Only for advanced users with detailed knowledge of Fluentd configuration and performance.

Only for performance tuning. They have no effect on functional aspects of logging.

For more information on the Fluentd chunk lifecycle, see Buffer Plugins in the Fluentd documentation.

Edit the ClusterLogging custom resource (CR) in the openshift-logging project:

Add or modify any of the following parameters:

Verify that the Fluentd pods are redeployed:

Check that the new values are in the fluentd config map: