Cluster Observability Operator overview

The Cluster Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
The Cluster Observability Operator (COO) is an optional component of the Red Hat OpenShift Container Platform. You can deploy it to create standalone monitoring stacks that are independently configurable for use by different services and users.

The COO deploys the following monitoring components:

Prometheus

Thanos Querier (optional)

Alertmanager (optional)


The COO components function independently of the default in-cluster monitoring stack, which is deployed and managed by the Cluster Monitoring Operator (CMO). Monitoring stacks deployed by the two Operators do not conflict. You can use a COO monitoring stack in addition to the default platform monitoring components deployed by the CMO.
Understanding the Cluster Observability Operator
A default monitoring stack created by the Cluster Observability Operator (COO) includes a highly available Prometheus instance capable of sending metrics to an external endpoint by using remote write.

Each COO stack also includes an optional Thanos Querier component, which you can use to query a highly available Prometheus instance from a central location, and an optional Alertmanager component, which you can use to set up alert configurations for different services.

Advantages of using the Cluster Observability Operator
The MonitoringStack CRD used by the COO offers an opinionated default monitoring configuration for COO-deployed monitoring components, but you can customize it to suit more complex requirements.

Deploying a COO-managed monitoring stack can help meet monitoring needs that are difficult or impossible to address by using the core platform monitoring stack deployed by the Cluster Monitoring Operator (CMO). A monitoring stack deployed using COO has the following advantages over core platform and user workload monitoring:


Extendability
Users can add more metrics to a COO-deployed monitoring stack, which is not possible with core platform monitoring without losing support.
In addition, COO-managed stacks can receive certain cluster-specific metrics from core platform monitoring by using federation.
Multi-tenancy support
The COO can create a monitoring stack per user namespace.
You can also deploy multiple stacks per namespace or a single stack for multiple namespaces.
For example, cluster administrators, SRE teams, and development teams can all deploy their own monitoring stacks on a single cluster, rather than having to use a single shared stack of monitoring components.
Users on different teams can then independently configure features such as separate alerts, alert routing, and alert receivers for their applications and services.
Scalability
You can create COO-managed monitoring stacks as needed.
Multiple monitoring stacks can run on a single cluster, which can facilitate the monitoring of very large clusters by using manual sharding. This ability addresses cases where the number of metrics exceeds the monitoring capabilities of a single Prometheus instance.
Flexibility
Deploying the COO with Operator Lifecycle Manager (OLM) decouples COO releases from Red Hat OpenShift Container Platform release cycles.
This method of deployment enables faster release iterations and the ability to respond rapidly to changing requirements and issues.
Additionally, by deploying a COO-managed monitoring stack, users can manage alerting rules independently of Red Hat OpenShift Container Platform release cycles.
Highly customizable
The COO can delegate ownership of single configurable fields in custom resources to users by using Server-Side Apply (SSA), which enhances customization.


Kubernetes documentation for Server-Side Apply (SSA)