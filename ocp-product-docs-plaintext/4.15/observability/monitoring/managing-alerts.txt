Managing alerts

In Red Hat OpenShift Container Platform 4.15, the Alerting UI enables you to manage alerts, silences, and alerting rules.

Alerting rules. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.

Alerts. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an Red Hat OpenShift Container Platform cluster.

Silences. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.


The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to. For example, if you are logged in as a user with the cluster-admin role, you can access all alerts, silences, and alerting rules.

If you are a non-administrator user, you can create and silence alerts if you are assigned the following user roles:

The cluster-monitoring-view cluster role, which allows you to access Alertmanager

The monitoring-alertmanager-edit role, which permits you to create and silence alerts in the Administrator perspective in the web console

The monitoring-rules-edit cluster role, which permits you to create and silence alerts in the Developer perspective in the web console
Accessing the Alerting UI in the Administrator and Developer perspectives
The Alerting UI is accessible through the Administrator perspective and the Developer perspective of the Red Hat OpenShift Container Platform web console.

In the Administrator perspective, go to Observe -> Alerting. The three main pages in the Alerting UI in this perspective are the Alerts, Silences, and Alerting rules pages.


In the Developer perspective, go to Observe -> <project_name> -> Alerts. In this perspective, alerts, silences, and alerting rules are all managed from the Alerts page. The results shown in the Alerts page are specific to the selected project.


In the Developer perspective, you can select from core Red Hat OpenShift Container Platform and user-defined projects that you have access to in the Project: <project_name> list. However, alerts, silences, and alerting rules relating to core Red Hat OpenShift Container Platform projects are not displayed if you are not logged in as a cluster administrator.
Searching and filtering alerts, silences, and alerting rules
You can filter the alerts, silences, and alerting rules that are displayed in the Alerting UI. This section provides a description of each of the available filtering options.


In the Administrator perspective, the Alerts page in the Alerting UI provides details about alerts relating to default Red Hat OpenShift Container Platform and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown.

You can filter by alert state, severity, and source. By default, only Platform alerts that are Firing are displayed. The following describes each alert filtering option:

State filters:

Severity filters:

Source filters:



In the Administrator perspective, the Silences page in the Alerting UI provides details about silences applied to alerts in default Red Hat OpenShift Container Platform and user-defined projects. The page includes a summary of the state of each silence and the time at which a silence ends.

You can filter by silence state. By default, only Active and Pending silences are displayed. The following describes each silence state filter option:

State filters:



In the Administrator perspective, the Alerting rules page in the Alerting UI provides details about alerting rules relating to default Red Hat OpenShift Container Platform and user-defined projects. The page includes a summary of the state, severity, and source for each alerting rule.

You can filter alerting rules by alert state, severity, and source. By default, only Platform alerting rules are displayed. The following describes each alerting rule filtering option:

Alert state filters:

Severity filters:

Source filters:



In the Developer perspective, the Alerts page in the Alerting UI provides a combined view of alerts and silences relating to the selected project. A link to the governing alerting rule is provided for each displayed alert.

In this view, you can filter by alert state and severity. By default, all alerts in the selected project are displayed if you have permission to access the project. These filters are the same as those described for the Administrator perspective.
Getting information about alerts, silences, and alerting rules
The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.

You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing alerts for.


To obtain information about alerts in the Administrator perspective:

Open the Red Hat OpenShift Container Platform web console and go to the Observe -> Alerting -> Alerts page.

Optional: Search for alerts by name by using the Name field in the search list.

Optional: Filter alerts by state, severity, and source by selecting filters in the Filter list.

Optional: Sort the alerts by clicking one or more of the Name, Severity, State, and Source column headers.

Click the name of an alert to view its Alert details page. The page includes a graph that illustrates alert time series data. It also provides the following information about the alert:


To obtain information about silences in the Administrator perspective:

Go to the Observe -> Alerting -> Silences page.

Optional: Filter the silences by name using the Search by name field.

Optional: Filter silences by state by selecting filters in the Filter list. By default, Active and Pending filters are applied.

Optional: Sort the silences by clicking one or more of the Name, Firing alerts, State, and Creator column headers.

Select the name of a silence to view its Silence details page. The page includes the following details:


To obtain information about alerting rules in the Administrator perspective:

Go to the Observe -> Alerting -> Alerting rules page.

Optional: Filter alerting rules by state, severity, and source by selecting filters in the Filter list.

Optional: Sort the alerting rules by clicking one or more of the Name, Severity, Alert state, and Source column headers.

Select the name of an alerting rule to view its Alerting rule details page. The page provides the following details about the alerting rule:


To obtain information about alerts, silences, and alerting rules in the Developer perspective:

Go to the Observe -> <project_name> -> Alerts page.

View details for an alert, silence, or an alerting rule:


Only alerts, silences, and alerting rules relating to the selected project are displayed in the Developer perspective.
See the Cluster Monitoring Operator runbooks to help diagnose and resolve issues that trigger specific Red Hat OpenShift Container Platform monitoring alerts.
Managing silences
You can create a silence for an alert in the Red Hat OpenShift Container Platform web console in both the Administrator and Developer perspectives. After you create a silence, you will not receive notifications about an alert when the alert fires.

Creating silences is useful in scenarios where you have received an initial alert notification, and you do not want to receive further notifications during the time in which you resolve the underlying issue causing the alert to fire.

When creating a silence, you must specify whether it becomes active immediately or at a later time. You must also set a duration period after which the silence expires.

After you create silences, you can view, edit, and expire them.

When you create silences, they are replicated across Alertmanager pods. However, if you do not configure persistent storage for Alertmanager, silences might be lost. This can happen, for example, if all Alertmanager pods restart at the same time.
Configuring persistent storage


Silencing alerts
You can silence a specific alert or silence alerts that match a specification that you define.

If you are a cluster administrator, you have access to the cluster as a user with the cluster-admin role.

If you are a non-administrator user, you have access to the cluster as a user with the following user roles:


To silence a specific alert in the Administrator perspective:

Go to Observe -> Alerting -> Alerts in the Red Hat OpenShift Container Platform web console.

For the alert that you want to silence, click . and select Silence alert to open the Silence alert page with a default configuration for the chosen alert.

Optional: Change the default configuration details for the silence.

To save the silence, click Silence.


To silence a specific alert in the Developer perspective:

Go to Observe -> <project_name> -> Alerts in the Red Hat OpenShift Container Platform web console.

If necessary, expand the details for the alert by selecting a greater than symbol (>) next to the alert name.

Click the alert message in the expanded view to open the Alert details page for the alert.

Click Silence alert to open the Silence alert page with a default configuration for the alert.

Optional: Change the default configuration details for the silence.

To save the silence, click Silence.


To silence a set of alerts by creating a silence configuration in the Administrator perspective:

Go to Observe -> Alerting -> Silences in the Red Hat OpenShift Container Platform web console.

Click Create silence.

On the Create silence page, set the schedule, duration, and label details for an alert.

To create silences for alerts that match the labels that you entered, click Silence.


To silence a set of alerts by creating a silence configuration in the Developer perspective:

Go to Observe -> <project_name> -> Silences in the Red Hat OpenShift Container Platform web console.

Click Create silence.

On the Create silence page, set the duration and label details for an alert.

To create silences for alerts that match the labels that you entered, click Silence.
Editing silences
You can edit a silence, which expires the existing silence and creates a new one with the changed configuration.

If you are a cluster administrator, you have access to the cluster as a user with the cluster-admin role.

If you are a non-administrator user, you have access to the cluster as a user with the following user roles:


To edit a silence in the Administrator perspective:

Go to Observe -> Alerting -> Silences.

For the silence you want to modify, click . and select Edit silence.

On the Edit silence page, make changes and click Silence. Doing so expires the existing silence and creates one with the updated configuration.


To edit a silence in the Developer perspective:

Go to Observe -> <project_name> -> Silences.

For the silence you want to modify, click . and select Edit silence.

On the Edit silence page, make changes and click Silence. Doing so expires the existing silence and creates one with the updated configuration.
Expiring silences
You can expire a single silence or multiple silences. Expiring a silence deactivates it permanently.

You cannot delete expired, silenced alerts. Expired silences older than 120 hours are garbage collected.
If you are a cluster administrator, you have access to the cluster as a user with the cluster-admin role.

If you are a non-administrator user, you have access to the cluster as a user with the following user roles:


To expire a silence or silences in the Administrator perspective:

Go to Observe -> Alerting -> Silences.

For the silence or silences you want to expire, select the checkbox in the corresponding row.

Click Expire 1 silence to expire a single selected silence or Expire <n> silences to expire multiple selected silences, where <n> is the number of silences you selected.


To expire a silence in the Developer perspective:

Go to Observe -> <project_name> -> Silences.

For the silence or silences you want to expire, select the checkbox in the corresponding row.

Click Expire 1 silence to expire a single selected silence or Expire <n> silences to expire multiple selected silences, where <n> is the number of silences you selected.
Managing alerting rules for core platform monitoring
Red Hat OpenShift Container Platform 4.15 monitoring ships with a large set of default alerting rules for platform metrics. As a cluster administrator, you can customize this set of rules in two ways:

Modify the settings for existing platform alerting rules by adjusting thresholds or by adding and modifying labels.
For example, you can change the severity label for an alert from warning to critical to help you route and triage issues flagged by an alert.

Define and add new custom alerting rules by constructing a query expression based on core platform metrics in the openshift-monitoring namespace.


New alerting rules must be based on the default Red Hat OpenShift Container Platform monitoring metrics.

You must create the AlertingRule and AlertRelabelConfig objects in the openshift-monitoring namespace.

You can only add and modify alerting rules. You cannot create new recording rules or modify existing recording rules.

If you modify existing platform alerting rules by using an AlertRelabelConfig object, your modifications are not reflected in the Prometheus alerts API.
Therefore, any dropped alerts still appear in the Red Hat OpenShift Container Platform web console even though they are no longer forwarded to Alertmanager.
Additionally, any modifications to alerts, such as a changed severity label, do not appear in the web console.


Tips for optimizing alerting rules for core platform monitoring
If you customize core platform alerting rules to meet your organization's specific needs, follow these guidelines to help ensure that the customized rules are efficient and effective.

Minimize the number of new rules.
Create only rules that are essential to your specific requirements.
By minimizing the number of rules, you create a more manageable and focused alerting system in your monitoring environment.

Focus on symptoms rather than causes.
Create rules that notify users of symptoms instead of underlying causes.
This approach ensures that users are promptly notified of a relevant symptom so that they can investigate the root cause after an alert has triggered.
This tactic also significantly reduces the overall number of rules you need to create.

Plan and assess your needs before implementing changes.
First, decide what symptoms are important and what actions you want users to take if these symptoms occur.
Then, assess existing rules and decide if you can modify any of them to meet your needs instead of creating entirely new rules for each symptom.
By modifying existing rules and creating new ones judiciously, you help to streamline your alerting system.

Provide clear alert messaging.
When you create alert messages, describe the symptom, possible causes, and recommended actions.
Include unambiguous, concise explanations along with troubleshooting steps or links to more information.
Doing so helps users quickly assess the situation and respond appropriately.

Include severity levels.
Assign severity levels to your rules to indicate how a user needs to react when a symptom occurs and triggers an alert.
For example, classifying an alert as Critical signals that an individual or a critical response team needs to respond immediately.
By defining severity levels, you help users know how to respond to an alert and help ensure that the most urgent issues receive prompt attention.
Creating new alerting rules
As a cluster administrator, you can create new alerting rules based on platform metrics. These alerting rules trigger alerts based on the values of chosen metrics.

If you create a customized AlertingRule resource based on an existing platform alerting rule, silence the original alert to avoid receiving conflicting alerts.

To help users understand the impact and cause of the alert, ensure that your alerting rule contains an alert message and severity value.
You have access to the cluster as a user that has the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Create a new YAML configuration file named example-alerting-rule.yaml.

Add an AlertingRule resource to the YAML file.
The following example creates a new alerting rule named example, similar to the default Watchdog alert:

Apply the configuration file to the cluster:
Modifying core platform alerting rules
As a cluster administrator, you can modify core platform alerts before Alertmanager routes them to a receiver. For example, you can change the severity label of an alert, add a custom label, or exclude an alert from being sent to Alertmanager.

You have access to the cluster as a user with the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Create a new YAML configuration file named example-modified-alerting-rule.yaml.

Add an AlertRelabelConfig resource to the YAML file.
The following example modifies the severity setting to critical for the default platform watchdog alerting rule:

Apply the configuration file to the cluster:


See Monitoring overview for details about Red Hat OpenShift Container Platform 4.15 monitoring architecture.

See the Alertmanager documentation for information about alerting rules.

See the Prometheus relabeling documentation for information about how relabeling works.

See the Prometheus alerting documentation for further guidelines on optimizing alerts.
Managing alerting rules for user-defined projects
Red Hat OpenShift Container Platform monitoring ships with a set of default alerting rules. As a cluster administrator, you can view the default alerting rules.

In Red Hat OpenShift Container Platform 4.15, you can create, view, edit, and remove alerting rules in user-defined projects.

The default alerting rules are used specifically for the Red Hat OpenShift Container Platform cluster.

Some alerting rules intentionally have identical names. They send alerts about the same event with different thresholds, different severity, or both.

Inhibition rules prevent notifications for lower severity alerts that are firing when a higher severity alert is also firing.


Optimizing alerting for user-defined projects
You can optimize alerting for your own projects by considering the following recommendations when creating alerting rules:

Minimize the number of alerting rules that you create for your project. Create alerting rules that notify you of conditions that impact you. It is more difficult to notice relevant alerts if you generate many alerts for conditions that do not impact you.

Create alerting rules for symptoms instead of causes. Create alerting rules that notify you of conditions regardless of the underlying cause. The cause can then be investigated. You will need many more alerting rules if each relates only to a specific cause. Some causes are then likely to be missed.

Plan before you write your alerting rules. Determine what symptoms are important to you and what actions you want to take if they occur. Then build an alerting rule for each symptom.

Provide clear alert messaging. State the symptom and recommended actions in the alert message.

Include severity levels in your alerting rules. The severity of an alert depends on how you need to react if the reported symptom occurs. For example, a critical alert should be triggered if a symptom requires immediate attention by an individual or a critical response team.


See the Prometheus alerting documentation for further guidelines on optimizing alerts

See Monitoring overview for details about Red Hat OpenShift Container Platform 4.15 monitoring architecture
About creating alerting rules for user-defined projects
If you create alerting rules for a user-defined project, consider the following key behaviors and important limitations when you define the new rules:

A user-defined alerting rule can include metrics exposed by its own project in addition to the default metrics from core platform monitoring.
You cannot include metrics from another user-defined project.

To reduce latency and to minimize the load on core platform monitoring components, you can add the openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus label to a rule.
This label forces only the Prometheus instance deployed in the openshift-user-workload-monitoring project to evaluate the alerting rule and prevents the Thanos Ruler instance from doing so.
Creating alerting rules for user-defined projects
You can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.

When you create an alerting rule, a project label is enforced on it even if a rule with the same name exists in another project.

To help users understand the impact and cause of the alert, ensure that your alerting rule contains an alert message and severity value.
You have enabled monitoring for user-defined projects.

You are logged in as a user that has the monitoring-rules-edit cluster role for the project where you want to create an alerting rule.

You have installed the OpenShift CLI (oc).


Create a YAML file for alerting rules. In this example, it is called example-app-alerting-rule.yaml.

Add an alerting rule configuration to the YAML file.
The following example creates a new alerting rule named example-alert. The alerting rule fires an alert when the version metric exposed by the sample service becomes 0:

Apply the configuration file to the cluster:


See Monitoring overview for details about Red Hat OpenShift Container Platform 4.15 monitoring architecture.
Accessing alerting rules for user-defined projects
To list alerting rules for a user-defined project, you must have been assigned the monitoring-rules-view cluster role for the project.

You have enabled monitoring for user-defined projects.

You are logged in as a user that has the monitoring-rules-view cluster role for your project.

You have installed the OpenShift CLI (oc).


To list alerting rules in <project>:

To list the configuration of an alerting rule, run the following:
Listing alerting rules for all projects in a single view
As a cluster administrator, you can list alerting rules for core Red Hat OpenShift Container Platform and user-defined projects together in a single view.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (oc).


In the Administrator perspective, navigate to Observe -> Alerting -> Alerting rules.

Select the Platform and User sources in the Filter drop-down menu.
Removing alerting rules for user-defined projects
You can remove alerting rules for user-defined projects.

You have enabled monitoring for user-defined projects.

You are logged in as a user that has the monitoring-rules-edit cluster role for the project where you want to create an alerting rule.

You have installed the OpenShift CLI (oc).


To remove rule <foo> in <namespace>, run the following:


See the Alertmanager documentation
Sending notifications to external systems
In Red Hat OpenShift Container Platform 4.15, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure Red Hat OpenShift Container Platform to send alerts to the following receiver types:

PagerDuty

Webhook

Email

Slack

Microsoft Teams


Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.

Red Hat OpenShift Container Platform monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.

Configuring alert receivers
You can configure alert receivers to ensure that you learn about important issues with your cluster.

You have access to the cluster as a user with the cluster-admin cluster role.


In the Administrator perspective, go to Administration -> Cluster Settings -> Configuration -> Alertmanager.

Click Create Receiver in the Receivers section of the page.

In the Create Receiver form, add a Receiver name and choose a Receiver type from the list.

Edit the receiver configuration:

By default, firing alerts with labels that match all of the selectors are sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver, perform the following steps:

Click Create to create the receiver.
Configuring different alert receivers for default platform alerts and user-defined alerts
You can configure different alert receivers for default platform alerts and user-defined alerts to ensure the following results:

All default platform alerts are sent to a receiver owned by the team in charge of these alerts.

All user-defined alerts are sent to another receiver so that the team can focus only on platform alerts.


You can achieve this by using the openshift_io_alert_source="platform" label that is added by the Cluster Monitoring Operator to all platform alerts:

Use the openshift_io_alert_source="platform" matcher to match default platform alerts.

Use the openshift_io_alert_source!="platform" or 'openshift_io_alert_source=""' matcher to match user-defined alerts.


This configuration does not apply if you have enabled a separate instance of Alertmanager dedicated to user-defined alerts.
Creating alert routing for user-defined projects
If you are a non-administrator user who has been given the alert-routing-edit cluster role, you can create or edit alert routing for user-defined projects.

A cluster administrator has enabled monitoring for user-defined projects.

A cluster administrator has enabled alert routing for user-defined projects.

You are logged in as a user that has the alert-routing-edit cluster role for the project for which you want to create alert routing.

You have installed the OpenShift CLI (oc).


Create a YAML file for alert routing. The example in this procedure uses a file called example-app-alert-routing.yaml.

Add an AlertmanagerConfig YAML definition to the file. For example:

Save the file.

Apply the resource to the cluster:
Applying a custom Alertmanager configuration
You can overwrite the default Alertmanager configuration by editing the alertmanager-main secret in the openshift-monitoring namespace for the platform instance of Alertmanager.

You have access to the cluster as a user with the cluster-admin cluster role.


To change the Alertmanager configuration from the CLI:

Print the currently active Alertmanager configuration into file alertmanager.yaml:

Edit the configuration in alertmanager.yaml:

Apply the new configuration in the file:


To change the Alertmanager configuration from the Red Hat OpenShift Container Platform web console:

Go to the Administration -> Cluster Settings -> Configuration -> Alertmanager -> YAML page of the web console.

Modify the YAML configuration file.

Click Save.
Applying a custom configuration to Alertmanager for user-defined alert routing
If you have enabled a separate instance of Alertmanager dedicated to user-defined alert routing, you can overwrite the configuration for this instance of Alertmanager by editing the alertmanager-user-workload secret in the openshift-user-workload-monitoring namespace.

You have access to the cluster as a user with the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Print the currently active Alertmanager configuration into the file alertmanager.yaml:

Edit the configuration in alertmanager.yaml:

Apply the new configuration in the file:


See the PagerDuty official site for more information on PagerDuty.

See the PagerDuty Prometheus Integration Guide to learn how to retrieve the service_key.

See Alertmanager configuration for configuring alerting through different alert receivers.

See Enabling alert routing for user-defined projects to learn how to enable a dedicated instance of Alertmanager for user-defined alert routing.
Next steps
Reviewing monitoring dashboards