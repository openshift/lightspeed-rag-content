Configuring the monitoring stack

The Red Hat OpenShift Container Platform installation program provides only a low number of configuration options before installation. Configuring most Red Hat OpenShift Container Platform framework components, including the cluster monitoring stack, happens after the installation.

This section explains what configuration is supported, shows how to configure the monitoring stack, and demonstrates several common configuration scenarios.

Not all configuration parameters for the monitoring stack are exposed. Only the parameters and fields listed in the Config map reference for the Cluster Monitoring Operator are supported for configuration.
Prerequisites
The monitoring stack imposes additional resource requirements. Consult the computing resources recommendations in Scaling the Cluster Monitoring Operator and verify that you have sufficient resources.
Maintenance and support for monitoring
Not all configuration options for the monitoring stack are exposed. The only supported way of configuring Red Hat OpenShift Container Platform monitoring is by configuring the Cluster Monitoring Operator using the options described in the Config map reference for the Cluster Monitoring Operator. Do not use other configurations, as they are unsupported.

Configuration paradigms might change across Prometheus releases, and such cases can only be handled gracefully if all configuration possibilities are controlled. If you use configurations other than those described in the Config map reference for the Cluster Monitoring Operator, your changes will disappear because the Cluster Monitoring Operator automatically reconciles any differences and resets any unsupported changes back to the originally defined state by default and by design.

Support considerations for monitoring
Backward compatibility for metrics, recording rules, or alerting rules is not guaranteed.
The following modifications are explicitly not supported:

Creating additional ServiceMonitor, PodMonitor, and PrometheusRule objects in the openshift-&#42; and kube-&#42; projects.

Modifying any resources or objects deployed in the openshift-monitoring or openshift-user-workload-monitoring projects. The resources created by the Red Hat OpenShift Container Platform monitoring stack are not meant to be used by any other resources, as there are no guarantees about their backward compatibility.

Modifying resources of the stack. The Red Hat OpenShift Container Platform monitoring stack ensures its resources are always in the state it expects them to be. If they are modified, the stack will reset them.

Deploying user-defined workloads to openshift-&#42;, and kube-&#42; projects. These projects are reserved for Red Hat provided components and they should not be used for user-defined workloads.

Enabling symptom based monitoring by using the Probe custom resource definition (CRD) in Prometheus Operator.

Manually deploying monitoring resources into namespaces that have the openshift.io/cluster-monitoring: "true" label.

Adding the openshift.io/cluster-monitoring: "true" label to namespaces. This label is reserved only for the namespaces with core Red Hat OpenShift Container Platform components and Red Hat certified components.

Installing custom Prometheus instances on Red Hat OpenShift Container Platform. A custom instance is a Prometheus custom resource (CR) managed by the Prometheus Operator.
Support policy for monitoring Operators
Monitoring Operators ensure that Red Hat OpenShift Container Platform monitoring resources function as designed and tested. If Cluster Version Operator (CVO) control of an Operator is overridden, the Operator does not respond to configuration changes, reconcile the intended state of cluster objects, or receive updates.

While overriding CVO control for an Operator can be helpful during debugging, this is  unsupported and the cluster administrator assumes full control of the individual component configurations and upgrades.

The spec.overrides parameter can be added to the configuration for the CVO to allow administrators to provide a list of overrides to the behavior of the CVO for a component. Setting the spec.overrides[].unmanaged parameter to true for a component blocks cluster upgrades and alerts the administrator after a CVO override has been set:

Disabling ownership via cluster version overrides prevents upgrades. Please remove overrides before continuing.
Setting a CVO override puts the entire cluster in an unsupported state and prevents the monitoring stack from being reconciled to its intended state. This impacts the reliability features built into Operators and prevents updates from being received. Reported issues must be reproduced after removing any overrides for support to proceed.
Support version matrix for monitoring components
The following matrix contains information about versions of monitoring components for Red Hat OpenShift Container Platform 4.12 and later releases:


The openshift-state-metrics agent and Telemeter Client are OpenShift-specific components. Therefore, their versions correspond with the versions of Red Hat OpenShift Container Platform.
Preparing to configure the monitoring stack
You can configure the monitoring stack by creating and updating monitoring config maps. These config maps configure the Cluster Monitoring Operator (CMO), which in turn configures the components of the monitoring stack.

Creating a cluster monitoring config map
You can configure the core Red Hat OpenShift Container Platform monitoring components by creating the cluster-monitoring-config ConfigMap object in the openshift-monitoring project. The Cluster Monitoring Operator (CMO) then configures the core components of the monitoring stack.

When you save your changes to the cluster-monitoring-config ConfigMap object, some or all of the pods in the openshift-monitoring project might be redeployed. It can sometimes take a while for these components to redeploy.
You have access to the cluster as a user with the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Check whether the cluster-monitoring-config ConfigMap object exists:

If the ConfigMap object does not exist:
Creating a user-defined workload monitoring config map
You can configure the user workload monitoring components with the user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project. The Cluster Monitoring Operator (CMO) then configures the components that monitor user-defined projects.

If you enable monitoring for user-defined projects, the user-workload-monitoring-config ConfigMap object is created by default.

When you save your changes to the user-workload-monitoring-config ConfigMap object, some or all of the pods in the openshift-user-workload-monitoring project might be redeployed. It can sometimes take a while for these components to redeploy.
You have access to the cluster as a user with the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Check whether the user-workload-monitoring-config ConfigMap object exists:

If the user-workload-monitoring-config ConfigMap object does not exist:


Enabling monitoring for user-defined projects
Configuring the monitoring stack
In Red Hat OpenShift Container Platform 4.15, you can configure the monitoring stack using the cluster-monitoring-config or user-workload-monitoring-config ConfigMap objects. Config maps configure the Cluster Monitoring Operator (CMO), which in turn configures the components of the stack.

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object.

Save the file to apply the changes to the ConfigMap object. The pods affected by the new configuration are restarted automatically.


Configuration reference for the cluster-monitoring-config config map

Configuration reference for the user-workload-monitoring-config config map

See Preparing to configure the monitoring stack for steps to create monitoring config maps

Enabling monitoring for user-defined projects
Configurable monitoring components
This table shows the monitoring components you can configure and the keys used to specify the components in the cluster-monitoring-config and user-workload-monitoring-config ConfigMap objects.


The Prometheus key is called prometheusK8s in the cluster-monitoring-config ConfigMap object and prometheus in the user-workload-monitoring-config ConfigMap object.
Using node selectors to move monitoring components
By using the nodeSelector constraint with labeled nodes, you can move any of the monitoring stack components to specific nodes. By doing so, you can control the placement and distribution of the monitoring components across a cluster.

By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and segregate workloads based on specific requirements or policies.

How node selectors work with other constraints
If you move monitoring components by using node selector constraints, be aware that other constraints to control pod scheduling might exist for a cluster:

Topology spread constraints might be in place to control pod placement.

Hard anti-affinity rules are in place for Prometheus, Thanos Querier, Alertmanager, and other monitoring components to ensure that multiple pods for these components are always spread across different nodes and are therefore always highly available.


When scheduling pods onto nodes, the pod scheduler tries to satisfy all existing constraints when determining pod placement. That is, all constraints compound when the pod scheduler determines which pods will be placed on which nodes.

Therefore, if you configure a node selector constraint but existing constraints cannot all be satisfied, the pod scheduler cannot match all constraints and will not schedule a pod for placement onto a node.

To maintain resilience and high availability for monitoring components, ensure that enough nodes are available and match all constraints when you configure a node selector constraint to move a component.

Understanding how to update labels on nodes

Placing pods on specific nodes using node selectors

Placing pods relative to other pods using affinity and anti-affinity rules

Controlling pod placement by using pod topology spread constraints

Using pod topology spread constraints for monitoring

Kubernetes documentation about node selectors
Moving monitoring components to different nodes
To specify the nodes in your cluster on which monitoring stack components will run, configure the nodeSelector constraint in the component's ConfigMap object to match labels assigned to the nodes.

You cannot add a node selector constraint directly to an existing scheduled pod.
If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


If you have not done so yet, add a label to the nodes on which you want to run the monitoring components:

Edit the ConfigMap object:

Save the file to apply the changes.
The components specified in the new configuration are moved to the new nodes automatically.


See Preparing to configure the monitoring stack for steps to create monitoring config maps

Enabling monitoring for user-defined projects

Understanding how to update labels on nodes

Placing pods on specific nodes using node selectors

See the Kubernetes documentation for details on the nodeSelector constraint
Assigning tolerations to monitoring components
You can assign tolerations to any of the monitoring stack components to enable moving them to tainted nodes.

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object:

Save the file to apply the changes. The new component placement configuration is applied automatically.


See Preparing to configure the monitoring stack for steps to create monitoring config maps

Enabling monitoring for user-defined projects

See the Red Hat OpenShift Container Platform documentation on taints and tolerations

See the Kubernetes documentation on taints and tolerations
Setting the body size limit for metrics scraping
By default, no limit exists for the uncompressed body size for data returned from scraped metrics targets. You can set a body size limit to help avoid situations in which Prometheus consumes excessive amounts of memory when scraped targets return a response that contains a large amount of data. In addition, by setting a body size limit, you can reduce the impact that a malicious target might have on Prometheus and on the cluster as a whole.

After you set a value for enforcedBodySizeLimit, the alert PrometheusScrapeBodySizeLimitHit fires when at least one Prometheus scrape target replies with a response body larger than the configured value.

If metrics data scraped from a target has an uncompressed body size exceeding the configured size limit, the scrape fails. Prometheus then considers this target to be down and sets its up metric value to 0, which can trigger the TargetDown alert.
You have access to the cluster as a user with the cluster-admin cluster role.

You have installed the OpenShift CLI (oc).


Edit the cluster-monitoring-config ConfigMap object in the openshift-monitoring namespace:

Add a value for enforcedBodySizeLimit to data/config.yaml/prometheusK8s to limit the body size that can be accepted per target scrape:

Save the file to apply the changes automatically.


Prometheus scrape configuration documentation
Managing CPU and memory resources for monitoring components
You can ensure that the containers that run monitoring components have enough CPU and memory resources by specifying values for resource limits and requests for those components.

You can configure these limits and requests for core platform monitoring components in the openshift-monitoring namespace and for the components that monitor user-defined projects in the openshift-user-workload-monitoring namespace.

About specifying limits and requests for monitoring components
You can configure resource limits and request settings for core platform monitoring components and for the components that monitor user-defined projects, including the following components:

Alertmanager (for core platform monitoring and for user-defined projects)

kube-state-metrics

monitoring-plugin

node-exporter

openshift-state-metrics

Prometheus (for core platform monitoring and for user-defined projects)

Prometheus Adapter

Prometheus Operator and its admission webhook service

Telemeter Client

Thanos Querier

Thanos Ruler


By defining resource limits, you limit a container's resource usage, which prevents the container from exceeding the specified maximum values for CPU and memory resources.

By defining resource requests, you specify that a container can be scheduled only on a node that has enough CPU and memory resources available to match the requested resources.
Specifying limits and requests for monitoring components
To configure CPU and memory resources, specify values for resource limits and requests in the appropriate ConfigMap object for the namespace in which the monitoring component is located:

The cluster-monitoring-config config map in the openshift-monitoring namespace for core platform monitoring

The user-workload-monitoring-config config map in the openshift-user-workload-monitoring namespace for components that monitor user-defined projects


If you are configuring core platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


To configure core platform monitoring components, edit the cluster-monitoring-config config map object in the openshift-monitoring namespace:

Add values to define resource limits and requests for each core platform monitoring component you want to configure.

Save the file to apply the changes automatically.


Kubernetes requests and limits documentation
Configuring persistent storage
Run cluster monitoring with persistent storage to gain the following benefits:

Protect your metrics and alerting data from data loss by storing them in a persistent volume (PV). As a result, they can survive pods being restarted or recreated.

Avoid getting duplicate notifications and losing silences for alerts when the Alertmanager pods are restarted.


For production environments, it is highly recommended to configure persistent storage. Because of the high IO demands, it is advantageous to use local storage.

Persistent storage prerequisites
Dedicate sufficient local persistent storage to ensure that the disk does not become full. How much storage you need depends on the number of pods.

Verify that you have a persistent volume (PV) ready to be claimed by the persistent volume claim (PVC), one PV for each replica. Because Prometheus and Alertmanager both have two replicas, you need four PVs to support the entire monitoring stack. The PVs are available from the Local Storage Operator, but not if you have enabled dynamically provisioned storage.

Use Filesystem as the storage type value for the volumeMode parameter when you configure the persistent volume.
Configuring a local persistent volume claim
For monitoring components to use a persistent volume (PV), you must configure a persistent volume claim (PVC).

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object:

Save the file to apply the changes. The pods affected by the new configuration are restarted automatically and the new storage configuration is applied.
Resizing a persistent storage volume
Red Hat OpenShift Container Platform does not support resizing an existing persistent storage volume used by StatefulSet resources, even if the underlying StorageClass resource used supports persistent volume sizing. Therefore, even if you update the storage field for an existing persistent volume claim (PVC) with a larger size, this setting will not be propagated to the associated persistent volume (PV).

However, resizing a PV is still possible by using a manual process. If you want to resize a PV for a monitoring component such as Prometheus, Thanos Ruler, or Alertmanager, you can update the appropriate config map in which the component is configured. Then, patch the PVC, and delete and orphan the pods. Orphaning the pods recreates the StatefulSet resource immediately and automatically updates the size of the volumes mounted in the pods with the new PVC settings. No service disruption occurs during this process.

You have installed the OpenShift CLI (oc).

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:


Edit the ConfigMap object:

Save the file to apply the changes. The pods affected by the new configuration restart automatically.

Manually patch every PVC with the updated storage request. The following example resizes the storage size for the Prometheus component in the openshift-monitoring namespace to 100Gi:

Delete the underlying StatefulSet with the --cascade=orphan parameter:
Modifying the retention time and size for Prometheus metrics data
By default, Prometheus retains metrics data for the following durations:

Core platform monitoring: 15 days

Monitoring for user-defined projects: 24 hours


You can modify the retention time for Prometheus to change how soon the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses. If the data reaches this size limit, Prometheus deletes the oldest data first until the disk space used is again below the limit.

Note the following behaviors of these data retention settings:

The size-based retention policy applies to all data block directories in the /prometheus directory, including persistent blocks, write-ahead log (WAL) data, and m-mapped chunks.

Data in the /wal and /head_chunks directories counts toward the retention size limit, but Prometheus never purges data from these directories based on size- or time-based retention policies.
Thus, if you set a retention size limit lower than the maximum size set for the /wal and /head_chunks directories, you have configured the system not to retain any data blocks in the /prometheus data directories.

The size-based retention policy is applied only when Prometheus cuts a new data block, which occurs every two hours after the WAL contains at least three hours of data.

If you do not explicitly define values for either retention or retentionSize, retention time defaults to 15 days for core platform monitoring and 24 hours for user-defined project monitoring. Retention size is not set.

If you define values for both retention and retentionSize, both values apply.
If any data blocks exceed the defined retention time or the defined size limit, Prometheus purges these data blocks.

If you define a value for retentionSize and do not define retention, only the retentionSize value applies.

If you do not define a value for retentionSize and only define a value for retention, only the retention value applies.

If you set the retentionSize or retention value to 0, the default settings apply. The default settings set retention time to 15 days for core platform monitoring and 24 hours for user-defined project monitoring. By default, retention size is not set.


Data compaction occurs every two hours. Therefore, a persistent volume (PV) might fill up before compaction, potentially exceeding the retentionSize limit. In such cases, the KubePersistentVolumeFillingUp alert fires until the space on a PV is lower than the retentionSize limit.
If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object:

Save the file to apply the changes. The pods affected by the new configuration restart automatically.
Modifying the retention time for Thanos Ruler metrics data
By default, for user-defined projects, Thanos Ruler automatically retains metrics data for 24 hours. You can modify the retention time to change how long this data is retained by specifying a time value in the user-workload-monitoring-config config map in the openshift-user-workload-monitoring namespace.

You have access to the cluster as a user with the cluster-admin cluster role or as a user with the user-workload-monitoring-config-edit role in the openshift-user-workload-monitoring project.

A cluster administrator has enabled monitoring for user-defined projects.

You have installed the OpenShift CLI (oc).


Edit the user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project:

Add the retention time configuration under data/config.yaml:

Save the file to apply the changes. The pods affected by the new configuration automatically restart.


Creating a cluster monitoring config map

Prometheus database storage requirements

Recommended configurable storage technology

Understanding persistent storage

Optimizing storage

Configure local persistent storage

Enabling monitoring for user-defined projects
Configuring remote write storage
You can configure remote write storage to enable Prometheus to send ingested metrics to remote systems for long-term storage. Doing so has no impact on how or for how long Prometheus stores metrics.

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).

You have set up a remote write compatible endpoint (such as Thanos) and know the endpoint URL. See the Prometheus remote endpoints and storage documentation for information about endpoints that are compatible with the remote write feature.

You have set up authentication credentials in a Secret object for the remote write endpoint. You must create the secret in the
same namespace as the Prometheus object for which you configure remote write: the openshift-monitoring namespace for default platform monitoring or the openshift-user-workload-monitoring namespace for user workload monitoring.


Edit the ConfigMap object:

Save the file to apply the changes. The pods affected by the new configuration restart automatically.


Supported remote write authentication settings
You can use different methods to authenticate with a remote write endpoint. Currently supported authentication methods are AWS Signature Version 4, basic authentication, authorization, OAuth 2.0, and TLS client. The following table provides details about supported authentication methods for use with remote write.
Example remote write authentication settings
The following samples show different authentication settings you can use to connect to a remote write endpoint. Each sample also shows how to configure a corresponding Secret object that contains authentication credentials and other relevant settings. Each sample configures authentication for use with default platform monitoring in the openshift-monitoring namespace.

The following shows the settings for a sigv4 secret named sigv4-credentials in the openshift-monitoring namespace.

apiVersion: v1
kind: Secret
metadata:
  name: sigv4-credentials
  namespace: openshift-monitoring
stringData:
  accessKey: <AWS_access_key> 1
  secretKey: <AWS_secret_key> 2
type: Opaque
The AWS API access key.

The AWS API secret key.


The following shows sample AWS Signature Version 4 remote write authentication settings that use a Secret object named sigv4-credentials in the openshift-monitoring namespace:

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        sigv4:
          region: <AWS_region> 1
          accessKey:
            name: sigv4-credentials 2
            key: accessKey 3
          secretKey:
            name: sigv4-credentials 2
            key: secretKey 4
          profile: <AWS_profile_name> 5
          roleArn: <AWS_role_arn> 6
The AWS region.

The name of the Secret object containing the AWS API access credentials.

The key that contains the AWS API access key in the specified Secret object.

The key that contains the AWS API secret key in the specified Secret object.

The name of the AWS profile that is being used to authenticate.

The unique identifier for the Amazon Resource Name (ARN) assigned to your role.
The following shows sample basic authentication settings for a Secret object named rw-basic-auth in the openshift-monitoring namespace:

apiVersion: v1
kind: Secret
metadata:
  name: rw-basic-auth
  namespace: openshift-monitoring
stringData:
  user: <basic_username> 1
  password: <basic_password> 2
type: Opaque
The username.

The password.


The following sample shows a basicAuth remote write configuration that uses a Secret object named rw-basic-auth in the openshift-monitoring namespace. It assumes that you have already set up authentication credentials for the endpoint.

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://basicauth.example.com/api/write"
        basicAuth:
          username:
            name: rw-basic-auth 1
            key: user 2
          password:
            name: rw-basic-auth 1
            key: password 3
The name of the Secret object that contains the authentication credentials.

The key that contains the username  in the specified Secret object.

The key that contains the password in the specified Secret object.
The following shows bearer token settings for a Secret object named rw-bearer-auth in the openshift-monitoring namespace:

apiVersion: v1
kind: Secret
metadata:
  name: rw-bearer-auth
  namespace: openshift-monitoring
stringData:
  token: <authentication_token> 1
type: Opaque
The authentication token.


The following shows sample bearer token config map settings that use a Secret object named rw-bearer-auth in the openshift-monitoring namespace:

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        authorization:
          type: Bearer 1
          credentials:
            name: rw-bearer-auth 2
            key: token 3
The authentication type of the request. The default value is Bearer.

The name of the Secret object that contains the authentication credentials.

The key that contains the authentication token in the specified Secret object.
The following shows sample OAuth 2.0 settings for a Secret object named oauth2-credentials in the openshift-monitoring namespace:

apiVersion: v1
kind: Secret
metadata:
  name: oauth2-credentials
  namespace: openshift-monitoring
stringData:
  id: <oauth2_id> 1
  secret: <oauth2_secret> 2
  token: <oauth2_authentication_token> 3
type: Opaque
The Oauth 2.0 ID.

The OAuth 2.0 secret.

The OAuth 2.0 token.


The following shows an oauth2 remote write authentication sample configuration that uses a Secret object named oauth2-credentials in the openshift-monitoring namespace:

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://test.example.com/api/write"
        oauth2:
          clientId:
            secret:
              name: oauth2-credentials 1
              key: id 2
          clientSecret:
            name: oauth2-credentials 1
            key: secret 2
          tokenUrl: https://example.com/oauth2/token 3
          scopes: 4
          - <scope_1>
          - <scope_2>
          endpointParams: 5
            param1: <parameter_1>
            param2: <parameter_2>
The name of the corresponding Secret object. Note that ClientId can alternatively refer to a ConfigMap object, although clientSecret must refer to a Secret object.

The key that contains the OAuth 2.0 credentials in the specified Secret object.

The URL used to fetch a token with the specified clientId and clientSecret.

The OAuth 2.0 scopes for the authorization request. These scopes limit what data the tokens can access.

The OAuth 2.0 authorization request parameters required for the authorization server.
The following shows sample TLS client settings for a tls Secret object named mtls-bundle in the openshift-monitoring namespace.

apiVersion: v1
kind: Secret
metadata:
  name: mtls-bundle
  namespace: openshift-monitoring
data:
  ca.crt: <ca_cert> 1
  client.crt: <client_cert> 2
  client.key: <client_key> 3
type: tls
The CA certificate in the Prometheus container with which to validate the server certificate.

The client certificate for authentication with the server.

The client key.


The following sample shows a tlsConfig remote write authentication configuration that uses a TLS Secret object named mtls-bundle.

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle 1
              key: ca.crt 2
          cert:
            secret:
              name: mtls-bundle 1
              key: client.crt 3
          keySecret:
            name: mtls-bundle 1
            key: client.key 4
The name of the corresponding Secret object that contains the TLS authentication credentials. Note that ca and cert can alternatively refer to a ConfigMap object, though keySecret must refer to a Secret object.

The key in the specified Secret object that contains the CA certificate for the endpoint.

The key in the specified Secret object that contains the client certificate for the endpoint.

The key in the specified Secret object that contains the client key secret.
See Setting up remote write compatible endpoints for steps to create a remote write compatible endpoint (such as Thanos).

See Tuning remote write settings for information about how to optimize remote write settings for different use cases.

See Understanding secrets for steps to create and configure Secret objects in Red Hat OpenShift Container Platform.

See the Prometheus REST API reference for remote write for information about additional optional fields.
Adding cluster ID labels to metrics
If you manage multiple Red Hat OpenShift Container Platform clusters and use the remote write feature to send metrics data from these clusters to an external storage location, you can add cluster ID labels to identify the metrics data coming from different clusters. You can then query these labels to identify the source cluster for a metric and distinguish that data from similar metrics data sent by other clusters.

This way, if you manage many clusters for multiple customers and send metrics data to a single centralized storage system, you can use cluster ID labels to query metrics for a particular cluster or customer.

Creating and using cluster ID labels involves three general steps:

Configuring the write relabel settings for remote write storage.

Adding cluster ID labels to the metrics.

Querying these labels to identify the source cluster or customer for a metric.


Creating cluster ID labels for metrics
You can create cluster ID labels for metrics for default platform monitoring and for user workload monitoring.

For default platform monitoring, you add cluster ID labels for metrics in the write_relabel settings for remote write storage in the cluster-monitoring-config config map in the openshift-monitoring namespace.

For user workload monitoring, you edit the settings in the user-workload-monitoring-config config map in the openshift-user-workload-monitoring namespace.

When Prometheus scrapes user workload targets that expose a namespace label, the system stores this label as exported_namespace. This behavior ensures that the final namespace label value is equal to the namespace of the target pod. You cannot override this default configuration by setting the value of the honorLabels field to true for PodMonitor or ServiceMonitor objects.
If you are configuring default platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).

You have configured remote write storage.


Edit the ConfigMap object:

Save the file to apply the changes to the ConfigMap object.
The pods affected by the updated configuration automatically restart.


For details about write relabel configuration, see Configuring remote write storage.

For information about how to get your cluster ID, see Obtaining your cluster ID.
Configuring metrics collection profiles
Using a metrics collection profile is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see https://access.redhat.com/support/offerings/techpreview.
By default, Prometheus collects metrics exposed by all default metrics targets in Red Hat OpenShift Container Platform components. However, you might want Prometheus to collect fewer metrics from a cluster in certain scenarios:

If cluster administrators require only alert, telemetry, and console metrics and do not require other metrics to be available.

If a cluster increases in size, and the increased size of the default metrics data collected now requires a significant increase in CPU and memory resources.


You can use a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data. When you collect minimal metrics data, basic monitoring features such as alerting continue to work. At the same time, the CPU and memory resources required by Prometheus decrease.

About metrics collection profiles
You can enable one of two metrics collection profiles:

full: Prometheus collects metrics data exposed by all platform components. This setting is the default.

minimal: Prometheus collects only the metrics data required for platform alerts, recording rules, telemetry, and console dashboards.
Choosing a metrics collection profile
To choose a metrics collection profile for core Red Hat OpenShift Container Platform monitoring components, edit the cluster-monitoring-config ConfigMap object.

You have installed the OpenShift CLI (oc).

You have enabled Technology Preview features by using the FeatureGate custom resource (CR).

You have created the cluster-monitoring-config ConfigMap object.

You have access to the cluster as a user with the cluster-admin cluster role.


Saving changes to a monitoring config map might restart monitoring processes and redeploy the pods and other resources in the related project. The running monitoring processes in that project might also restart.
Edit the cluster-monitoring-config ConfigMap object in the openshift-monitoring project:

Add the metrics collection profile setting under data/config.yaml/prometheusK8s:

Save the file to apply the changes. The pods affected by the new configuration restart automatically.


See Viewing a list of available metrics for steps to view a list of metrics being collected for a cluster.

See Enabling features using feature gates for steps to enable Technology Preview features.
Controlling the impact of unbound metrics attributes in user-defined projects
Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a customer_id attribute is unbound because it has an infinite number of possible values.

Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.

Cluster administrators can use the following measures to control the impact of unbound metrics attributes in user-defined projects:

Limit the number of samples that can be accepted per target scrape in user-defined projects

Limit the number of scraped labels, the length of label names, and the length of label values

Create alerts that fire when a scrape sample threshold is reached or when the target cannot be scraped


Limiting scrape samples can help prevent the issues caused by adding many unbound attributes to labels. Developers can also prevent the underlying cause by limiting the number of unbound attributes that they define for metrics. Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
Setting scrape sample and label limits for user-defined projects
You can limit the number of samples that can be accepted per target scrape in user-defined projects. You can also limit the number of scraped labels, the length of label names, and the length of label values.

If you set sample or label limits, no further sample data is ingested for that target scrape after the limit is reached.
You have access to the cluster as a user with the cluster-admin cluster role, or as a user with the user-workload-monitoring-config-edit role in the openshift-user-workload-monitoring project.

A cluster administrator has enabled monitoring for user-defined projects.

You have installed the OpenShift CLI (oc).


Edit the user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project:

Add the enforcedSampleLimit configuration to data/config.yaml to limit the number of samples that can be accepted per target scrape in user-defined projects:

Add the enforcedLabelLimit, enforcedLabelNameLengthLimit, and enforcedLabelValueLengthLimit configurations to data/config.yaml to limit the number of scraped labels, the length of label names, and the length of label values in user-defined projects:

Save the file to apply the changes. The limits are applied automatically.
Creating scrape sample alerts
You can create alerts that notify you when:

The target cannot be scraped or is not available for the specified for duration

A scrape sample threshold is reached or is exceeded for the specified for duration


You have access to the cluster as a user with the cluster-admin cluster role, or as a user with the user-workload-monitoring-config-edit role in the openshift-user-workload-monitoring project.

A cluster administrator has enabled monitoring for user-defined projects.

You have limited the number of samples that can be accepted per target scrape in user-defined projects, by using enforcedSampleLimit.

You have installed the OpenShift CLI (oc).


Create a YAML file with alerts that inform you when the targets are down and when the enforced sample limit is approaching. The file in this example is called monitoring-stack-alerts.yaml:

Apply the configuration to the user-defined project:


Creating a user-defined workload monitoring config map

Enabling monitoring for user-defined projects

See Determining why Prometheus is consuming a lot of disk space for steps to query which metrics have the highest number of scrape samples.
Configuring external Alertmanager instances
The Red Hat OpenShift Container Platform monitoring stack includes a local Alertmanager instance that routes alerts from Prometheus. You can add external Alertmanager instances to route alerts for core Red Hat OpenShift Container Platform projects or user-defined projects.

If you add the same external Alertmanager configuration for multiple clusters and disable the local instance for each cluster, you can then manage alert routing for multiple clusters by using a single external Alertmanager instance.

If you are configuring core Red Hat OpenShift Container Platform monitoring components in the openshift-monitoring project:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object.

Save the file to apply the changes to the ConfigMap object. The new component placement configuration is applied automatically.

Save the file to apply the changes to the ConfigMap object. The new component placement configuration is applied automatically.
Configuring secrets for Alertmanager
The Red Hat OpenShift Container Platform monitoring stack includes Alertmanager, which routes alerts from Prometheus to endpoint receivers. If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can configure Alertmanager to use a secret that contains authentication credentials for the receiver.

For example, you can configure Alertmanager to use a secret to authenticate with an endpoint receiver that requires a certificate issued by a private Certificate Authority (CA). You can also configure Alertmanager to use a secret to authenticate with a receiver that requires a password file for Basic HTTP authentication. In either case, authentication details are contained in the Secret object rather than in the ConfigMap object.

Adding a secret to the Alertmanager configuration
You can add secrets to the Alertmanager configuration for core platform monitoring components by editing the cluster-monitoring-config config map in the openshift-monitoring project.

After you add a secret to the config map, the secret is mounted as a volume at /etc/alertmanager/secrets/<secret_name> within the alertmanager container for the Alertmanager pods.

If you are configuring core Red Hat OpenShift Container Platform monitoring components in the openshift-monitoring project:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object.

Save the file to apply the changes to the ConfigMap object. The new configuration is applied automatically.
Attaching additional labels to your time series and alerts
You can attach custom labels to all time series and alerts leaving Prometheus by using the external labels feature of Prometheus.

If you are configuring core Red Hat OpenShift Container Platform monitoring components:

If you are configuring components that monitor user-defined projects:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object:

Save the file to apply the changes. The new configuration is applied automatically.


See Preparing to configure the monitoring stack for steps to create monitoring config maps.

Enabling monitoring for user-defined projects
Using pod topology spread constraints for monitoring
You can use pod topology spread constraints to control how the monitoring pods are spread across a network topology when Red Hat OpenShift Container Platform pods are deployed in multiple availability zones.

Pod topology spread constraints are suitable for controlling pod scheduling within hierarchical topologies in which nodes are spread across different infrastructure levels, such as regions and zones within those regions. Additionally, by being able to schedule pods in different zones, you can improve network latency in certain scenarios.

Controlling pod placement by using pod topology spread constraints

Kubernetes Pod Topology Spread Constraints documentation


Configuring pod topology spread constraints
You can configure pod topology spread constraints for all the pods deployed by the Cluster Monitoring Operator to control how pod replicas are scheduled to nodes across zones. This ensures that the pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.

You can configure pod topology spread constraints for monitoring pods by using the cluster-monitoring-config or the user-workload-monitoring-config config map.

If you are configuring pods for core Red Hat OpenShift Container Platform monitoring:

If you are configuring pods for user-defined monitoring:

You have installed the OpenShift CLI (oc).


To configure pod topology spread constraints for core Red Hat OpenShift Container Platform monitoring:

To configure pod topology spread constraints for user-defined monitoring:
Setting log levels for monitoring components
You can configure the log level for Alertmanager, Prometheus Operator, Prometheus, Thanos Querier, and Thanos Ruler.

The following log levels can be applied to the relevant component in the cluster-monitoring-config and user-workload-monitoring-config ConfigMap objects:

debug. Log debug, informational, warning, and error messages.

info. Log informational, warning, and error messages.

warn. Log warning and error messages only.

error. Log error messages only.


The default log level is info.

If you are setting a log level for Alertmanager, Prometheus Operator, Prometheus, or Thanos Querier in the openshift-monitoring project:

If you are setting a log level for Prometheus Operator, Prometheus, or Thanos Ruler in the openshift-user-workload-monitoring project:

You have installed the OpenShift CLI (oc).


Edit the ConfigMap object:

Save the file to apply the changes. The pods for the component restart automatically when you apply the log-level change.

Confirm that the log-level has been applied by reviewing the deployment or pod configuration in the related project. The following example checks the log level in the prometheus-operator deployment in the openshift-user-workload-monitoring project:

Check that the pods for the component are running. The following example lists the status of pods in the openshift-user-workload-monitoring project:
Enabling the query log file for Prometheus
You can configure Prometheus to write all queries that have been run by the engine to a log file. You can do so for default platform monitoring and for user-defined workload monitoring.

Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the ConfigMap object to enable the feature.
If you are enabling the query log file feature for Prometheus in the openshift-monitoring project:

If you are enabling the query log file feature for Prometheus in the openshift-user-workload-monitoring project:

You have installed the OpenShift CLI (oc).


To set the query log file for Prometheus in the openshift-monitoring project:

To set the query log file for Prometheus in the openshift-user-workload-monitoring project:


See Preparing to configure the monitoring stack for steps to create monitoring config maps

See Enabling monitoring for user-defined projects for steps to enable user-defined monitoring.
Enabling query logging for Thanos Querier
For default platform monitoring in the openshift-monitoring project, you can enable the Cluster Monitoring Operator to log all queries run by Thanos Querier.

Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the ConfigMap object to enable the feature.
You have installed the OpenShift CLI (oc).

You have access to the cluster as a user with the cluster-admin cluster role.

You have created the cluster-monitoring-config ConfigMap object.


You can enable query logging for Thanos Querier in the openshift-monitoring project:

Edit the cluster-monitoring-config ConfigMap object in the openshift-monitoring project:

Add a thanosQuerier section under data/config.yaml and add values as shown in the following example:

Save the file to apply the changes.


Verify that the Thanos Querier pods are running. The following sample command lists the status of pods in the openshift-monitoring project:

Run a test query using the following sample commands as a model:

Run the following command to read the query log:

After you examine the logged query information, disable query logging by changing the enableRequestLogging value to false in the config map.


See Preparing to configure the monitoring stack for steps to create monitoring config maps.
Setting audit log levels for the Prometheus Adapter
In default platform monitoring, you can configure the audit log level for the Prometheus Adapter.

You have installed the OpenShift CLI (oc).

You have access to the cluster as a user with the cluster-admin cluster role.

You have created the cluster-monitoring-config ConfigMap object.


You can set an audit log level for the Prometheus Adapter in the default openshift-monitoring project:

Edit the cluster-monitoring-config ConfigMap object in the openshift-monitoring project:

Add profile: in the k8sPrometheusAdapter/audit section under data/config.yaml:

Set the audit log level by using one of the following values for the profile: parameter:

Save the file to apply the changes. The pods for the Prometheus Adapter restart automatically when you apply the change.


In the config map, under k8sPrometheusAdapter/audit/profile, set the log level to Request and save the file.

Confirm that the pods for the Prometheus Adapter are running. The following example lists the status of pods in the openshift-monitoring project:

Confirm that the audit log level and audit log file path are correctly configured:

Confirm that the correct log level has been applied in the prometheus-adapter deployment in the openshift-monitoring project:

Review the audit log for the Prometheus Adapter:


See Preparing to configure the monitoring stack for steps to create monitoring config maps.
Disabling the local Alertmanager
A local Alertmanager that routes alerts from Prometheus instances is enabled by default in the openshift-monitoring project of the Red Hat OpenShift Container Platform monitoring stack.

If you do not need the local Alertmanager, you can disable it by configuring the cluster-monitoring-config config map in the openshift-monitoring project.

You have access to the cluster as a user with the cluster-admin cluster role.

You have created the cluster-monitoring-config config map.

You have installed the OpenShift CLI (oc).


Edit the cluster-monitoring-config config map in the openshift-monitoring project:

Add enabled: false for the alertmanagerMain component under data/config.yaml:

Save the file to apply the changes. The Alertmanager instance is disabled automatically when you apply the change.


Prometheus Alertmanager documentation

Managing alerts
Next steps
Enabling monitoring for user-defined projects

Learn about remote health reporting and, if necessary, opt out of it.