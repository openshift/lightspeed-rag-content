Customizing nodes

Red Hat OpenShift Container Platform supports both cluster-wide and per-machine configuration via Ignition, which allows arbitrary partitioning and file content changes to the operating system. In general, if a configuration file is documented in Red Hat Enterprise Linux (RHEL), then modifying it via Ignition is supported.

There are two ways to deploy machine config changes:

Creating machine configs that are included in manifest files
to start up a cluster during openshift-install.

Creating machine configs that are passed to running
Red Hat OpenShift Container Platform nodes via the Machine Config Operator.


Additionally, modifying the reference config, such as the Ignition config that is passed to coreos-installer when installing bare-metal nodes allows per-machine configuration. These changes are currently not visible to the Machine Config Operator.

The following sections describe features that you might want to configure on your nodes in this way.
Creating machine configs with Butane
Machine configs are used to configure control plane and worker machines by instructing machines how to create users and file systems, set up the network, install systemd units, and more.

Because modifying machine configs can be difficult, you can use Butane configs to create machine configs for you, thereby making node configuration much easier.

About Butane
Butane is a command-line utility that Red Hat OpenShift Container Platform uses to provide convenient, short-hand syntax for writing machine configs, as well as for performing additional validation of machine configs. The format of the Butane config file that Butane accepts is defined in the OpenShift Butane config spec.
Installing Butane
You can install the Butane tool (butane) to create Red Hat OpenShift Container Platform machine configs from a command-line interface. You can install butane on Linux, Windows, or macOS by downloading the corresponding binary file.

Butane releases are backwards-compatible with older releases and with the Fedora CoreOS Config Transpiler (FCCT).
Navigate to the Butane image download page at https://mirror.openshift.com/pub/openshift-v4/clients/butane/.

Get the butane binary:

Make the downloaded binary file executable:

Move the butane binary file to a directory on your PATH.


You can now use the Butane tool by running the butane command:
Creating a MachineConfig object by using Butane
You can use Butane to produce a MachineConfig object so that you can configure worker or control plane nodes at installation time or via the Machine Config Operator.

You have installed the butane utility.


Create a Butane config file. The following example creates a file named 99-worker-custom.bu that configures the system console to show kernel debug messages and specifies custom settings for the chrony time service:

Create a MachineConfig object by giving Butane the file that you created in the previous step:

Save the Butane config in case you need to update the MachineConfig object in the future.

If the cluster is not running yet, generate manifest files and add the MachineConfig object YAML file to the openshift directory. If the cluster is already running, apply the file as follows:


Adding kernel modules to nodes

Encrypting and mirroring disks during installation
Adding day-1 kernel arguments
Although it is often preferable to modify kernel arguments as a day-2 activity, you might want to add kernel arguments to all master or worker nodes during initial cluster installation. Here are some reasons you might want to add kernel arguments during cluster installation so they take effect before the systems first boot up:

You need to do some low-level network configuration before the systems start.

You want to disable a feature, such as SELinux, so it has no impact on the systems when they first come up.


To add kernel arguments to master or worker nodes, you can create a MachineConfig object and inject that object into the set of manifest files used by Ignition during cluster setup.

For a listing of arguments you can pass to a RHEL 8 kernel at boot time, see Kernel.org kernel parameters. It is best to only add kernel arguments with this procedure if they are needed to complete the initial Red Hat OpenShift Container Platform installation.

Change to the directory that contains the installation program and generate the Kubernetes manifests for the cluster:

Decide if you want to add kernel arguments to worker or control plane nodes.

In the openshift directory, create a file (for example,
99-openshift-machineconfig-master-kargs.yaml) to define a MachineConfig
object to add the kernel settings.
This example adds a loglevel=7 kernel argument to control plane nodes:


You can now continue on to create the cluster.
Adding kernel modules to nodes
For most common hardware, the Linux kernel includes the device driver modules needed to use that hardware when the computer starts up. For some hardware, however, modules are not available in Linux. Therefore, you must find a way to provide those modules to each host computer. This procedure describes how to do that for nodes in an Red Hat OpenShift Container Platform cluster.

When a kernel module is first deployed by following these instructions, the module is made available for the current kernel. If a new kernel is installed, the kmods-via-containers software will rebuild and deploy the module so a compatible version of that module is available with the new kernel.

The way that this feature is able to keep the module up to date on each node is by:

Adding a systemd service to each node that starts at boot time to detect
if a new kernel has been installed and

If a new kernel is detected, the
service rebuilds the module and installs it to the kernel


For information on the software needed for this procedure, see the kmods-via-containers github site.

A few important issues to keep in mind:

This procedure is Technology Preview.

Software tools and examples are not yet available in official RPM form
and can only be obtained for now from unofficial github.com sites noted in the procedure.

Third-party kernel modules you might add through these procedures are not supported by Red Hat.

In this procedure, the software needed to build your kernel modules is
deployed in a RHEL 8 container. Keep in mind that modules are rebuilt
automatically on each node when that node gets a new kernel. For that
reason, each node needs access to a yum repository that contains the
kernel and related packages needed to rebuild the module. That content
is best provided with a valid RHEL subscription.


Building and testing the kernel module container
Before deploying kernel modules to your Red Hat OpenShift Container Platform cluster, you can test the process on a separate RHEL system. Gather the kernel module's source code, the KVC framework, and the kmod-via-containers software. Then build and test the module. To do that on a RHEL 8 system, do the following:

Register a RHEL 8 system:

Attach a subscription to the RHEL 8 system:

Install software that is required to build the software and container:

Clone the kmod-via-containers repository:

Install a KVC framework instance on your RHEL 8 build host to test the module.
This adds a kmods-via-container systemd service and loads it:

Get the kernel module source code. The source code might be used to
build a third-party module that you do not
have control over, but is supplied by others. You will need content
similar to the content shown in the kvc-simple-kmod example that can
be cloned to your system as follows:

Edit the configuration file, simple-kmod.conf file, in this example, and
change the name of the Dockerfile to Dockerfile.rhel:

Create an instance of kmods-via-containers@.service for your kernel module,
simple-kmod in this example:

Enable the kmods-via-containers@.service instance:

Enable and start the systemd service:

To confirm that the kernel modules are loaded, use the lsmod command to list the modules:

Optional. Use other methods to check that the simple-kmod example is working:


Going forward, when the system boots this service will check if a new kernel is running. If there is a new kernel, the service builds a new version of the kernel module and then loads it. If the module is already built, it will just load it.
Provisioning a kernel module to Red Hat OpenShift Container Platform
Depending on whether or not you must have the kernel module in place when Red Hat OpenShift Container Platform cluster first boots, you can set up the kernel modules to be deployed in one of two ways:

Provision kernel modules at cluster install time (day-1):
You can create the content as a MachineConfig object and provide it to openshift-install
by including it with a set of manifest files.

Provision kernel modules via Machine Config Operator (day-2): If you can wait until the
cluster is up and running to add your kernel module, you can deploy the kernel
module software via the Machine Config Operator (MCO).


In either case, each node needs to be able to get the kernel packages and related software packages at the time that a new kernel is detected. There are a few ways you can set up each node to be able to obtain that content.

Provide RHEL entitlements to each node.

Get RHEL entitlements from an existing RHEL host, from the /etc/pki/entitlement directory
and copy them to the same location as the other files you provide
when you build your Ignition config.

Inside the Dockerfile, add pointers to a yum repository containing the kernel and other packages.
This must include new kernel packages as they are needed to match newly installed kernels.


Provision kernel modules via a MachineConfig object
By packaging kernel module software with a MachineConfig object, you can deliver that software to worker or control plane nodes at installation time or via the Machine Config Operator.

Register a RHEL 8 system:

Attach a subscription to the RHEL 8 system:

Install software needed to build the software:

Create a directory to host the kernel module and tooling:

Get the kmods-via-containers software:

Get your module software. In this example, kvc-simple-kmod is used.

Create a fakeroot directory and populate it with files that you want to
deliver via Ignition, using the repositories cloned earlier:

Clone the fakeroot directory, replacing any symbolic links with copies of their targets, by running the following command:

Create a Butane config file, 99-simple-kmod.bu, that embeds the kernel module tree and enables the systemd service.

Use Butane to generate a machine config YAML file, 99-simple-kmod.yaml, containing the files and configuration to be delivered:

If the cluster is not up yet, generate manifest files and add this file to the
openshift directory. If the cluster is already running, apply the file as follows:

To confirm that the kernel modules are loaded, you can log in to a node
(using oc debug node/<openshift-node>, then chroot /host).
To list the modules, use the lsmod command:
Encrypting and mirroring disks during installation
During an Red Hat OpenShift Container Platform installation, you can enable boot disk encryption and mirroring on the cluster nodes.

About disk encryption
You can enable encryption for the boot disks on the control plane and compute nodes at installation time. Red Hat OpenShift Container Platform supports the Trusted Platform Module (TPM) v2 and Tang encryption modes.


TPM v2
This is the preferred mode.
TPM v2 stores passphrases in a secure cryptoprocessor on the server.
You can use this mode to prevent decryption of the boot disk data on a cluster node if the disk is removed from the server.
Tang
Tang and Clevis are server and client components that enable network-bound disk encryption (NBDE).
You can bind the boot disk data on your cluster nodes to one or more Tang servers.
This prevents decryption of the data unless the nodes are on a secure network where the Tang servers are accessible.
Clevis is an automated decryption framework used to implement decryption on the client side.


The use of the Tang encryption mode to encrypt your disks is only supported for bare metal and vSphere installations on user-provisioned infrastructure.
In earlier versions of Red Hat Enterprise Linux CoreOS (RHCOS), disk encryption was configured by specifying /etc/clevis.json in the Ignition config. That file is not supported in clusters created with Red Hat OpenShift Container Platform 4.7 or later. Configure disk encryption by using the following procedure.

When the TPM v2 or Tang encryption modes are enabled, the RHCOS boot disks are encrypted using the LUKS2 format.

This feature:

Is available for installer-provisioned infrastructure, user-provisioned infrastructure, and Assisted Installer deployments

For Assisted installer deployments:

Is supported on Red Hat Enterprise Linux CoreOS (RHCOS) systems only

Sets up disk encryption during the manifest installation phase, encrypting all data written to disk, from first boot forward

Requires no user intervention for providing passphrases

Uses AES-256-XTS encryption, or AES-256-CBC if FIPS mode is enabled


Configuring an encryption threshold
In Red Hat OpenShift Container Platform, you can specify a requirement for more than one Tang server. You can also configure the TPM v2 and Tang encryption modes simultaneously. This enables boot disk data decryption only if the TPM secure cryptoprocessor is present and the Tang servers are accessible over a secure network.

You can use the threshold attribute in your Butane configuration to define the minimum number of TPM v2 and Tang encryption conditions required for decryption to occur.

The threshold is met when the stated value is reached through any combination of the declared conditions. In the case of offline provisioning, the offline server is accessed using an included advertisement, and only uses that supplied advertisement if the number of online servers do not meet the set threshold.

For example, the threshold value of 2 in the following configuration can be reached by accessing two Tang servers, with the offline server available as a backup, or by accessing the TPM secure cryptoprocessor and one of the Tang servers:

variant: openshift
version: 4.15.0
metadata:
  name: worker-storage
  labels:
    machineconfiguration.openshift.io/role: worker
boot_device:
  layout: x86_64 1
  luks:
    tpm2: true 2
    tang: 3
      - url: http://tang1.example.com:7500
        thumbprint: jwGN5tRFK-kF6pIX89ssF3khxxX
      - url: http://tang2.example.com:7500
        thumbprint: VCJsvZFjBSIHSldw78rOrq7h2ZF
      - url: http://tang3.example.com:7500
        thumbprint: PLjNyRdGw03zlRoGjQYMahSZGu9
        advertisement: "{\"payload\": \"...\", \"protected\": \"...\", \"signature\": \"...\"}" 4
    threshold: 2 5
openshift:
  fips: true
Set this field to the instruction set architecture of the cluster nodes.
Some examples include, x86_64, aarch64, or ppc64le.

Include this field if you want to use a Trusted Platform Module (TPM) to encrypt the root file system.

Include this section if you want to use one or more Tang servers.

Optional: Include this field for offline provisioning. Ignition will provision the Tang server binding rather than fetching the advertisement from the server at runtime. This lets the server be unavailable at provisioning time.

Specify the minimum number of TPM v2 and Tang encryption conditions required for decryption to occur.


The default threshold value is 1. If you include multiple encryption conditions in your configuration but do not specify a threshold, decryption can occur if any of the conditions are met.
If you require TPM v2 and Tang for decryption, the value of the threshold attribute must equal the total number of stated Tang servers plus one. If the threshold value is lower, it is possible to reach the threshold value by using a single encryption mode. For example, if you set tpm2 to true and specify two Tang servers, a threshold of 2 can be met by accessing the two Tang servers, even if the TPM secure cryptoprocessor is not available.
About disk mirroring
During Red Hat OpenShift Container Platform installation on control plane and worker nodes, you can enable mirroring of the boot and other disks to two or more redundant storage devices. A node continues to function after storage device failure provided one device remains available.

Mirroring does not support replacement of a failed disk. Reprovision the node to restore the mirror to a pristine, non-degraded state.

For user-provisioned infrastructure deployments, mirroring is available only on RHCOS systems. Support for mirroring is available on x86_64 nodes booted with BIOS or UEFI and on ppc64le nodes.
Configuring disk encryption and mirroring
You can enable and configure encryption and mirroring during an Red Hat OpenShift Container Platform installation.

You have downloaded the Red Hat OpenShift Container Platform installation program on your installation node.

You installed Butane on your installation node.

You have access to a Red Hat Enterprise Linux (RHEL) 8 machine that can be used to generate a thumbprint of the Tang exchange key.


If you want to use TPM v2 to encrypt your cluster, check to see if TPM v2 encryption needs to be enabled in the host firmware for each node.
This is required on most Dell systems.
Check the manual for your specific system.

If you want to use Tang to encrypt your cluster, follow these preparatory steps:

On your installation node, change to the directory that contains the installation program and generate the Kubernetes manifests for the cluster:

Create a Butane config that configures disk encryption, mirroring, or both.
For example, to configure storage for compute nodes, create a $HOME/clusterconfig/worker-storage.bu file.

Create a control plane or compute node manifest from the corresponding Butane configuration file and save it to the <installation_directory>/openshift directory.
For example, to create a manifest for the compute nodes, run the following command:

Save the Butane configuration file in case you need to update the manifests in the future.

Continue with the remainder of the Red Hat OpenShift Container Platform installation.


After installing Red Hat OpenShift Container Platform, you can verify if boot disk encryption or mirroring is enabled on the cluster nodes.

From the installation host, access a cluster node by using a debug pod:

If you configured boot disk encryption, verify if it is enabled:

If you configured mirroring, verify if it is enabled:

Repeat the verification steps for each Red Hat OpenShift Container Platform node type.


For more information about the TPM v2 and Tang encryption modes, see Configuring automated unlocking of encrypted volumes using policy-based decryption.
Configuring a RAID-enabled data volume
You can enable software RAID partitioning to provide an external data volume. Red Hat OpenShift Container Platform supports RAID 0, RAID 1, RAID 4, RAID 5, RAID 6, and RAID 10 for data protection and fault tolerance. See "About disk mirroring" for more details.

You have downloaded the Red Hat OpenShift Container Platform installation program on your installation node.

You have installed Butane on your installation node.


Create a Butane config that configures a data volume by using software RAID.

Create a RAID manifest from the Butane config you created in the previous step and save it to the <installation_directory>/openshift directory. For example, to create a manifest for the compute nodes, run the following command:

Save the Butane config in case you need to update the manifest in the future.

Continue with the remainder of the Red Hat OpenShift Container Platform installation.
Configuring chrony time service
You can set the time server and related settings used by the chrony time service (chronyd) by modifying the contents of the chrony.conf file and passing those contents to your nodes as a machine config.

Create a Butane config including the contents of the chrony.conf file. For example, to configure chrony on worker nodes, create a 99-worker-chrony.bu file.

Use Butane to generate a MachineConfig object file, 99-worker-chrony.yaml, containing the configuration to be delivered to the nodes:

Apply the configurations in one of two ways:
Additional resources
For information on Butane, see Creating machine configs with Butane.

For information on FIPS support, see Support for FIPS cryptography.