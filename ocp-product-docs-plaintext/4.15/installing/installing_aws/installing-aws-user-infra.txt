Installing a cluster on user-provisioned infrastructure in AWS by using CloudFormation templates

In Red Hat OpenShift Container Platform version 4.15, you can install a cluster on Amazon Web Services (AWS) that uses infrastructure that you provide.

One way to create this infrastructure is to use the provided CloudFormation templates. You can modify the templates to customize your infrastructure or use the information that they contain to create AWS objects according to your company's policies.

The steps for performing a user-provisioned infrastructure installation are provided as an example only. Installing a cluster with infrastructure you provide requires knowledge of the cloud provider and the installation process of Red Hat OpenShift Container Platform. Several CloudFormation templates are provided to assist in completing these steps or to help model your own. You are also free to create the required resources through other methods; the templates are just an example.
Prerequisites
You reviewed details about the Red Hat OpenShift Container Platform installation and update processes.

You read the documentation on selecting a cluster installation method and preparing it for users.

You configured an AWS account to host the cluster.

You downloaded the AWS CLI and installed it on your computer. See Install the AWS CLI Using the Bundled Installer (Linux, macOS, or UNIX) in the AWS documentation.

If you use a firewall, you configured it to allow the sites that your cluster requires access to.

If the cloud identity and access management (IAM) APIs are not accessible in your environment, or if you do not want to store an administrator-level credential secret in the kube-system namespace, you can manually create and maintain long-term credentials.
Internet access for Red Hat OpenShift Container Platform
In Red Hat OpenShift Container Platform 4.15, you require access to the internet to install your cluster.

You must have internet access to:

Access https://console.redhat.com/openshift [OpenShift Cluster Manager Hybrid Cloud Console] to download the installation program and perform subscription management. If the cluster has internet access and you do not disable Telemetry, that service automatically entitles your cluster.

Access Quay.io to obtain the packages that are required to install your cluster.

Obtain the packages that are required to perform cluster updates.
Requirements for a cluster with user-provisioned infrastructure
For a cluster that contains user-provisioned infrastructure, you must deploy all of the required machines.

This section describes the requirements for deploying Red Hat OpenShift Container Platform on user-provisioned infrastructure.

Required machines for cluster installation
The smallest Red Hat OpenShift Container Platform clusters require the following hosts:


To maintain high availability of your cluster, use separate physical hosts for these cluster machines.
The bootstrap and control plane machines must use Red Hat Enterprise Linux CoreOS (RHCOS) as the operating system. However, the compute machines can choose between Red Hat Enterprise Linux CoreOS (RHCOS), Red Hat Enterprise Linux (RHEL) 8.6 and later.

Note that RHCOS is based on Red Hat Enterprise Linux (RHEL) 9.2 and inherits all of its hardware certifications and requirements. See Red Hat Enterprise Linux technology capabilities and limits.
Minimum resource requirements for cluster installation
Each cluster machine must meet the following minimum requirements:


One vCPU is equivalent to one physical core when simultaneous multithreading (SMT), or hyperthreading, is not enabled. When enabled, use the following formula to calculate the corresponding ratio: (threads per core × cores) × sockets = vCPUs.

Red Hat OpenShift Container Platform and Kubernetes are sensitive to disk performance, and faster storage is recommended, particularly for etcd on the control plane nodes which require a 10 ms p99 fsync duration. Note that on many cloud platforms, storage size and IOPS scale together, so you might need to over-allocate storage volume to obtain sufficient performance.

As with all user-provisioned installations, if you choose to use RHEL compute machines in your cluster, you take responsibility for all operating system life cycle management and maintenance, including performing system updates, applying patches, and completing all other required tasks. Use of RHEL 7 compute machines is deprecated and has been removed in Red Hat OpenShift Container Platform 4.10 and later.
If an instance type for your platform meets the minimum requirements for cluster machines, it is supported to use in Red Hat OpenShift Container Platform.

Optimizing storage
Tested instance types for AWS
The following Amazon Web Services (AWS) instance types have been tested with Red Hat OpenShift Container Platform.

Use the machine types included in the following charts for your AWS instances. If you use an instance type that is not listed in the chart, ensure that the instance size you use matches the minimum resource requirements that are listed in the section named "Minimum resource requirements for cluster installation".
https://raw.githubusercontent.com/openshift/installer/master/docs/user/aws/tested_instance_types_x86_64.md
Tested instance types for AWS on 64-bit ARM infrastructures
The following Amazon Web Services (AWS) 64-bit ARM instance types have been tested with Red Hat OpenShift Container Platform.

Use the machine types included in the following charts for your AWS ARM instances. If you use an instance type that is not listed in the chart, ensure that the instance size you use matches the minimum resource requirements that are listed in "Minimum resource requirements for cluster installation".
https://raw.githubusercontent.com/openshift/installer/master/docs/user/aws/tested_instance_types_aarch64.md
Certificate signing requests management
Because your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The kube-controller-manager only approves the kubelet client CSRs. The machine-approver cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.
Required AWS infrastructure components
To install Red Hat OpenShift Container Platform on user-provisioned infrastructure in Amazon Web Services (AWS), you must manually create both the machines and their supporting infrastructure.

For more information about the integration testing for different platforms, see the OpenShift Container Platform 4.x Tested Integrations page.

By using the provided CloudFormation templates, you can create stacks of AWS resources that represent the following components:

An AWS Virtual Private Cloud (VPC)

Networking and load balancing components

Security groups and roles

An Red Hat OpenShift Container Platform bootstrap node

Red Hat OpenShift Container Platform control plane nodes

An Red Hat OpenShift Container Platform compute node


Alternatively, you can manually create the components or you can reuse existing infrastructure that meets the cluster requirements. Review the CloudFormation templates for more details about how the components interrelate.

Other infrastructure components
A VPC

DNS entries

Load balancers (classic or network) and listeners

A public and a private Route 53 zone

Security groups

IAM roles

S3 buckets


If you are working in a disconnected environment, you are unable to reach the public IP addresses for EC2, ELB, and S3 endpoints. Depending on the level to which you want to restrict internet traffic during the installation, the following configuration options are available:


Create a VPC endpoint and attach it to the subnets that the clusters are using. Name the endpoints as follows:

ec2.<aws_region>.amazonaws.com

elasticloadbalancing.<aws_region>.amazonaws.com

s3.<aws_region>.amazonaws.com


With this option, network traffic remains private between your VPC and the required AWS services.


As part of the installation process, you can configure an HTTP or HTTPS proxy. With this option, internet traffic goes through the proxy to reach the required AWS services.


As part of the installation process, you can configure an HTTP or HTTPS proxy with VPC endpoints. Create a VPC endpoint and attach it to the subnets that the clusters are using. Name the endpoints as follows:

ec2.<aws_region>.amazonaws.com

elasticloadbalancing.<aws_region>.amazonaws.com

s3.<aws_region>.amazonaws.com


When configuring the proxy in the install-config.yaml file, add these endpoints to the noProxy field. With this option, the proxy prevents the cluster from accessing the internet directly. However, network traffic remains private between your VPC and the required AWS services.

You must provide a suitable VPC and subnets that allow communication to your machines.


Your DNS and load balancer configuration needs to use a public hosted zone and can use a private hosted zone similar to the one that the installation program uses if it provisions the cluster's infrastructure. You must create a DNS entry that resolves to your load balancer. An entry for api.<cluster_name>.<domain> must point to the external load balancer, and an entry for api-int.<cluster_name>.<domain> must point to the internal load balancer.

The cluster also requires load balancers and listeners for port 6443, which are required for the Kubernetes API and its extensions, and port 22623, which are required for the Ignition config files for new machines. The targets will be the control plane nodes. Port 6443 must be accessible to both clients external to the cluster and nodes within the cluster. Port 22623 must be accessible to nodes within the cluster.


The control plane and worker machines require access to the following ports:


The control plane machines require the following Ingress groups. Each Ingress group is a AWS::EC2::SecurityGroupIngress resource.


The worker machines require the following Ingress groups. Each Ingress group is a AWS::EC2::SecurityGroupIngress resource.


You must grant the machines permissions in AWS. The provided CloudFormation templates grant the machines Allow permissions for the following AWS::IAM::Role objects and provide a AWS::IAM::InstanceProfile for each set of roles. If you do not use the templates, you can grant the machines the following broad permissions or the following individual permissions.
Cluster machines
You need AWS::EC2::Instance objects for the following machines:

A bootstrap machine. This machine is required during installation, but you can remove it after your cluster deploys.

Three control plane machines. The control plane machines are not governed by a control plane machine set.

Compute machines. You must create at least two compute machines, which are also known as worker machines, during installation. These machines are not governed by a compute machine set.
Required AWS permissions for the IAM user
Your IAM user must have the permission tag:GetResources in the region us-east-1 to delete the base cluster resources. As part of the AWS API requirement, the Red Hat OpenShift Container Platform installation program performs various actions in this region.
When you attach the AdministratorAccess policy to the IAM user that you create in Amazon Web Services (AWS), you grant that user all of the required permissions. To deploy all components of an Red Hat OpenShift Container Platform cluster, the IAM user requires the following permissions:

ec2:AttachNetworkInterface

ec2:AuthorizeSecurityGroupEgress

ec2:AuthorizeSecurityGroupIngress

ec2:CopyImage

ec2:CreateNetworkInterface

ec2:CreateSecurityGroup

ec2:CreateTags

ec2:CreateVolume

ec2:DeleteSecurityGroup

ec2:DeleteSnapshot

ec2:DeleteTags

ec2:DeregisterImage

ec2:DescribeAccountAttributes

ec2:DescribeAddresses

ec2:DescribeAvailabilityZones

ec2:DescribeDhcpOptions

ec2:DescribeImages

ec2:DescribeInstanceAttribute

ec2:DescribeInstanceCreditSpecifications

ec2:DescribeInstances

ec2:DescribeInstanceTypes

ec2:DescribeInternetGateways

ec2:DescribeKeyPairs

ec2:DescribeNatGateways

ec2:DescribeNetworkAcls

ec2:DescribeNetworkInterfaces

ec2:DescribePrefixLists

ec2:DescribeRegions

ec2:DescribeRouteTables

ec2:DescribeSecurityGroupRules

ec2:DescribeSecurityGroups

ec2:DescribeSubnets

ec2:DescribeTags

ec2:DescribeVolumes

ec2:DescribeVpcAttribute

ec2:DescribeVpcClassicLink

ec2:DescribeVpcClassicLinkDnsSupport

ec2:DescribeVpcEndpoints

ec2:DescribeVpcs

ec2:GetEbsDefaultKmsKeyId

ec2:ModifyInstanceAttribute

ec2:ModifyNetworkInterfaceAttribute

ec2:RevokeSecurityGroupEgress

ec2:RevokeSecurityGroupIngress

ec2:RunInstances

ec2:TerminateInstances
ec2:AllocateAddress

ec2:AssociateAddress

ec2:AssociateDhcpOptions

ec2:AssociateRouteTable

ec2:AttachInternetGateway

ec2:CreateDhcpOptions

ec2:CreateInternetGateway

ec2:CreateNatGateway

ec2:CreateRoute

ec2:CreateRouteTable

ec2:CreateSubnet

ec2:CreateVpc

ec2:CreateVpcEndpoint

ec2:ModifySubnetAttribute

ec2:ModifyVpcAttribute


If you use an existing Virtual Private Cloud (VPC), your account does not require these permissions for creating network resources.
elasticloadbalancing:AddTags

elasticloadbalancing:ApplySecurityGroupsToLoadBalancer

elasticloadbalancing:AttachLoadBalancerToSubnets

elasticloadbalancing:ConfigureHealthCheck

elasticloadbalancing:CreateListener

elasticloadbalancing:CreateLoadBalancer

elasticloadbalancing:CreateLoadBalancerListeners

elasticloadbalancing:CreateTargetGroup

elasticloadbalancing:DeleteLoadBalancer

elasticloadbalancing:DeregisterInstancesFromLoadBalancer

elasticloadbalancing:DeregisterTargets

elasticloadbalancing:DescribeInstanceHealth

elasticloadbalancing:DescribeListeners

elasticloadbalancing:DescribeLoadBalancerAttributes

elasticloadbalancing:DescribeLoadBalancers

elasticloadbalancing:DescribeTags

elasticloadbalancing:DescribeTargetGroupAttributes

elasticloadbalancing:DescribeTargetHealth

elasticloadbalancing:ModifyLoadBalancerAttributes

elasticloadbalancing:ModifyTargetGroup

elasticloadbalancing:ModifyTargetGroupAttributes

elasticloadbalancing:RegisterInstancesWithLoadBalancer

elasticloadbalancing:RegisterTargets

elasticloadbalancing:SetLoadBalancerPoliciesOfListener


Red Hat OpenShift Container Platform uses both the ELB and ELBv2 API services to provision load balancers. The permission list shows permissions required by both services. A known issue exists in the AWS web console where both services use the same elasticloadbalancing action prefix but do not recognize the same actions. You can ignore the warnings about the service not recognizing certain elasticloadbalancing actions.
iam:AddRoleToInstanceProfile

iam:CreateInstanceProfile

iam:CreateRole

iam:DeleteInstanceProfile

iam:DeleteRole

iam:DeleteRolePolicy

iam:GetInstanceProfile

iam:GetRole

iam:GetRolePolicy

iam:GetUser

iam:ListInstanceProfilesForRole

iam:ListRoles

iam:ListUsers

iam:PassRole

iam:PutRolePolicy

iam:RemoveRoleFromInstanceProfile

iam:SimulatePrincipalPolicy

iam:TagInstanceProfile

iam:TagRole


If you have not created a load balancer in your AWS account, the IAM user also requires the iam:CreateServiceLinkedRole permission.
route53:ChangeResourceRecordSets

route53:ChangeTagsForResource

route53:CreateHostedZone

route53:DeleteHostedZone

route53:GetChange

route53:GetHostedZone

route53:ListHostedZones

route53:ListHostedZonesByName

route53:ListResourceRecordSets

route53:ListTagsForResource

route53:UpdateHostedZoneComment
s3:CreateBucket

s3:DeleteBucket

s3:GetAccelerateConfiguration

s3:GetBucketAcl

s3:GetBucketCors

s3:GetBucketLocation

s3:GetBucketLogging

s3:GetBucketObjectLockConfiguration

s3:GetBucketPolicy

s3:GetBucketRequestPayment

s3:GetBucketTagging

s3:GetBucketVersioning

s3:GetBucketWebsite

s3:GetEncryptionConfiguration

s3:GetLifecycleConfiguration

s3:GetReplicationConfiguration

s3:ListBucket

s3:PutBucketAcl

s3:PutBucketTagging

s3:PutEncryptionConfiguration
s3:DeleteObject

s3:GetObject

s3:GetObjectAcl

s3:GetObjectTagging

s3:GetObjectVersion

s3:PutObject

s3:PutObjectAcl

s3:PutObjectTagging
autoscaling:DescribeAutoScalingGroups

ec2:DeleteNetworkInterface

ec2:DeletePlacementGroup

ec2:DeleteVolume

elasticloadbalancing:DeleteTargetGroup

elasticloadbalancing:DescribeTargetGroups

iam:DeleteAccessKey

iam:DeleteUser

iam:DeleteUserPolicy

iam:ListAttachedRolePolicies

iam:ListInstanceProfiles

iam:ListRolePolicies

iam:ListUserPolicies

s3:DeleteObject

s3:ListBucketVersions

tag:GetResources
ec2:DeleteDhcpOptions

ec2:DeleteInternetGateway

ec2:DeleteNatGateway

ec2:DeleteRoute

ec2:DeleteRouteTable

ec2:DeleteSubnet

ec2:DeleteVpc

ec2:DeleteVpcEndpoints

ec2:DetachInternetGateway

ec2:DisassociateRouteTable

ec2:ReleaseAddress

ec2:ReplaceRouteTableAssociation


If you use an existing VPC, your account does not require these permissions to delete network resources. Instead, your account only requires the tag:UntagResources permission to delete network resources.
kms:CreateGrant

kms:Decrypt

kms:DescribeKey

kms:Encrypt

kms:GenerateDataKey

kms:GenerateDataKeyWithoutPlainText

kms:ListGrants

kms:RevokeGrant
iam:UntagRole
iam:GetUserPolicy

iam:ListAccessKeys

iam:PutUserPolicy

iam:TagUser

s3:AbortMultipartUpload

s3:GetBucketPublicAccessBlock

s3:ListBucket

s3:ListBucketMultipartUploads

s3:PutBucketPublicAccessBlock

s3:PutLifecycleConfiguration


If you are managing your cloud provider credentials with mint mode, the IAM user also requires the iam:CreateAccessKey and iam:CreateUser permissions.
ec2:DescribeInstanceTypeOfferings

servicequotas:ListAWSDefaultServiceQuotas
sts:AssumeRole
Obtaining an AWS Marketplace image
If you are deploying an Red Hat OpenShift Container Platform cluster using an AWS Marketplace image, you must first subscribe through AWS. Subscribing to the offer provides you with the AMI ID that the installation program uses to deploy compute nodes.

You have an AWS account to purchase the offer. This account does not have to be the same account that is used to install the cluster.


Complete the Red Hat OpenShift Container Platform subscription from the AWS Marketplace.

Record the AMI ID for your specific AWS Region. If you use the CloudFormation template to deploy your compute nodes, you must update the worker0.type.properties.ImageID parameter with the AMI ID value.
Obtaining the installation program
Before you install Red Hat OpenShift Container Platform, download the installation file on  the host you are using for installation.

You have a computer that runs Linux or macOS, with 500 MB of local disk space.


Access the Infrastructure Provider page on the OpenShift Cluster Manager site. If you have a Red Hat account, log in with your credentials. If you do not, create an account.

Select your infrastructure provider.

Navigate to the page for your installation type, download the installation program that corresponds with your host operating system and architecture, and place the file in the directory where you will store the installation configuration files.

Extract the installation program. For example, on a computer that uses a Linux
operating system, run the following command:

Download your installation https://console.redhat.com/openshift/install/pull-secret [pull secret from the Red Hat OpenShift Cluster Manager]. This pull secret allows you to authenticate with the services that are provided by the included authorities, including Quay.io, which serves the container images for Red Hat OpenShift Container Platform components.
Generating a key pair for cluster node SSH access
During an Red Hat OpenShift Container Platform installation, you can provide an SSH public key to the installation program. The key is passed to the Red Hat Enterprise Linux CoreOS (RHCOS) nodes through their Ignition config files and is used to authenticate SSH access to the nodes. The key is added to the ~/.ssh/authorized_keys list for the core user on each node, which enables password-less authentication.

After the key is passed to the nodes, you can use the key pair to SSH in to the RHCOS nodes as the user core. To access the nodes through SSH, the private key identity must be managed by SSH for your local user.

If you want to SSH in to your cluster nodes to perform installation debugging or disaster recovery, you must provide the SSH public key during the installation process. The ./openshift-install gather command also requires the SSH public key to be in place on the cluster nodes.

Do not skip this procedure in production environments, where disaster recovery and debugging is required.
You must use a local key, not one that you configured with platform-specific approaches such as AWS key pairs.
If you do not have an existing SSH key pair on your local machine to use for authentication onto your cluster nodes, create one. For example, on a computer that uses a Linux operating system, run the following command:

View the public SSH key:

Add the SSH private key identity to the SSH agent for your local user, if it has not already been added. SSH agent management of the key is required for password-less SSH authentication onto your cluster nodes, or if you want to use the ./openshift-install gather command.

Add your SSH private key to the ssh-agent:


When you install Red Hat OpenShift Container Platform, provide the SSH public key to the installation program.
If you install a cluster on infrastructure that you provision, you must provide the key to the installation program.
Creating the installation files for AWS
To install Red Hat OpenShift Container Platform on Amazon Web Services (AWS) using user-provisioned infrastructure, you must generate the files that the installation program needs to deploy your cluster and modify them so that the cluster creates only the machines that it will use. You generate and customize the install-config.yaml file, Kubernetes manifests, and Ignition config files. You also have the option to first set up a separate var partition during the preparation phases of installation.

Optional: Creating a separate /var partition
It is recommended that disk partitioning for Red Hat OpenShift Container Platform be left to the installer. However, there are cases where you might want to create separate partitions in a part of the filesystem that you expect to grow.

Red Hat OpenShift Container Platform supports the addition of a single partition to attach storage to either the /var partition or a subdirectory of /var. For example:

/var/lib/containers: Holds container-related content that can grow as more images and containers are added to a system.

/var/lib/etcd: Holds data that you might want to keep separate for purposes such as performance optimization of etcd storage.

/var: Holds data that you might want to keep separate for purposes such as auditing.


Storing the contents of a /var directory separately makes it easier to grow storage for those areas as needed and reinstall Red Hat OpenShift Container Platform at a later date and keep that data intact. With this method, you will not have to pull all your containers again, nor will you have to copy massive log files when you update systems.

Because /var must be in place before a fresh installation of Red Hat Enterprise Linux CoreOS (RHCOS), the following procedure sets up the separate /var partition by creating a machine config manifest that is inserted during the openshift-install preparation phases of an Red Hat OpenShift Container Platform installation.

If you follow the steps to create a separate /var partition in this procedure, it is not necessary to create the Kubernetes manifest and Ignition config files again as described later in this section.
Create a directory to hold the Red Hat OpenShift Container Platform installation files:

Run openshift-install to create a set of files in the manifest and openshift subdirectories. Answer the system questions as you are prompted:

Optional: Confirm that the installation program created manifests in the clusterconfig/openshift directory:

Create a Butane config that configures the additional partition. For example, name the file $HOME/clusterconfig/98-var-partition.bu, change the disk device name to the name of the storage device on the worker systems, and set the storage size as appropriate. This example places the /var directory on a separate partition:

Create a manifest from the Butane config and save it to the clusterconfig/openshift directory. For example, run the following command:

Run openshift-install again to create Ignition configs from a set of files in the manifest and openshift subdirectories:


Now you can use the Ignition config files as input to the installation procedures to install Red Hat Enterprise Linux CoreOS (RHCOS) systems.
Creating the installation configuration file
Generate and customize the installation configuration file that the installation program needs to deploy your cluster.

You obtained the Red Hat OpenShift Container Platform installation program
for user-provisioned infrastructure
and the pull secret for your cluster.

You checked that you are deploying your cluster to an AWS Region with an accompanying Red Hat Enterprise Linux CoreOS (RHCOS) AMI published by Red Hat. If you are deploying to an AWS Region that requires a custom AMI, such as an AWS GovCloud Region, you must create the install-config.yaml file manually.


Create the install-config.yaml file.

If you are installing a three-node cluster, modify the install-config.yaml file by setting the compute.replicas parameter to 0. This ensures that the cluster's control planes are schedulable. For more information, see "Installing a three-node cluster on AWS".

Optional: Back up the install-config.yaml file.


See Configuration and credential file settings in the AWS documentation for more information about AWS profile and credential configuration.
Configuring the cluster-wide proxy during installation
Production environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. You can configure a new Red Hat OpenShift Container Platform cluster to use a proxy by configuring the proxy settings in the install-config.yaml file.

You have an existing install-config.yaml file.

You reviewed the sites that your cluster requires access to and determined whether any of them need to bypass the proxy. By default, all cluster egress traffic is proxied, including calls to hosting cloud provider APIs. You added sites to the Proxy object's spec.noProxy field to bypass the proxy if necessary.


Edit your install-config.yaml file and add the proxy settings. For example:

Save the file and reference it when installing Red Hat OpenShift Container Platform.


The installation program creates a cluster-wide proxy that is named cluster that uses the proxy settings in the provided install-config.yaml file. If no proxy settings are provided, a cluster Proxy object is still created, but it will have a nil spec.

Only the Proxy object named cluster is supported, and no additional proxies can be created.
Creating the Kubernetes manifest and Ignition config files
Because you must modify some cluster definition files and manually start the cluster machines, you must generate the Kubernetes manifest and Ignition config files that the cluster needs to configure the machines.

The installation configuration file transforms into the Kubernetes manifests. The manifests wrap into the Ignition configuration files, which are later used to configure the cluster machines.

The Ignition config files that the Red Hat OpenShift Container Platform installation program generates contain certificates that expire after 24 hours, which are then renewed at that time. If the cluster is shut down before renewing the certificates and the cluster is later restarted after the 24 hours have elapsed, the cluster automatically recovers the expired certificates. The exception is that you must manually approve the pending node-bootstrapper certificate signing requests (CSRs) to recover kubelet certificates. See the documentation for Recovering from expired control plane certificates for more information.

It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed. By using the Ignition config files within 12 hours, you can avoid installation failure if the certificate update runs during installation.
You obtained the Red Hat OpenShift Container Platform installation program.

You created the install-config.yaml installation configuration file.


Change to the directory that contains the Red Hat OpenShift Container Platform installation program and generate the Kubernetes manifests for the cluster:

Remove the Kubernetes manifest files that define the control plane machines:

Remove the Kubernetes manifest files that define the control plane machine set:

Check that the mastersSchedulable parameter in the <installation_directory>/manifests/cluster-scheduler-02-config.yml Kubernetes manifest file is set to false. This setting prevents pods from being scheduled on the control plane machines:

Optional: If you do not want
the Ingress Operator
to create DNS records on your behalf, remove the privateZone and publicZone
sections from the <installation_directory>/manifests/cluster-dns-02-config.yml DNS configuration file:

To create the Ignition configuration files, run the following command from the directory that contains the installation program:
Extracting the infrastructure name
The Ignition config files contain a unique cluster identifier that you can use to uniquely identify your cluster in Amazon Web Services (AWS). The infrastructure name is also used to locate the appropriate AWS resources during an Red Hat OpenShift Container Platform installation. The provided CloudFormation templates contain references to this infrastructure name, so you must extract it.

You obtained the Red Hat OpenShift Container Platform installation program and the pull secret for your cluster.

You generated the Ignition config files for your cluster.

You installed the jq package.


To extract and view the infrastructure name from the Ignition config file
metadata, run the following command:
Creating a VPC in AWS
You must create a Virtual Private Cloud (VPC) in Amazon Web Services (AWS) for your Red Hat OpenShift Container Platform cluster to use. You can customize the VPC to meet your requirements, including VPN and route tables.

You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources that represent the VPC.

If you do not use the provided CloudFormation template to create your AWS infrastructure, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.


Create a JSON file that contains the parameter values that the template
requires:

Copy the template from the CloudFormation template for the VPC
section of this topic and save it as a YAML file on your computer. This template
describes the VPC that your cluster requires.

Launch the CloudFormation template to create a stack of AWS resources that represent the VPC:

Confirm that the template components exist:


CloudFormation template for the VPC
You can use the following CloudFormation template to deploy the VPC that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/01_vpc.yaml[role=include]
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.
Creating networking and load balancing components in AWS
You must configure networking and classic or network load balancing in Amazon Web Services (AWS) that your Red Hat OpenShift Container Platform cluster can use.

You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources. The stack represents the networking and load balancing components that your Red Hat OpenShift Container Platform cluster requires. The template also creates a hosted zone and subnet tags.

You can run the template multiple times within a single Virtual Private Cloud (VPC).

If you do not use the provided CloudFormation template to create your AWS infrastructure, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.


Obtain the hosted zone ID for the Route 53 base domain that you specified in the
install-config.yaml file for your cluster. You can obtain details about your hosted zone by running the following command:

Create a JSON file that contains the parameter values that the template
requires:

Copy the template from the CloudFormation template for the network and load balancers
section of this topic and save it as a YAML file on your computer. This template
describes the networking and load balancing objects that your cluster requires.

Launch the CloudFormation template to create a stack of AWS resources that provide the networking and load balancing components:

Confirm that the template components exist:


CloudFormation template for the network and load balancers
You can use the following CloudFormation template to deploy the networking objects and load balancers that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/02_cluster_infra.yaml[role=include]
If you are deploying your cluster to an AWS government or secret region, you must update the InternalApiServerRecord to use CNAME records. Records of type ALIAS are not supported for AWS government regions. For example:

Type: CNAME
TTL: 10
ResourceRecords:
- !GetAtt IntApiElb.DNSName
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.

You can view details about your hosted zones by navigating to the AWS Route 53 console.

See Listing public hosted zones in the AWS documentation for more information about listing public hosted zones.
Creating security group and roles in AWS
You must create security groups and roles in Amazon Web Services (AWS) for your Red Hat OpenShift Container Platform cluster to use.

You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources. The stack represents the security groups and roles that your Red Hat OpenShift Container Platform cluster requires.

If you do not use the provided CloudFormation template to create your AWS infrastructure, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.


Create a JSON file that contains the parameter values that the template
requires:

Copy the template from the CloudFormation template for security objects
section of this topic and save it as a YAML file on your computer. This template
describes the security groups and roles that your cluster requires.

Launch the CloudFormation template to create a stack of AWS resources that represent the security groups and roles:

Confirm that the template components exist:


CloudFormation template for security objects
You can use the following CloudFormation template to deploy the security objects that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/03_cluster_security.yaml[role=include]
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.
Accessing RHCOS AMIs with stream metadata
In Red Hat OpenShift Container Platform, stream metadata provides standardized metadata about RHCOS in the JSON format and injects the metadata into the cluster. Stream metadata is a stable format that supports multiple architectures and is intended to be self-documenting for maintaining automation.

You can use the coreos print-stream-json sub-command of openshift-install to access information about the boot images in the stream metadata format. This command provides a method for printing stream metadata in a scriptable, machine-readable format.

For user-provisioned installations, the openshift-install binary contains references to the version of RHCOS boot images that are tested for use with Red Hat OpenShift Container Platform, such as the AWS AMI.

To parse the stream metadata, use one of the following methods:

From a Go program, use the official stream-metadata-go library at https://github.com/coreos/stream-metadata-go. You can also view example code in the library.

From another programming language, such as Python or Ruby, use the JSON library of your preferred programming language.

From a command-line utility that handles JSON data, such as jq:
RHCOS AMIs for the AWS infrastructure
Red Hat provides Red Hat Enterprise Linux CoreOS (RHCOS) AMIs that are valid for the various AWS regions and instance architectures that you can manually specify for your Red Hat OpenShift Container Platform nodes.

By importing your own AMI, you can also install to regions that do not have a published RHCOS AMI.


AWS regions without a published RHCOS AMI
You can deploy an Red Hat OpenShift Container Platform cluster to Amazon Web Services (AWS) regions without native support for a Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) or the AWS software development kit (SDK). If a published AMI is not available for an AWS region, you can upload a custom AMI prior to installing the cluster.

If you are deploying to a region not supported by the AWS SDK and you do not specify a custom AMI, the installation program copies the us-east-1 AMI to the user account automatically. Then the installation program creates the control plane machines with encrypted EBS volumes using the default or user-specified Key Management Service (KMS) key. This allows the AMI to follow the same process workflow as published RHCOS AMIs.

A region without native support for an RHCOS AMI is not available to select from the terminal during cluster creation because it is not published. However, you can install to this region by configuring the custom AMI in the install-config.yaml file.
Uploading a custom RHCOS AMI in AWS
If you are deploying to a custom Amazon Web Services (AWS) region, you must upload a custom Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) that belongs to that region.

You configured an AWS account.

You created an Amazon S3 bucket with the required IAM
service role.

You uploaded your RHCOS VMDK file to Amazon S3.

You downloaded the AWS CLI and installed it on your computer. See
Install the AWS CLI Using the Bundled Installer.


Export your AWS profile as an environment variable:

Export the region to associate with your custom AMI as an environment
variable:

Export the version of RHCOS you uploaded to Amazon S3 as an environment
variable:

Export the Amazon S3 bucket name as an environment variable:

Create the containers.json file and define your RHCOS VMDK file:

Import the RHCOS disk as an Amazon EBS snapshot:

Check the status of the image import:

Create a custom RHCOS AMI from the RHCOS snapshot:


To learn more about these APIs, see the AWS documentation for importing snapshots and creating EBS-backed AMIs.
Creating the bootstrap node in AWS
You must create the bootstrap node in Amazon Web Services (AWS) to use during Red Hat OpenShift Container Platform cluster initialization. You do this by:

Providing a location to serve the bootstrap.ign Ignition config file to your cluster. This file is located in your installation directory. The provided CloudFormation Template assumes that the Ignition config files for your cluster are served from an S3 bucket. If you choose to serve the files from another location, you must modify the templates.

Using the provided CloudFormation template and a custom parameter file to create a stack of AWS resources. The stack represents the bootstrap node that your Red Hat OpenShift Container Platform installation requires.


If you do not use the provided CloudFormation template to create your bootstrap node, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.

You created and configured DNS, load balancers, and listeners in AWS.

You created the security groups and roles required for your cluster in AWS.


Create the bucket by running the following command:

Upload the bootstrap.ign Ignition config file to the bucket by running the following command:

Verify that the file uploaded by running the following command:

Create a JSON file that contains the parameter values that the template requires:

Copy the template from the CloudFormation template for the bootstrap machine
section of this topic and save it as a YAML file on your computer. This template
describes the bootstrap machine that your cluster requires.

Optional: If you are deploying the cluster with a proxy, you must update the ignition in the template to add the  ignition.config.proxy fields. Additionally, If you have added the Amazon EC2, Elastic Load Balancing, and S3 VPC endpoints to your VPC, you must add these endpoints to the noProxy field.

Launch the CloudFormation template to create a stack of AWS resources that represent the bootstrap node:

Confirm that the template components exist:


CloudFormation template for the bootstrap machine
You can use the following CloudFormation template to deploy the bootstrap machine that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/04_cluster_bootstrap.yaml[role=include]
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.

See RHCOS AMIs for the AWS infrastructure for details about the Red Hat Enterprise Linux CoreOS (RHCOS) AMIs for the AWS zones.
Creating the control plane machines in AWS
You must create the control plane machines in Amazon Web Services (AWS) that your cluster will use.

You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources that represent the control plane nodes.

The CloudFormation template creates a stack that represents three control plane nodes.
If you do not use the provided CloudFormation template to create your control plane nodes, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.

You created and configured DNS, load balancers, and listeners in AWS.

You created the security groups and roles required for your cluster in AWS.

You created the bootstrap machine.


Create a JSON file that contains the parameter values that the template
requires:

Copy the template from the CloudFormation template for control plane machines
section of this topic and save it as a YAML file on your computer. This template
describes the control plane machines that your cluster requires.

If you specified an m5 instance type as the value for MasterInstanceType,
add that instance type to the MasterInstanceType.AllowedValues parameter
in the CloudFormation template.

Launch the CloudFormation template to create a stack of AWS resources that represent the control plane nodes:

Confirm that the template components exist:


CloudFormation template for control plane machines
You can use the following CloudFormation template to deploy the control plane machines that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/05_cluster_master_nodes.yaml[role=include]
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.
Creating the worker nodes in AWS
You can create worker nodes in Amazon Web Services (AWS) for your cluster to use.

If you are installing a three-node cluster, skip this step. A three-node cluster consists of three control plane machines, which also act as compute machines.
You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources that represent a worker node.

The CloudFormation template creates a stack that represents one worker node. You must create a stack for each worker node.
If you do not use the provided CloudFormation template to create your worker nodes, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.

You created and configured DNS, load balancers, and listeners in AWS.

You created the security groups and roles required for your cluster in AWS.

You created the bootstrap machine.

You created the control plane machines.


Create a JSON file that contains the parameter values that the CloudFormation
template requires:

Copy the template from the CloudFormation template for worker machines
section of this topic and save it as a YAML file on your computer. This template
describes the networking objects and load balancers that your cluster requires.

Optional: If you specified an m5 instance type as the value for WorkerInstanceType, add that instance type to the WorkerInstanceType.AllowedValues parameter in the CloudFormation template.

Optional: If you are deploying with an AWS Marketplace image, update the Worker0.type.properties.ImageID parameter with the AMI ID that you obtained from your subscription.

Use the CloudFormation template to create a stack of AWS resources that represent a worker node:

Confirm that the template components exist:

Continue to create worker stacks until you have created enough worker machines for your cluster. You can create additional worker stacks by referencing the same template and parameter files and specifying a different stack name.


CloudFormation template for worker machines
You can use the following CloudFormation template to deploy the worker machines that you need for your Red Hat OpenShift Container Platform cluster.

link:https://raw.githubusercontent.com/openshift/installer/release-4.15/upi/aws/cloudformation/06_cluster_worker_node.yaml[role=include]
You can view details about the CloudFormation stacks that you create by navigating to the AWS CloudFormation console.
Initializing the bootstrap sequence on AWS with user-provisioned infrastructure
After you create all of the required infrastructure in Amazon Web Services (AWS), you can start the bootstrap sequence that initializes the Red Hat OpenShift Container Platform control plane.

You configured an AWS account.

You added your AWS keys and region to your local AWS profile by running aws configure.

You generated the Ignition config files for your cluster.

You created and configured a VPC and associated subnets in AWS.

You created and configured DNS, load balancers, and listeners in AWS.

You created the security groups and roles required for your cluster in AWS.

You created the bootstrap machine.

You created the control plane machines.

You created the worker nodes.


Change to the directory that contains the installation program and start the bootstrap process that initializes the Red Hat OpenShift Container Platform control plane:


See Monitoring installation progress for details about monitoring the installation, bootstrap, and control plane logs as an Red Hat OpenShift Container Platform installation progresses.

See Gathering bootstrap node diagnostic data for information about troubleshooting issues related to the bootstrap process.

You can view details about the running instances that are created by using the AWS EC2 console.
Installing the OpenShift CLI by downloading the binary
You can install the OpenShift CLI (`oc`) to interact with Red Hat OpenShift Container Platform from a command-line interface. You can install oc on Linux, Windows, or macOS.

If you installed an earlier version of oc, you cannot use it to complete all of the commands in Red Hat OpenShift Container Platform 4.15. Download and install the new version of oc.

You can install the OpenShift CLI (oc) binary on Linux by using the following procedure.

Navigate to the Red Hat OpenShift Container Platform downloads page on the Red Hat Customer Portal.

Select the architecture from the Product Variant drop-down list.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v4.15 Linux Client entry and save the file.

Unpack the archive:

Place the oc binary in a directory that is on your PATH.


After you install the OpenShift CLI, it is available using the oc command:



You can install the OpenShift CLI (oc) binary on Windows by using the following procedure.

Navigate to the Red Hat OpenShift Container Platform downloads page on the Red Hat Customer Portal.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v4.15 Windows Client entry and save the file.

Unzip the archive with a ZIP program.

Move the oc binary to a directory that is on your PATH.


After you install the OpenShift CLI, it is available using the oc command:



You can install the OpenShift CLI (oc) binary on macOS by using the following procedure.

Navigate to the Red Hat OpenShift Container Platform downloads page on the Red Hat Customer Portal.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v4.15 macOS Client entry and save the file.

Unpack and unzip the archive.

Move the oc binary to a directory on your PATH.


After you install the OpenShift CLI, it is available using the oc command:
Logging in to the cluster by using the CLI
You can log in to your cluster as a default system user by exporting the cluster kubeconfig file. The kubeconfig file contains information about the cluster that is used by the CLI to connect a client to the correct cluster and API server. The file is specific to a cluster and is created during Red Hat OpenShift Container Platform installation.

You deployed an Red Hat OpenShift Container Platform cluster.

You installed the oc CLI.


Export the kubeadmin credentials:

Verify you can run oc commands successfully using the exported configuration:
Approving the certificate signing requests for your machines
When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.

You added machines to your cluster.


Confirm that the cluster recognizes the machines:

Review the pending CSRs and ensure that you see the client requests with the Pending or Approved status for each machine that you added to the cluster:

If the CSRs were not approved, after all of the pending CSRs for the machines you added are in Pending status, approve the CSRs for your cluster machines:

Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:

If the remaining CSRs are not approved, and are in the Pending status, approve the CSRs for your cluster machines:

After all client and server CSRs have been approved, the machines have the Ready status. Verify this by running the following command:


For more information on CSRs, see Certificate Signing Requests.
Initial Operator configuration
After the control plane initializes, you must immediately configure some Operators so that they all become available.

Your control plane has initialized.


Watch the cluster components come online:

Configure the Operators that are not available.


Image registry storage configuration
Amazon Web Services provides default storage, which means the Image Registry Operator is available after installation. However, if the Registry Operator cannot create an S3 bucket and automatically configure storage, you must manually configure registry storage.

Instructions are shown for configuring a persistent volume, which is required for production clusters. Where applicable, instructions are shown for configuring an empty directory as the storage location, which is available for only non-production clusters.

Additional instructions are provided for allowing the image registry to use block storage types by using the Recreate rollout strategy during upgrades.

You can configure registry storage for user-provisioned infrastructure in AWS to deploy Red Hat OpenShift Container Platform to hidden regions. See Configuring the registry for AWS user-provisioned infrastructure for more information.

Configuring registry storage for AWS with user-provisioned infrastructure
During installation, your cloud credentials are sufficient to create an Amazon S3 bucket and the Registry Operator will automatically configure storage.

If the Registry Operator cannot create an S3 bucket and automatically configure storage, you can create an S3 bucket and configure storage with the following procedure.

You have a cluster on AWS with user-provisioned infrastructure.

For Amazon S3 storage, the secret is expected to contain two keys:


Use the following procedure if the Registry Operator cannot create an S3 bucket and automatically configure storage.

Set up a Bucket Lifecycle Policy
to abort incomplete multipart uploads that are one day old.

Fill in the storage configuration in
configs.imageregistry.operator.openshift.io/cluster:


To secure your registry images in AWS, block public access to the S3 bucket.
Configuring storage for the image registry in non-production clusters
You must configure storage for the Image Registry Operator. For non-production clusters, you can set the image registry to an empty directory. If you do so, all images are lost if you restart the registry.

To set the image registry storage to an empty directory:
Deleting the bootstrap resources
After you complete the initial Operator configuration for the cluster, remove the bootstrap resources from Amazon Web Services (AWS).

You completed the initial Operator configuration for your cluster.


Delete the bootstrap resources. If you used the CloudFormation template,
delete its stack:
Creating the Ingress DNS Records
If you removed the DNS Zone configuration, manually create DNS records that point to the Ingress load balancer. You can create either a wildcard record or specific records. While the following procedure uses A records, you can use other record types that you require, such as CNAME or alias.

You deployed an Red Hat OpenShift Container Platform cluster on Amazon Web Services (AWS) that uses infrastructure that you provisioned.

You installed the OpenShift CLI (oc).

You installed the jq package.

You downloaded the AWS CLI and installed it on your computer. See
Install the AWS CLI Using the Bundled Installer (Linux, macOS, or Unix).


Determine the routes to create.

Retrieve the Ingress Operator load balancer status and note the value of the external IP address that it uses, which is shown in the EXTERNAL-IP column:

Locate the hosted zone ID for the load balancer:

Obtain the public hosted zone ID for your cluster's domain:

Add the alias records to your private zone:

Add the records to your public zone:
Completing an AWS installation on user-provisioned infrastructure
After you start the Red Hat OpenShift Container Platform installation on Amazon Web Service (AWS) user-provisioned infrastructure, monitor the deployment to completion.

You removed the bootstrap node for an Red Hat OpenShift Container Platform cluster on user-provisioned AWS infrastructure.

You installed the oc CLI.


From the directory that contains the installation program, complete
the cluster installation:
Logging in to the cluster by using the web console
The kubeadmin user exists by default after an Red Hat OpenShift Container Platform installation. You can log in to your cluster as the kubeadmin user by using the Red Hat OpenShift Container Platform web console.

You have access to the installation host.

You completed a cluster installation and all cluster Operators are available.


Obtain the password for the kubeadmin user from the kubeadmin-password file on the installation host:

List the Red Hat OpenShift Container Platform web console route:

Navigate to the route detailed in the output of the preceding command in a web browser and log in as the kubeadmin user.


See Accessing the web console for more details about accessing and understanding the Red Hat OpenShift Container Platform web console.
Telemetry access for Red Hat OpenShift Container Platform
In Red Hat OpenShift Container Platform 4.15, the Telemetry service, which runs by default to provide metrics about cluster health and the success of updates, requires internet access. If your cluster is connected to the internet, Telemetry runs automatically, and your cluster is registered to https://console.redhat.com/openshift [OpenShift Cluster Manager Hybrid Cloud Console].

After you confirm that your https://console.redhat.com/openshift [OpenShift Cluster Manager Hybrid Cloud Console] inventory is correct, either maintained automatically by Telemetry or manually by using OpenShift Cluster Manager, use subscription watch to track your Red Hat OpenShift Container Platform subscriptions at the account or multi-cluster level.

See About remote health monitoring for more information about the Telemetry service.
Additional resources
See Working with stacks in the AWS documentation for more information about AWS CloudFormation stacks.
Next steps
Validating an installation.

Customize your cluster.

If necessary, you can opt out of remote health reporting.

If necessary, you can remove cloud provider credentials.