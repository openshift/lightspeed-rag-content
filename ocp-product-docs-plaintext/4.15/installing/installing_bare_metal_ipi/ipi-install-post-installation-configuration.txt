Installer-provisioned postinstallation configuration

After successfully deploying an installer-provisioned cluster, consider the following postinstallation procedures.
Optional: Configuring NTP for disconnected clusters
Red Hat OpenShift Container Platform installs the chrony Network Time Protocol (NTP) service on the cluster nodes. Use the following procedure to configure NTP servers on the control plane nodes and configure worker nodes as NTP clients of the control plane nodes after a successful deployment.


Red Hat OpenShift Container Platform nodes must agree on a date and time to run properly. When worker nodes retrieve the date and time from the NTP servers on the control plane nodes, it enables the installation and operation of clusters that are not connected to a routable network and thereby do not have access to a higher stratum NTP server.

Create a Butane config, 99-master-chrony-conf-override.bu, including the contents of the chrony.conf file for the control plane nodes.

Use Butane to generate a MachineConfig object file, 99-master-chrony-conf-override.yaml, containing the configuration to be delivered to the control plane nodes:

Create a Butane config, 99-worker-chrony-conf-override.bu, including the contents of the chrony.conf file for the worker nodes that references the NTP servers on the control plane nodes.

Use Butane to generate a MachineConfig object file, 99-worker-chrony-conf-override.yaml, containing the configuration to be delivered to the worker nodes:

Apply the 99-master-chrony-conf-override.yaml policy to the control plane nodes.

Apply the 99-worker-chrony-conf-override.yaml policy to the worker nodes.

Check the status of the applied NTP settings.
Enabling a provisioning network after installation
The assisted installer and installer-provisioned installation for bare metal clusters provide the ability to deploy a cluster without a provisioning network. This capability is for scenarios such as proof-of-concept clusters or deploying exclusively with Redfish virtual media when each node's baseboard management controller is routable via the baremetal network.

You can enable a provisioning network after installation using the Cluster Baremetal Operator (CBO).

A dedicated physical network must exist, connected to all worker and control plane nodes.

You must isolate the native, untagged physical network.

The network cannot have a DHCP server when the provisioningNetwork configuration setting is set to Managed.

You can omit the provisioningInterface setting in Red Hat OpenShift Container Platform 4.10 to use the bootMACAddress configuration setting.


When setting the provisioningInterface setting, first identify the provisioning interface name for the cluster nodes. For example, eth0 or eno1.

Enable the Preboot eXecution Environment (PXE) on the provisioning network interface of the cluster nodes.

Retrieve the current state of the provisioning network and save it to a provisioning custom resource (CR) file:

Modify the provisioning CR file:

Save the changes to the provisioning CR file.

Apply the provisioning CR file to the cluster:
Services for an external load balancer
You can configure an Red Hat OpenShift Container Platform cluster to use an external load balancer in place of the default load balancer.

Configuring an external load balancer depends on your vendor's load balancer.

The information and examples in this section are for guideline purposes only. Consult the vendor documentation for more specific information about the vendor's load balancer.
Red Hat supports the following services for an external load balancer:

Ingress Controller

OpenShift API

OpenShift MachineConfig API


You can choose whether you want to configure one or all of these services for an external load balancer. Configuring only the Ingress Controller service is a common configuration option. To better understand each service, view the following diagrams:




The following configuration options are supported for external load balancers:

Use a node selector to map the Ingress Controller to a specific set of nodes. You must assign a static IP address to each node in this set, or configure each node to receive the same IP address from the Dynamic Host Configuration Protocol (DHCP). Infrastructure nodes commonly receive this type of configuration.

Target all IP addresses on a subnet. This configuration can reduce maintenance overhead, because you can create and destroy nodes within those networks without reconfiguring the load balancer targets. If you deploy your ingress pods by using a machine set on a smaller network, such as a /27 or /28, you can simplify your load balancer targets.


Before you configure an external load balancer for your Red Hat OpenShift Container Platform cluster, consider the following information:

For a front-end IP address, you can use the same IP address for the front-end IP address, the Ingress Controller's load balancer, and API load balancer. Check the vendor's documentation for this capability.

For a back-end IP address, ensure that an IP address for an Red Hat OpenShift Container Platform control plane node does not change during the lifetime of the external load balancer. You can achieve this by completing one of the following actions:

Manually define each node that runs the Ingress Controller in the external load balancer for the Ingress Controller back-end service. For example, if the Ingress Controller moves to an undefined node, a connection outage can occur.


Configuring an external load balancer
You can configure an Red Hat OpenShift Container Platform cluster to use an external load balancer in place of the default load balancer.

Before you configure an external load balancer, ensure that you read the "Services for an external load balancer" section.
Read the following prerequisites that apply to the service that you want to configure for your external load balancer.

MetalLB, that runs on a cluster, functions as an external load balancer.
You defined a front-end IP address.

TCP ports 6443 and 22623 are exposed on the front-end IP address of your load balancer. Check the following items:

The front-end IP address and port 6443 are reachable by all users of your system with a location external to your Red Hat OpenShift Container Platform cluster.

The front-end IP address and port 22623 are reachable only by Red Hat OpenShift Container Platform nodes.

The load balancer backend can communicate with Red Hat OpenShift Container Platform control plane nodes on port 6443 and 22623.


You defined a front-end IP address.

TCP ports 443 and 80 are exposed on the front-end IP address of your load balancer.

The front-end IP address, port 80 and port 443 are be reachable by all users of your system with a location external to your Red Hat OpenShift Container Platform cluster.

The front-end IP address, port 80 and port 443 are reachable to all nodes that operate in your Red Hat OpenShift Container Platform cluster.

The load balancer backend can communicate with Red Hat OpenShift Container Platform nodes that run the Ingress Controller on ports 80, 443, and 1936.


You can configure most load balancers by setting health check URLs that determine if a service is available or unavailable. Red Hat OpenShift Container Platform provides these health checks for the OpenShift API, Machine Configuration API, and Ingress Controller backend services.

The following examples demonstrate health check specifications for the previously listed backend services:

Path: HTTPS:6443/readyz
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 10
Interval: 10
Path: HTTPS:22623/healthz
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 10
Interval: 10
Path: HTTP:1936/healthz/ready
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 5
Interval: 10
Configure the HAProxy Ingress Controller, so that you can enable access to the cluster from your load balancer on ports 6443, 443, and 80:

Use the curl CLI command to verify that the external load balancer and its resources are operational:

Configure the DNS records for your cluster to target the front-end IP addresses of the external load balancer. You must update records to your DNS server for the cluster API and applications over the load balancer.

Use the curl CLI command to verify that the external load balancer and DNS record configuration are operational: