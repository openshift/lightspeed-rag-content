Troubleshooting

Troubleshooting the installer workflow
Prior to troubleshooting the installation environment, it is critical to understand the overall flow of the installer-provisioned installation on bare metal. The diagrams below provide a troubleshooting flow with a step-by-step breakdown for the environment.



Workflow 1 of 4 illustrates a troubleshooting workflow when the install-config.yaml file has errors or the Red Hat Enterprise Linux CoreOS (RHCOS) images are inaccessible.  Troubleshooting suggestions can be found at Troubleshooting install-config.yaml.



Workflow 2 of 4 illustrates a troubleshooting workflow for  bootstrap VM issues,  bootstrap VMs that cannot boot up the cluster nodes, and   inspecting logs. When installing an "Red Hat OpenShift Container Platform" cluster without the provisioning network, this workflow does not apply.



Workflow 3 of 4 illustrates a troubleshooting workflow for  cluster nodes that will not PXE boot. If installing using RedFish Virtual Media, each node must meet minimum firmware requirements for the installer to deploy the node. See Firmware requirements for installing with virtual media in the Prerequisites section for additional details.



Workflow 4 of 4 illustrates a troubleshooting workflow from  a non-accessible API to a validated installation.
Troubleshooting install-config.yaml
The install-config.yaml configuration file represents all of the nodes that are part of the "Red Hat OpenShift Container Platform" cluster. The file contains the necessary options consisting of but not limited to apiVersion, baseDomain, imageContentSources and virtual IP addresses. If errors occur early in the deployment of the "Red Hat OpenShift Container Platform" cluster, the errors are likely in the install-config.yaml configuration file.

Use the guidelines in YAML-tips.

Verify the YAML syntax is correct using syntax-check.

Verify the Red Hat Enterprise Linux CoreOS (RHCOS) QEMU images are properly defined and accessible via the URL provided in the install-config.yaml. For example:
Bootstrap VM issues
The "Red Hat OpenShift Container Platform" installation program spawns a bootstrap node virtual machine, which handles provisioning the "Red Hat OpenShift Container Platform" cluster nodes.

About 10 to 15 minutes after triggering the installation program, check to ensure the bootstrap VM is operational using the virsh command:

Verify libvirtd is running on the system:

Use the virsh console command to find the IP address of the bootstrap VM:

After you obtain the IP address, log in to the bootstrap VM using the ssh command:


If you are not successful logging in to the bootstrap VM, you have likely encountered one of the following scenarios:

You cannot reach the 172.22.0.0/24 network. Verify the network connectivity between the provisioner and the provisioning network bridge. This issue might occur if you are using a provisioning network.
`

You cannot reach the bootstrap VM through the public network. When attempting
to SSH via baremetal network, verify connectivity on the
provisioner host specifically around the baremetal network bridge.

You encountered Permission denied (publickey,password,keyboard-interactive). When
attempting to access the bootstrap VM, a Permission denied error
might occur. Verify that the SSH key for the user attempting to log
in to the VM is set within the install-config.yaml file.


Bootstrap VM cannot boot up the cluster nodes
During the deployment, it is possible for the bootstrap VM to fail to boot the cluster nodes, which prevents the VM from provisioning the nodes with the RHCOS image. This scenario can arise due to:

A problem with the install-config.yaml file.

Issues with out-of-band network access when using the baremetal network.


To verify the issue, there are three containers related to ironic:

ironic

ironic-inspector


Log in to the bootstrap VM:

To check the container logs, execute the following:


The cluster nodes might be in the ON state when deployment started.

Power off the "Red Hat OpenShift Container Platform" cluster nodes before you begin the installation over IPMI:

$ ipmitool -I lanplus -U root -P <password> -H <out_of_band_ip> power off
Inspecting logs
When experiencing issues downloading or accessing the RHCOS images, first verify that the URL is correct in the install-config.yaml configuration file.

bootstrapOSImage: http://<ip:port>/rhcos-43.81.202001142154.0-qemu.<architecture>.qcow2.gz?sha256=9d999f55ff1d44f7ed7c106508e5deecd04dc3c06095d34d36bf1cd127837e0c
clusterOSImage: http://<ip:port>/rhcos-43.81.202001142154.0-openstack.<architecture>.qcow2.gz?sha256=a1bda656fa0892f7b936fdc6b6a6086bddaed5dafacedcd7a1e811abb78fe3b0
The coreos-downloader container downloads resources from a webserver or from the external quay.io registry, whichever the install-config.yaml configuration file specifies. Verify that the coreos-downloader container is up and running and inspect its logs as needed.

Log in to the bootstrap VM:

Check the status of the coreos-downloader container within the bootstrap VM by running the following command:

To inspect the bootkube logs that indicate if all the containers launched during the deployment phase, execute the following:

Verify all the pods, including dnsmasq, mariadb, httpd, and ironic, are running:

If there are issues with the pods, check the logs of the containers with issues. To check the logs of the ironic service, run the following command:
Cluster nodes will not PXE boot
When "Red Hat OpenShift Container Platform" cluster nodes will not PXE boot, execute the following checks on the cluster nodes that will not PXE boot. This procedure does not apply when installing an "Red Hat OpenShift Container Platform" cluster without the provisioning network.

Check the network connectivity to the provisioning network.

Ensure PXE is enabled on the NIC for the provisioning network and PXE is disabled for all other NICs.

Verify that the install-config.yaml configuration file has the proper hardware profile and boot MAC address for the NIC connected to the provisioning network. For example:
Unable to discover new bare metal hosts using the BMC
In some cases, the installation program will not be able to discover the new bare metal hosts and issue an error, because it cannot mount the remote virtual media share.

For example:

ProvisioningError 51s metal3-baremetal-controller Image provisioning failed: Deploy step deploy.deploy failed with BadRequestError: HTTP POST
https://<bmc_address>/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia
returned code 400.
Base.1.8.GeneralError: A general error has occurred. See ExtendedInfo for more information
Extended information: [
  {
    "Message": "Unable to mount remote share https://<ironic_address>/redfish/boot-<uuid>.iso.",
    "MessageArgs": [
      "https://<ironic_address>/redfish/boot-<uuid>.iso"
    ],
    "MessageArgs@odata.count": 1,
    "MessageId": "IDRAC.2.5.RAC0720",
    "RelatedProperties": [
      "#/Image"
    ],
    "RelatedProperties@odata.count": 1,
    "Resolution": "Retry the operation.",
    "Severity": "Informational"
  }
].
In this situation, if you are using virtual media with an unknown certificate authority, you can configure your baseboard management controller (BMC) remote file share settings to trust an unknown certificate authority to avoid this error.

This resolution was tested on "Red Hat OpenShift Container Platform" 4.11 with Dell iDRAC 9 and firmware version 5.10.50.
The API is not accessible
When the cluster is running and clients cannot access the API, domain name resolution issues might impede access to the API.

Hostname Resolution: Check the cluster nodes to ensure they have a fully qualified domain name, and not just localhost.localdomain. For example:

Incorrect Name Resolution: Ensure that each node has the correct name resolution in the DNS server using dig and nslookup. For example:
Troubleshooting worker nodes that cannot join the cluster
Installer-provisioned clusters deploy with a DNS server that includes a DNS entry for the api-int.<cluster_name>.<base_domain> URL. If the nodes within the cluster use an external or upstream DNS server to resolve the api-int.<cluster_name>.<base_domain> URL and there is no such entry, worker nodes might fail to join the cluster. Ensure that all nodes in the cluster can resolve the domain name.

Add a DNS A/AAAA or CNAME record to internally identify the API load balancer. For example, when using dnsmasq, modify the dnsmasq.conf configuration file:

Add a DNS PTR record to internally identify the API load balancer. For example, when using dnsmasq, modify the dnsmasq.conf configuration file:

Restart the DNS server. For example, when using dnsmasq, execute the following command:


These records must be resolvable from all the nodes within the cluster.
Cleaning up previous installations
In the event of a previous failed deployment, remove the artifacts from the failed attempt before attempting to deploy "Red Hat OpenShift Container Platform" again.

Power off all bare metal nodes prior to installing the "Red Hat OpenShift Container Platform" cluster:

Remove all old bootstrap resources if any are left over from a previous deployment attempt:

Remove the following from the clusterconfigs directory to prevent Terraform from failing:
Issues with creating the registry
When creating a disconnected registry, you might encounter a "User Not Authorized" error when attempting to mirror the registry. This error might occur if you fail to append the new authentication to the existing pull-secret.txt file.

Check to ensure authentication is successful:

After mirroring the registry, confirm that you can access it in your
disconnected environment:
Miscellaneous issues
Addressing the runtime network not ready error
After the deployment of a cluster you might receive the following error:

`runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: Missing CNI default network`
The Cluster Network Operator is responsible for deploying the networking components in response to a special object created by the installer. It runs very early in the installation process, after the control plane (master) nodes have come up, but before the bootstrap control plane has been torn down. It can be indicative of more subtle installer issues, such as long delays in bringing up control plane (master) nodes or issues with apiserver communication.

Inspect the pods in the openshift-network-operator namespace:

On the provisioner node, determine that the network configuration exists:

Check that the network-operator is running:

Retrieve the logs:
Addressing the "No disk found with matching rootDeviceHints" error message
After you deploy a cluster, you might receive the following error message:

No disk found with matching rootDeviceHints
To address the No disk found with matching rootDeviceHints error message, a temporary workaround is to change the rootDeviceHints to minSizeGigabytes: 300.

After you change the rootDeviceHints settings, boot the CoreOS and then verify the disk information by using the following command:

$ udevadm info /dev/sda
If you are using DL360 Gen 10 servers, be aware that they have an SD-card slot that might be assigned the /dev/sda device name. If no SD card is present in the server, it can cause conflicts. Ensure that the SD card slot is disabled in the server's BIOS settings.

If the minSizeGigabytes workaround is not fulfilling the requirements, you might need to revert rootDeviceHints back to /dev/sda. This change allows ironic images to boot successfully.

An alternative approach to fixing this problem is by using the serial ID of the disk. However, be aware that finding the serial ID can be challenging and might make the configuration file less readable. If you choose this path, ensure that you gather the serial ID using the previously documented command and incorporate it into your configuration.
Cluster nodes not getting the correct IPv6 address over DHCP
If the cluster nodes are not getting the correct IPv6 address over DHCP, check the following:

Ensure the reserved IPv6 addresses reside outside the DHCP range.

In the IP address reservation on the DHCP server, ensure the reservation specifies the correct DHCP Unique Identifier (DUID). For example:

Ensure that route announcements are working.

Ensure that the DHCP server is listening on the required interfaces serving the IP address ranges.
Cluster nodes not getting the correct hostname over DHCP
During IPv6 deployment, cluster nodes must get their hostname over DHCP. Sometimes the NetworkManager does not assign the hostname immediately. A control plane (master) node might report an error such as:

Failed Units: 2
  NetworkManager-wait-online.service
  nodeip-configuration.service
This error indicates that the cluster node likely booted without first receiving a hostname from the DHCP server, which causes kubelet to boot with a localhost.localdomain hostname. To address the error, force the node to renew the hostname.

Retrieve the hostname:

Force the cluster node to renew the DHCP lease:

Check hostname again:

If the hostname is still localhost.localdomain, restart NetworkManager:

If the hostname is still localhost.localdomain, wait a few minutes and check again. If the hostname remains  localhost.localdomain, repeat the previous steps.

Restart the nodeip-configuration service:

Reload the unit files definition since the kubelet changed in the previous step:

Restart the kubelet service:

Ensure kubelet booted with the correct hostname:


If the cluster node is not getting the correct hostname over DHCP after the cluster is up and running, such as during a reboot, the cluster will have a pending csr. Do not approve a csr, or other issues might arise.

Get CSRs on the cluster:

Verify if a pending csr contains Subject Name: localhost.localdomain:

Remove any csr that contains Subject Name: localhost.localdomain:
Routes do not reach endpoints
During the installation process, it is possible to encounter a Virtual Router Redundancy Protocol (VRRP) conflict. This conflict might occur if a previously used "Red Hat OpenShift Container Platform" node that was once part of a cluster deployment using a specific cluster name is still running but not part of the current "Red Hat OpenShift Container Platform" cluster deployment using that same cluster name. For example, a cluster was deployed using the cluster name openshift, deploying three control plane (master) nodes and three worker nodes. Later, a separate install uses the same cluster name openshift, but this redeployment only installed three control plane (master) nodes, leaving the three worker nodes from a previous deployment in an ON state. This might cause a Virtual Router Identifier (VRID) conflict and a VRRP conflict.

Get the route:

Check the service endpoint:

Attempt to reach the service from a control plane (master) node:

Identify the authentication-operator errors from the provisioner node:


Ensure that the cluster name for every deployment is unique, ensuring no conflict.

Turn off all the rogue nodes which are not part of the cluster deployment that are using the same cluster name. Otherwise, the authentication pod of the  "Red Hat OpenShift Container Platform" cluster might never start successfully.
Failed Ignition during Firstboot
During the Firstboot, the Ignition configuration may fail.

Connect to the node where the Ignition configuration failed:

Restart the machine-config-daemon-firstboot service:
NTP out of sync
The deployment of "Red Hat OpenShift Container Platform" clusters depends on NTP synchronized clocks among the cluster nodes. Without synchronized clocks, the deployment may fail due to clock drift if the time difference is greater than two seconds.

Check for differences in the AGE of the cluster nodes. For example:

Check for inconsistent timing delays due to clock drift. For example:


Create a Butane config file including the contents of the chrony.conf file to be delivered to the nodes. In the following example, create 99-master-chrony.bu to add the file to the control plane nodes. You can modify the file for worker nodes or repeat this procedure for the worker role.

Use Butane to generate a MachineConfig object file, 99-master-chrony.yaml, containing the configuration to be delivered to the nodes:

Apply the MachineConfig object file:

Ensure the System clock synchronized value is yes:
Reviewing the installation
After installation, ensure the installer deployed the nodes and pods successfully.

When the "Red Hat OpenShift Container Platform" cluster nodes are installed appropriately, the following Ready state is seen within the STATUS column:

Confirm the installer deployed all pods successfully. The following command
removes any pods that are still running or have completed as part of the output.