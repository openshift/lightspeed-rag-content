Prerequisites

Installer-provisioned installation of "Red Hat OpenShift Container Platform" requires:

One provisioner node with Red Hat Enterprise Linux (RHEL) 9.x installed. The provisioner can be removed after installation.

Three control plane nodes

Baseboard management controller (BMC) access to each node

At least one network:


Before starting an installer-provisioned installation of "Red Hat OpenShift Container Platform", ensure the hardware environment meets the following requirements.
Node requirements
Installer-provisioned installation involves a number of hardware node requirements:

CPU architecture: All nodes must use x86_64
or aarch64
CPU architecture.

Similar nodes: Red Hat recommends nodes have an identical configuration per role. That is, Red Hat recommends nodes be the same brand and model with the same CPU, memory, and storage configuration.

Baseboard Management Controller: The provisioner node must be able to access the baseboard management controller (BMC) of each "Red Hat OpenShift Container Platform" cluster node. You may use IPMI, Redfish, or a proprietary protocol.

Latest generation: Nodes must be of the most recent generation. Installer-provisioned installation relies on BMC protocols, which must be compatible across nodes. Additionally, RHEL 9.x ships with the most recent drivers for RAID controllers. Ensure that the nodes are recent enough to support RHEL 9.x for the provisioner node and RHCOS 9.x for the control plane and worker nodes.

Registry node: (Optional) If setting up a disconnected mirrored registry, it is recommended the registry reside in its own node.

Provisioner node: Installer-provisioned installation requires one provisioner node.

Control plane: Installer-provisioned installation requires three control plane nodes for high availability. You can deploy an "Red Hat OpenShift Container Platform" cluster with only three control plane nodes, making the control plane nodes schedulable as worker nodes. Smaller clusters are more resource efficient for administrators and developers during development, production, and testing.

Worker nodes: While not required, a typical production cluster has two or more worker nodes.

Network interfaces: Each node must have at least one network interface for the routable baremetal network. Each node must have one network interface for a provisioning network when using the provisioning network for deployment. Using the provisioning network is the default configuration.

Unified Extensible Firmware Interface (UEFI): Installer-provisioned installation requires UEFI boot on all "Red Hat OpenShift Container Platform" nodes when using IPv6 addressing on the provisioning network. In addition, UEFI Device PXE Settings must be set to use the IPv6 protocol on the provisioning network NIC, but omitting the provisioning network removes this requirement.

Secure Boot: Many production scenarios require nodes with Secure Boot enabled to verify the node only boots with trusted software, such as UEFI firmware drivers, EFI applications, and the operating system. You may deploy with Secure Boot manually or managed.
Planning a bare metal cluster for OpenShift Virtualization
If you will use OpenShift Virtualization, it is important to be aware of several requirements before you install your bare metal cluster.

If you want to use live migration features, you must have multiple worker nodes at the time of cluster installation. This is because live migration requires the cluster-level high availability (HA) flag to be set to true. The HA flag is set when a cluster is installed and cannot be changed afterwards. If there are fewer than two worker nodes defined when you install your cluster, the HA flag is set to false for the life of the cluster.

Live migration requires shared storage. Storage for OpenShift Virtualization must support and use the ReadWriteMany (RWX) access mode.

If you plan to use Single Root I/O Virtualization (SR-IOV), ensure that your network interface controllers (NICs) are supported by "Red Hat OpenShift Container Platform".


Preparing your cluster for OpenShift Virtualization

About Single Root I/O Virtualization (SR-IOV) hardware networks

Connecting a virtual machine to an SR-IOV network
Firmware requirements for installing with virtual media
The installation program for installer-provisioned "Red Hat OpenShift Container Platform" clusters validates the hardware and firmware compatibility with Redfish virtual media. The installation program does not begin installation on a node if the node firmware is not compatible. The following tables list the minimum firmware versions tested and verified to work for installer-provisioned "Red Hat OpenShift Container Platform" clusters deployed by using Redfish virtual media.

Red Hat does not test every combination of firmware, hardware, or other third-party components. For further information about third-party support, see Red Hat third-party support policy. For information about updating the firmware, see the hardware documentation for the nodes or contact the hardware vendor.


For Dell servers, ensure the "Red Hat OpenShift Container Platform" cluster nodes have AutoAttach enabled through the iDRAC console. The menu path is Configuration -> Virtual Media -> Attach Mode -> AutoAttach . With iDRAC 9 firmware version 04.40.00.00 and all releases up to including the 5.xx series, the virtual console plugin defaults to eHTML5, an enhanced version of HTML5, which causes problems with the InsertVirtualMedia workflow. Set the plugin to use HTML5 to avoid this issue. The menu path is Configuration -> Virtual console -> Plug-in Type -> HTML5 .
Unable to discover new bare metal hosts using the BMC
Network requirements
Installer-provisioned installation of "Red Hat OpenShift Container Platform" involves several network requirements. First, installer-provisioned installation involves an optional non-routable provisioning network for provisioning the operating system on each bare metal node. Second, installer-provisioned installation involves a routable baremetal network.


Ensuring required ports are open
Certain ports must be open between cluster nodes for installer-provisioned installations to complete successfully. In certain situations, such as using separate subnets for far edge worker nodes, you must ensure that the nodes in these subnets can communicate with nodes in the other subnets on the following required ports.
Increase the network MTU
Before deploying "Red Hat OpenShift Container Platform", increase the network maximum transmission unit (MTU) to 1500 or more. If the MTU is lower than 1500, the Ironic image that is used to boot the node might fail to communicate with the Ironic inspector pod, and inspection will fail. If this occurs, installation stops because the nodes are not available for installation.
Configuring NICs
"Red Hat OpenShift Container Platform" deploys with two networks:

provisioning: The provisioning network is an optional non-routable network used for provisioning the underlying operating system on each node that is a part of the "Red Hat OpenShift Container Platform" cluster. The network interface for the provisioning network on each cluster node must have the BIOS or UEFI configured to PXE boot.

baremetal: The baremetal network is a routable network. You can use any NIC to interface with the baremetal network provided the NIC is not configured to use the provisioning network.


When using a VLAN, each NIC must be on a separate VLAN corresponding to the appropriate network.
DNS requirements
Clients access the "Red Hat OpenShift Container Platform" cluster nodes over the baremetal network. A network administrator must configure a subdomain or subzone where the canonical name extension is the cluster name.

<cluster_name>.<base_domain>
For example:

test-cluster.example.com
"Red Hat OpenShift Container Platform" includes functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. After the nodes are registered with the API, the cluster can disperse node information without using CoreDNS-mDNS. This eliminates the network traffic associated with multicast DNS.

In "Red Hat OpenShift Container Platform" deployments, DNS name resolution is required for the following components:

The Kubernetes API

The "Red Hat OpenShift Container Platform" application wildcard ingress API


A/AAAA records are used for name resolution and PTR records are used for reverse name resolution. Red Hat Enterprise Linux CoreOS (RHCOS) uses the reverse records or DHCP to set the hostnames for all the nodes.

Installer-provisioned installation includes functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. In each record, <cluster_name> is the cluster name and <base_domain> is the base domain that you specify in the install-config.yaml file. A complete DNS record takes the form: <component>.<cluster_name>.<base_domain>..


You can use the dig command to verify DNS resolution.
Dynamic Host Configuration Protocol (DHCP) requirements
By default, installer-provisioned installation deploys ironic-dnsmasq with DHCP enabled for the provisioning network. No other DHCP servers should be running on the provisioning network when the provisioningNetwork configuration setting is set to managed, which is the default value. If you have a DHCP server running on the provisioning network, you must set the provisioningNetwork configuration setting to unmanaged in the install-config.yaml file.

Network administrators must reserve IP addresses for each node in the "Red Hat OpenShift Container Platform" cluster for the baremetal network on an external DHCP server.
Reserving IP addresses for nodes with the DHCP server
For the baremetal network, a network administrator must reserve a number of IP addresses, including:

Two unique virtual IP addresses.

One IP address for the provisioner node.

One IP address for each control plane node.

One IP address for each worker node, if applicable.


Some administrators prefer to use static IP addresses so that each node's IP address remains constant in the absence of a DHCP server. To configure static IP addresses with NMState, see "(Optional) Configuring node network interfaces" in the "Setting up the environment for an OpenShift installation" section.
External load balancing services and the control plane nodes must run on the same L2 network, and on the same VLAN when using VLANs to route traffic between the load balancing services and the control plane nodes.
The storage interface requires a DHCP reservation or a static IP.
The following table provides an exemplary embodiment of fully qualified domain names. The API and Nameserver addresses begin with canonical name extensions. The hostnames of the control plane and worker nodes are exemplary, so you can use any host naming convention you prefer.


If you do not create DHCP reservations, the installer requires reverse DNS resolution to set the hostnames for the Kubernetes API node, the provisioner node, the control plane nodes, and the worker nodes.
Provisioner node requirements
You must specify the MAC address for the provisioner node in your installation configuration. The bootMacAddress specification is typically associated with PXE network booting. However, the Ironic provisioning service also requires the bootMacAddress specification to identify nodes during the inspection of the cluster, or during node redeployment in the cluster.

The provisioner node requires layer 2 connectivity for network booting, DHCP and DNS resolution, and local network communication. The provisioner node requires layer 3 connectivity for virtual media booting.
Network Time Protocol (NTP)
Each "Red Hat OpenShift Container Platform" node in the cluster must have access to an NTP server. "Red Hat OpenShift Container Platform" nodes use NTP to synchronize their clocks. For example, cluster nodes use SSL certificates that require validation, which might fail if the date and time between the nodes are not in sync.

Define a consistent clock date and time format in each cluster node's BIOS settings, or installation might fail.
You can reconfigure the control plane nodes to act as NTP servers on disconnected clusters, and reconfigure worker nodes to retrieve time from the control plane nodes.
Port access for the out-of-band management IP address
The out-of-band management IP address is on a separate network from the node. To ensure that the out-of-band management can communicate with the provisioner node during installation, the out-of-band management IP address must be granted access to port 6180 on the provisioner node and on the "Red Hat OpenShift Container Platform" control plane nodes. TLS port 6183 is required for virtual media installation, for example, by using Redfish.
Configuring nodes

Each node in the cluster requires the following configuration for proper installation.

A mismatch between nodes will cause an installation failure.
While the cluster nodes can contain more than two NICs, the installation process only focuses on the first two NICs. In the following table, NIC1 is a non-routable network (provisioning) that is only used for the installation of the "Red Hat OpenShift Container Platform" cluster.


The Red Hat Enterprise Linux (RHEL) 9.x installation process on the provisioner node might vary. To install Red Hat Enterprise Linux (RHEL) 9.x using a local Satellite server or a PXE server, PXE-enable NIC2.


Ensure PXE is disabled on all other NICs.
Configure the control plane and worker nodes as follows:



The installation process requires one NIC:


NICx is a routable network (baremetal) that is used for the installation of the "Red Hat OpenShift Container Platform" cluster, and routable to the internet.

The provisioning network is optional, but it is required for PXE booting. If you deploy without a provisioning network, you must use a virtual media BMC addressing option such as redfish-virtualmedia or idrac-virtualmedia.

Secure Boot prevents a node from booting unless it verifies the node is using only trusted software, such as UEFI firmware drivers, EFI applications, and the operating system.

Red Hat only supports manually configured Secure Boot when deploying with Redfish virtual media.
To enable Secure Boot manually, refer to the hardware guide for the node and execute the following:

Boot the node and enter the BIOS menu.

Set the node's boot mode to UEFI Enabled.

Enable Secure Boot.


Red Hat does not support Secure Boot with self-generated keys.
Out-of-band management
Nodes typically have an additional NIC used by the baseboard management controllers (BMCs). These BMCs must be accessible from the provisioner node.

Each node must be accessible via out-of-band management. When using an out-of-band management network, the provisioner node requires access to the out-of-band management network for a successful "Red Hat OpenShift Container Platform" installation.

The out-of-band management setup is out of scope for this document. Using a separate management network for out-of-band management can enhance performance and improve security. However, using the provisioning network or the bare metal network are valid options.

The bootstrap VM features a maximum of two network interfaces. If you configure a separate management network for out-of-band management, and you are using a provisioning network, the bootstrap VM requires routing access to the management network through one of the network interfaces. In this scenario, the bootstrap VM can then access three networks:

the bare metal network

the provisioning network

the management network routed through one of the network interfaces
Required data for installation
Prior to the installation of the "Red Hat OpenShift Container Platform" cluster, gather the following information from all cluster nodes:

Out-of-band management IP


NIC (provisioning) MAC address

NIC (baremetal) MAC address


NIC (baremetal) MAC address
Validation checklist for nodes
NIC1 VLAN is configured for the provisioning network.

NIC1 for the provisioning network is PXE-enabled on the provisioner, control plane, and worker nodes.

NIC2 VLAN is configured for the baremetal network.

PXE has been disabled on all other NICs.

DNS is configured with API and Ingress endpoints.

Control plane and worker nodes are configured.

All nodes accessible via out-of-band management.

(Optional) A separate management network has been created.

Required data for installation.


NIC1 VLAN is configured for the baremetal network.

DNS is configured with API and Ingress endpoints.

Control plane and worker nodes are configured.

All nodes accessible via out-of-band management.

(Optional) A separate management network has been created.

Required data for installation.