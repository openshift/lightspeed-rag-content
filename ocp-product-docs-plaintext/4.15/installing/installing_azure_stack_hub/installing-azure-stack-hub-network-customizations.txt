Installing a cluster on Azure Stack Hub with network customizations

In "Red Hat OpenShift Container Platform" version "4.15", you can install a cluster with a customized network configuration on infrastructure that the installation program provisions on Azure Stack Hub. By customizing your network configuration, your cluster can coexist with existing IP address allocations in your environment and integrate with existing MTU and VXLAN configurations.

While you can select azure when using the installation program to deploy a cluster using installer-provisioned infrastructure, this option is only supported for the Azure Public Cloud.
Prerequisites
You reviewed details about the "Red Hat OpenShift Container Platform" installation and update processes.

You read the documentation on selecting a cluster installation method and preparing it for users.

You configured an Azure Stack Hub account to host the cluster.

If you use a firewall, you configured it to allow the sites that your cluster requires access to.

You verified that you have approximately 16 GB of local disk space. Installing the cluster requires that you download the RHCOS virtual hard disk (VHD) cluster image and upload it to your Azure Stack Hub environment so that it is accessible during deployment. Decompressing the VHD files requires this amount of local disk space.
Internet access for "Red Hat OpenShift Container Platform"
In "Red Hat OpenShift Container Platform" "4.15", you require access to the internet to install your cluster.

You must have internet access to:

Access OpenShift Cluster Manager to download the installation program and perform subscription management. If the cluster has internet access and you do not disable Telemetry, that service automatically entitles your cluster.

Access Quay.io to obtain the packages that are required to install your cluster.

Obtain the packages that are required to perform cluster updates.
Generating a key pair for cluster node SSH access
During an "Red Hat OpenShift Container Platform" installation, you can provide an SSH public key to the installation program. The key is passed to the Red Hat Enterprise Linux CoreOS (RHCOS) nodes through their Ignition config files and is used to authenticate SSH access to the nodes. The key is added to the ~/.ssh/authorized_keys list for the core user on each node, which enables password-less authentication.

After the key is passed to the nodes, you can use the key pair to SSH in to the RHCOS nodes as the user core. To access the nodes through SSH, the private key identity must be managed by SSH for your local user.

If you want to SSH in to your cluster nodes to perform installation debugging or disaster recovery, you must provide the SSH public key during the installation process. The ./openshift-install gather command also requires the SSH public key to be in place on the cluster nodes.

Do not skip this procedure in production environments, where disaster recovery and debugging is required.
You must use a local key, not one that you configured with platform-specific approaches such as AWS key pairs.
If you do not have an existing SSH key pair on your local machine to use for authentication onto your cluster nodes, create one. For example, on a computer that uses a Linux operating system, run the following command:

View the public SSH key:

Add the SSH private key identity to the SSH agent for your local user, if it has not already been added. SSH agent management of the key is required for password-less SSH authentication onto your cluster nodes, or if you want to use the ./openshift-install gather command.

Add your SSH private key to the ssh-agent:


When you install "Red Hat OpenShift Container Platform", provide the SSH public key to the installation program.
Uploading the RHCOS cluster image
You must download the RHCOS virtual hard disk (VHD) cluster image and upload it to your Azure Stack Hub environment so that it is accessible during deployment.

Configure an Azure account.


Obtain the RHCOS VHD cluster image:

Decompress the VHD file.

Upload the local VHD to the Azure Stack Hub environment, making sure that the blob is publicly available. For example, you can upload the VHD to a blob using the az cli or the web portal.
Obtaining the installation program
Before you install "Red Hat OpenShift Container Platform", download the installation file on  the host you are using for installation.

You have a computer that runs Linux or macOS, with 500 MB of local disk space.


Access the Infrastructure Provider page on the OpenShift Cluster Manager site. If you have a Red Hat account, log in with your credentials. If you do not, create an account.

Select Azure as the cloud provider.

Navigate to the page for your installation type, download the installation program that corresponds with your host operating system and architecture, and place the file in the directory where you will store the installation configuration files.

Extract the installation program. For example, on a computer that uses a Linux
operating system, run the following command:

Download your installation pull secret from Red Hat OpenShift Cluster Manager. This pull secret allows you to authenticate with the services that are provided by the included authorities, including Quay.io, which serves the container images for "Red Hat OpenShift Container Platform" components.
Manually creating the installation configuration file
Installing the cluster requires that you manually create the installation configuration file.

You have an SSH public key on your local machine to provide to the installation program. The key will be used for SSH authentication onto your cluster nodes for debugging and disaster recovery.

You have obtained the "Red Hat OpenShift Container Platform" installation program and the pull secret for your
cluster.


Create an installation directory to store your required installation assets in:

Customize the sample install-config.yaml file template that is provided and save
it in the <installation_directory>.

Back up the install-config.yaml file so that you can use it to install multiple clusters.


Installation configuration parameters for Azure Stack Hub


Sample customized install-config.yaml file for Azure Stack Hub
You can customize the install-config.yaml file to specify more details about your "Red Hat OpenShift Container Platform" cluster's platform or modify the values of the required parameters.

This sample YAML file is provided for reference only. Use it as a resource to enter parameter values into the installation configuration file that you created manually.
apiVersion: v1
baseDomain: example.com 1
credentialsMode: Manual
controlPlane: 2 3
  name: master
  platform:
    azure:
      osDisk:
        diskSizeGB: 1024 4
        diskType: premium_LRS
  replicas: 3
compute: 2
- name: worker
  platform:
    azure:
      osDisk:
        diskSizeGB: 512 4
        diskType: premium_LRS
  replicas: 3
metadata:
  name: test-cluster 1 5
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16
  networkType: OVNKubernetes 6
  serviceNetwork:
  - 172.30.0.0/16
platform:
  azure:
    armEndpoint: azurestack_arm_endpoint 1 7
    baseDomainResourceGroupName: resource_group 1 8
    region: azure_stack_local_region 1 9
    resourceGroupName: existing_resource_group 10
    outboundType: Loadbalancer
    cloudName: AzureStackCloud 1
    clusterOSimage: https://vhdsa.blob.example.example.com/vhd/rhcos-410.84.202112040202-0-azurestack.x86_64.vhd 1 11
pullSecret: '{"auths": ...}' 1 12
fips: false 13
sshKey: ssh-ed25519 AAAA... 14
additionalTrustBundle: | 15
    -----BEGIN CERTIFICATE-----
    <MY_TRUSTED_CA_CERT>
    -----END CERTIFICATE-----
Required.

If you do not provide these parameters and values, the installation program provides the default value.

The controlPlane section is a single mapping, but the compute section is a sequence of mappings. To meet the requirements of the different data structures, the first line of the compute section must begin with a hyphen, -, and the first line of the controlPlane section must not. Although both sections currently define a single machine pool, it is possible that future versions of "Red Hat OpenShift Container Platform" will support defining multiple compute pools during installation. Only one control plane pool is used.

You can specify the size of the disk to use in GB. Minimum recommendation for control plane nodes is 1024 GB.

The name of the cluster.

The cluster network plugin to install. The default value OVNKubernetes is the only supported value.

The Azure Resource Manager endpoint that your Azure Stack Hub operator provides.

The name of the resource group that contains the DNS zone for your base domain.

The name of your Azure Stack Hub local region.

The name of an existing resource group to install your cluster to. If undefined, a new resource group is created for the cluster.

The URL of a storage blob in the Azure Stack environment that contains an RHCOS VHD.

The pull secret required to authenticate your cluster.

Whether to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that "Red Hat OpenShift Container Platform" runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead.

You can optionally provide the sshKey value that you use to access the machines in your cluster.

If the Azure Stack Hub environment is using an internal Certificate Authority (CA), adding the CA certificate is required.
Manually manage cloud credentials
The Cloud Credential Operator (CCO) only supports your cloud provider in manual mode. As a result, you must specify the identity and access management (IAM) secrets for your cloud provider.

If you have not previously created installation manifest files, do so by running the following command:

Set a $RELEASE_IMAGE variable with the release image from your installation file by running the following command:

Extract the list of CredentialsRequest custom resources (CRs) from the "Red Hat OpenShift Container Platform" release image by running the following command:

Create YAML files for secrets in the openshift-install manifests directory that you generated previously. The secrets must be stored using the namespace and secret name defined in the spec.secretRef for each CredentialsRequest object.


Before upgrading a cluster that uses manually maintained credentials, you must ensure that the CCO is in an upgradeable state.
Updating a cluster using the web console

Updating a cluster using the CLI
Configuring the cluster to use an internal CA
If the Azure Stack Hub environment is using an internal Certificate Authority (CA), update the cluster-proxy-01-config.yaml file to configure the cluster to use the internal CA.

Create the install-config.yaml file and specify the certificate trust bundle in .pem format.

Create the cluster manifests.


From the directory in which the installation program creates files, go to the manifests directory.

Add user-ca-bundle to  the spec.trustedCA.name field.

Optional: Back up the manifests/ cluster-proxy-01-config.yaml file. The installation program consumes the manifests/ directory when you deploy the cluster.
Network configuration phases
There are two phases prior to "Red Hat OpenShift Container Platform" installation where you can customize the network configuration.


Phase 1
You can customize the following network-related fields in the install-config.yaml file before you create the manifest files:
Phase 2
After creating the manifest files by running openshift-install create manifests, you can define a customized Cluster Network Operator manifest with only the fields you want to modify. You can use the manifest to specify advanced network configuration.


You cannot override the values specified in phase 1 in the install-config.yaml file during phase 2. However, you can further customize the network plugin during phase 2.
Specifying advanced network configuration
You can use advanced network configuration for your network plugin to integrate your cluster into your existing network environment. You can specify advanced network configuration only before you install the cluster.

Customizing your network configuration by modifying the "Red Hat OpenShift Container Platform" manifest files created by the installation program is not supported. Applying a manifest file that you create, as in the following procedure, is supported.
You have created the install-config.yaml file and completed any modifications to it.


Change to the directory that contains the installation program and create the manifests:

Create a stub manifest file for the advanced network configuration that is named cluster-network-03-config.yml in the <installation_directory>/manifests/ directory:

Specify the advanced network configuration for your cluster in the cluster-network-03-config.yml file, such as in the following example:

Optional: Back up the manifests/cluster-network-03-config.yml file. The
installation program consumes the manifests/ directory when you create the
Ignition config files.
Cluster Network Operator configuration
The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named cluster. The CR specifies the fields for the Network API in the operator.openshift.io API group.

The CNO configuration inherits the following fields during cluster installation from the Network API in the Network.config.openshift.io API group:


clusterNetwork
IP address pools from which pod IP addresses are allocated.
serviceNetwork
IP address pool for services.
defaultNetwork.type
Cluster network plugin. OVNKubernetes is the only supported plugin during installation.


You can specify the cluster network plugin configuration for your cluster by setting the fields for the defaultNetwork object in the CNO object named cluster.

Cluster Network Operator configuration object
The fields for the Cluster Network Operator (CNO) are described in the following table:



The values for the defaultNetwork object are defined in the following table:



The following table describes the configuration fields for the OVN-Kubernetes network plugin:





defaultNetwork:
  type: OVNKubernetes
  ovnKubernetesConfig:
    mtu: 1400
    genevePort: 6081
      ipsecConfig:
        mode: Full
Using OVNKubernetes can lead to a stack exhaustion problem on IBM Power&#174;.

The values for the kubeProxyConfig object are defined in the following table:
Configuring hybrid networking with OVN-Kubernetes
You can configure your cluster to use hybrid networking with the OVN-Kubernetes network plugin. This allows a hybrid cluster that supports different node networking configurations.

This configuration is necessary to run both Linux and Windows nodes in the same cluster.
You defined OVNKubernetes for the networking.networkType parameter in the install-config.yaml file. See the installation documentation for configuring "Red Hat OpenShift Container Platform" network customizations on your chosen cloud provider for more information.


Change to the directory that contains the installation program and create the manifests:

Create a stub manifest file for the advanced network configuration that is named cluster-network-03-config.yml in the <installation_directory>/manifests/ directory:

Open the cluster-network-03-config.yml file in an editor and configure OVN-Kubernetes with hybrid networking, such as in the following example:

Save the cluster-network-03-config.yml file and quit the text editor.

Optional: Back up the manifests/cluster-network-03-config.yml file. The
installation program deletes the manifests/ directory when creating the
cluster.


For more information on using Linux and Windows nodes in the same cluster, see Understanding Windows container workloads.
Deploying the cluster
You can install "Red Hat OpenShift Container Platform" on a compatible cloud platform.

You can run the create cluster command of the installation program only once, during initial installation.
You have configured an account with the cloud platform that hosts your cluster.

You have the "Red Hat OpenShift Container Platform" installation program and the pull secret for your cluster.

You have verified that the cloud provider account on your host has the correct permissions to deploy the cluster. An account with incorrect permissions causes the installation process to fail with an error message that displays the missing permissions.


Change to the directory that contains the installation program and initialize the cluster deployment:


When the cluster deployment completes successfully:

The terminal displays directions for accessing your cluster, including a link to the web console and credentials for the kubeadmin user.

Credential information also outputs to <installation_directory>/.openshift_install.log.


Do not delete the installation program or the files that the installation program creates. Both are required to delete the cluster.
...
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig'
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com
INFO Login to the console with user: "kubeadmin", and password: "password"
INFO Time elapsed: 36m22s
The Ignition config files that the installation program generates contain certificates that expire after 24 hours, which are then renewed at that time. If the cluster is shut down before renewing the certificates and the cluster is later restarted after the 24 hours have elapsed, the cluster automatically recovers the expired certificates. The exception is that you must manually approve the pending node-bootstrapper certificate signing requests (CSRs) to recover kubelet certificates. See the documentation for Recovering from expired control plane certificates for more information.

It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed. By using the Ignition config files within 12 hours, you can avoid installation failure if the certificate update runs during installation.
Installing the OpenShift CLI by downloading the binary
You can install the OpenShift CLI (oc) to interact with "Red Hat OpenShift Container Platform" from a command-line interface. You can install oc on Linux, Windows, or macOS.

If you installed an earlier version of oc, you cannot use it to complete all of the commands in "Red Hat OpenShift Container Platform" "4.15". Download and install the new version of oc.

You can install the OpenShift CLI (oc) binary on Linux by using the following procedure.

Navigate to the "Red Hat OpenShift Container Platform" downloads page on the Red Hat Customer Portal.

Select the architecture from the Product Variant drop-down list.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v"4.15" Linux Client entry and save the file.

Unpack the archive:

Place the oc binary in a directory that is on your PATH.


After you install the OpenShift CLI, it is available using the oc command:



You can install the OpenShift CLI (oc) binary on Windows by using the following procedure.

Navigate to the "Red Hat OpenShift Container Platform" downloads page on the Red Hat Customer Portal.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v"4.15" Windows Client entry and save the file.

Unzip the archive with a ZIP program.

Move the oc binary to a directory that is on your PATH.


After you install the OpenShift CLI, it is available using the oc command:



You can install the OpenShift CLI (oc) binary on macOS by using the following procedure.

Navigate to the "Red Hat OpenShift Container Platform" downloads page on the Red Hat Customer Portal.

Select the appropriate version from the Version drop-down list.

Click Download Now next to the OpenShift v"4.15" macOS Client entry and save the file.

Unpack and unzip the archive.

Move the oc binary to a directory on your PATH.


After you install the OpenShift CLI, it is available using the oc command:
Logging in to the cluster by using the CLI
You can log in to your cluster as a default system user by exporting the cluster kubeconfig file. The kubeconfig file contains information about the cluster that is used by the CLI to connect a client to the correct cluster and API server. The file is specific to a cluster and is created during "Red Hat OpenShift Container Platform" installation.

You deployed an "Red Hat OpenShift Container Platform" cluster.

You installed the oc CLI.


Export the kubeadmin credentials:

Verify you can run oc commands successfully using the exported configuration:
Logging in to the cluster by using the web console
The kubeadmin user exists by default after an "Red Hat OpenShift Container Platform" installation. You can log in to your cluster as the kubeadmin user by using the "Red Hat OpenShift Container Platform" web console.

You have access to the installation host.

You completed a cluster installation and all cluster Operators are available.


Obtain the password for the kubeadmin user from the kubeadmin-password file on the installation host:

List the "Red Hat OpenShift Container Platform" web console route:

Navigate to the route detailed in the output of the preceding command in a web browser and log in as the kubeadmin user.


Accessing the web console.
Telemetry access for "Red Hat OpenShift Container Platform"
In "Red Hat OpenShift Container Platform" "4.15", the Telemetry service, which runs by default to provide metrics about cluster health and the success of updates, requires internet access. If your cluster is connected to the internet, Telemetry runs automatically, and your cluster is registered to OpenShift Cluster Manager.

After you confirm that your OpenShift Cluster Manager inventory is correct, either maintained automatically by Telemetry or manually by using OpenShift Cluster Manager, use subscription watch to track your "Red Hat OpenShift Container Platform" subscriptions at the account or multi-cluster level.

About remote health monitoring
Next steps
Validating an installation.

Customize your cluster.

If necessary, you can opt out of remote health reporting.

If necessary, you can remove cloud provider credentials.