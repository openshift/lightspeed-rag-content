Working with nodes

As an administrator, you can perform several tasks to make your clusters more efficient.
Understanding how to evacuate pods on nodes
Evacuating pods allows you to migrate all or selected pods from a given node or nodes.

You can only evacuate pods backed by a replication controller. The replication controller creates new pods on other nodes and removes the existing pods from the specified node(s).

Bare pods, meaning those not backed by a replication controller, are unaffected by default. You can evacuate a subset of pods by specifying a pod-selector. Pod selectors are based on labels, so all the pods with the specified label will be evacuated.

Mark the nodes unschedulable before performing the pod evacuation.

Evacuate the pods using one of the following methods:

Mark the node as schedulable when done.
Understanding how to update labels on nodes
You can update any label on a node.

Node labels are not persisted after a node is deleted even if the node is backed up by a Machine.

Any change to a MachineSet object is not applied to existing machines owned by the compute machine set. For example, labels edited or added to an existing MachineSet object are not propagated to existing machines and nodes associated with the compute machine set.
The following command adds or updates labels on a node:

The following command updates all pods in the namespace:
Understanding how to mark nodes as unschedulable or schedulable
By default, healthy nodes with a Ready status are marked as schedulable, which means that you can place new pods on the node. Manually marking a node as unschedulable blocks any new pods from being scheduled on the node. Existing pods on the node are not affected.

The following command marks a node or nodes as unschedulable:

The following command marks a currently unschedulable node or nodes as schedulable:
Handling errors in single-node OpenShift clusters when the node reboots without draining application pods
In single-node OpenShift clusters and in "Red Hat OpenShift Container Platform" clusters in general, a situation can arise where a node reboot occurs without first draining the node. This can occur where an application pod requesting devices fails with the UnexpectedAdmissionError error. Deployment, ReplicaSet, or DaemonSet errors are reported because the application pods that require those devices start before the pod serving those devices. You cannot control the order of pod restarts.

While this behavior is to be expected, it can cause a pod to remain on the cluster even though it has failed to deploy successfully. The pod continues to report UnexpectedAdmissionError. This issue is mitigated by the fact that application pods are typically included in a Deployment, ReplicaSet, or DaemonSet. If a pod is in this error state, it is of little concern because another instance should be running. Belonging to a Deployment, ReplicaSet, or DaemonSet guarantees the successful creation and execution of subsequent pods and ensures the successful deployment of the application.

There is ongoing work upstream to ensure that such pods are gracefully terminated. Until that work is resolved, run the following command for a single-node OpenShift cluster to remove the failed pods:

$ oc delete pods --field-selector status.phase=Failed -n <POD_NAMESPACE>
The option to drain the node is unavailable for single-node OpenShift clusters.
Understanding how to evacuate pods on nodes
Deleting nodes
Deleting nodes from a cluster
To delete a node from the "Red Hat OpenShift Container Platform" cluster, scale down the appropriate MachineSet object.

When a cluster is integrated with a cloud provider, you must delete the corresponding machine to delete a node. Do not try to use the oc delete node command for this task.
When you delete a node by using the CLI, the node object is deleted in Kubernetes, but the pods that exist on the node are not deleted. Any bare pods that are not backed by a replication controller become inaccessible to "Red Hat OpenShift Container Platform". Pods backed by replication controllers are rescheduled to other available nodes. You must delete local manifest pods.

If you are running cluster on bare metal, you cannot delete a node by editing MachineSet objects. Compute machine sets are only available when a cluster is integrated with a cloud provider. Instead you must unschedule and drain the node before manually deleting it.
View the compute machine sets that are in the cluster by running the following command:

Scale down the compute machine set by using one of the following methods:


Manually scaling a compute machine set
Deleting nodes from a bare metal cluster
When you delete a node using the CLI, the node object is deleted in Kubernetes, but the pods that exist on the node are not deleted. Any bare pods not backed by a replication controller become inaccessible to "Red Hat OpenShift Container Platform". Pods backed by replication controllers are rescheduled to other available nodes. You must delete local manifest pods.

Delete a node from an "Red Hat OpenShift Container Platform" cluster running on bare metal by completing the following steps:

Mark the node as unschedulable:

Drain all pods on the node:

Delete the node from the cluster:

If you powered down the physical hardware, turn it back on so that the node can rejoin the cluster.