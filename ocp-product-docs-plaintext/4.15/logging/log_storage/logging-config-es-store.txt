Configuring the Elasticsearch log store

You can use Elasticsearch 6 to store and organize log data.

You can make modifications to your log store, including:

Storage for your Elasticsearch cluster

Shard replication across data nodes in the cluster, from full replication to no replication

External access to Elasticsearch data
Configuring log storage
You can configure which log storage type your logging uses by modifying the ClusterLogging custom resource (CR).

You have administrator permissions.

You have installed the OpenShift CLI (oc).

You have installed the Red Hat OpenShift Logging Operator and an internal log store that is either the LokiStack or Elasticsearch.

You have created a ClusterLogging CR.


The OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat provides bug fixes and support for this feature during the current release lifecycle, but this feature no longer receives enhancements. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.
Modify the ClusterLogging CR logStore spec:

Apply the ClusterLogging CR by running the following command:
Forwarding audit logs to the log store
In a logging deployment, container and infrastructure logs are forwarded to the internal log store defined in the ClusterLogging custom resource (CR) by default.

Audit logs are not forwarded to the internal log store by default because this does not provide secure storage. You are responsible for ensuring that the system to which you forward audit logs is compliant with your organizational and governmental regulations, and is properly secured.

If this default configuration meets your needs, you do not need to configure a ClusterLogForwarder CR. If a ClusterLogForwarder CR exists, logs are not forwarded to the internal log store unless a pipeline is defined that contains the default output.

To use the Log Forward API to forward audit logs to the internal Elasticsearch instance:

Create or edit a YAML file that defines the ClusterLogForwarder CR object:


About log collection and forwarding
Configuring log retention time
You can configure a retention policy that specifies how long the default Elasticsearch log store keeps indices for each of the three log sources: infrastructure logs, application logs, and audit logs.

To configure the retention policy, you set a maxAge parameter for each log source in the ClusterLogging custom resource (CR). The CR applies these values to the Elasticsearch rollover schedule, which determines when Elasticsearch deletes the rolled-over indices.

Elasticsearch rolls over an index, moving the current index and creating a new index, when an index matches any of the following conditions:

The index is older than the rollover.maxAge value in the Elasticsearch CR.

The index size is greater than 40 GB × the number of primary shards.

The index doc count is greater than 40960 KB × the number of primary shards.


Elasticsearch deletes the rolled-over indices based on the retention policy you configure. If you do not create a retention policy for any log sources, logs are deleted after seven days by default.

The Red Hat OpenShift Logging Operator and the OpenShift Elasticsearch Operator must be installed.


To configure the log retention time:

Edit the ClusterLogging CR to add or modify the retentionPolicy parameter:

You can verify the settings in the Elasticsearch custom resource (CR).
Configuring CPU and memory requests for the log store
Each component specification allows for adjustments to both the CPU and memory requests. You should not have to manually adjust these values as the OpenShift Elasticsearch Operator sets values sufficient for your environment.

In large-scale clusters, the default memory limit for the Elasticsearch proxy container might not be sufficient, causing the proxy container to be OOMKilled. If you experience this issue, increase the memory requests and limits for the Elasticsearch proxy.
Each Elasticsearch node can operate with a lower memory setting though this is not recommended for production deployments. For production use, you should have no less than the default 16Gi allocated to each pod. Preferably you should allocate as much as possible, up to 64Gi per pod.

The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.


Edit the ClusterLogging custom resource (CR) in the openshift-logging project:


When adjusting the amount of Elasticsearch memory, the same value should be used for both requests and limits.

For example:

      resources:
        limits: 1
          memory: "32Gi"
        requests: 2
          cpu: "8"
          memory: "32Gi"
The maximum amount of the resource.

The minimum amount required.


Kubernetes generally adheres the node configuration and does not allow Elasticsearch to use the specified limits. Setting the same value for the requests and limits ensures that Elasticsearch can use the memory you want, assuming the node has the memory available.
Configuring replication policy for the log store
You can define how Elasticsearch shards are replicated across data nodes in the cluster.

The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.


Edit the ClusterLogging custom resource (CR) in the openshift-logging project:


The number of primary shards for the index templates is equal to the number of Elasticsearch data nodes.
Scaling down Elasticsearch pods
Reducing the number of Elasticsearch pods in your cluster can result in data loss or Elasticsearch performance degradation.

If you scale down, you should scale down by one pod at a time and allow the cluster to re-balance the shards and replicas. After the Elasticsearch health status returns to green, you can scale down by another pod.

If your Elasticsearch cluster is set to ZeroRedundancy, you should not scale down your Elasticsearch pods.
Configuring persistent storage for the log store
Elasticsearch requires persistent storage. The faster the storage, the faster the Elasticsearch performance.

Using NFS storage as a volume or a persistent volume (or via NAS such as Gluster) is not supported for Elasticsearch storage, as Lucene relies on file system behavior that NFS does not supply. Data corruption and other problems can occur.
The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.


Edit the ClusterLogging CR to specify that each data node in the cluster is bound to a Persistent Volume Claim.


This example specifies each data node in the cluster is bound to a Persistent Volume Claim that requests "200G" of AWS General Purpose SSD (gp2) storage.

If you use a local volume for persistent storage, do not use a raw block volume, which is described with volumeMode: block in the LocalVolume object. Elasticsearch cannot use raw block volumes.
Configuring the log store for emptyDir storage
You can use emptyDir with your log store, which creates an ephemeral deployment in which all of a pod's data is lost upon restart.

When using emptyDir, if log storage is restarted or redeployed, you will lose data.
The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.


Edit the ClusterLogging CR to specify emptyDir:
Performing an Elasticsearch rolling cluster restart
Perform a rolling restart when you change the elasticsearch config map or any of the elasticsearch-* deployment configurations.

Also, a rolling restart is recommended if the nodes on which an Elasticsearch pod runs requires a reboot.

The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.


To perform a rolling cluster restart:

Change to the openshift-logging project:

Get the names of the Elasticsearch pods:

Scale down the collector pods so they stop sending new logs to Elasticsearch:

Perform a shard synced flush using the "Red Hat OpenShift Container Platform" es_util tool to ensure there are no pending operations waiting to be written to disk prior to shutting down:

Prevent shard balancing when purposely bringing down nodes using the "Red Hat OpenShift Container Platform" es_util tool:

After the command is complete, for each deployment you have for an ES cluster:

If you changed the Elasticsearch configuration map, repeat these steps for each Elasticsearch pod.

After all the deployments for the cluster have been rolled out, re-enable shard balancing:

Scale up the collector pods so they send new logs to Elasticsearch.
Exposing the log store service as a route
By default, the log store that is deployed with logging is not accessible from outside the logging cluster. You can enable a route with re-encryption termination for external access to the log store service for those tools that access its data.

Externally, you can access the log store by creating a reencrypt route, your "Red Hat OpenShift Container Platform" token and the installed log store CA certificate. Then, access a node that hosts the log store service with a cURL request that contains:

The Authorization: Bearer ${token}

The Elasticsearch reencrypt route and an Elasticsearch API request.


Internally, you can access the log store service using the log store cluster IP, which you can get by using either of the following commands:

$ oc get service elasticsearch -o jsonpath={.spec.clusterIP} -n openshift-logging
172.30.183.229
$ oc get service elasticsearch -n openshift-logging
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
elasticsearch   ClusterIP   172.30.183.229   <none>        9200/TCP   22h
You can check the cluster IP address with a command similar to the following:

$ oc exec elasticsearch-cdm-oplnhinv-1-5746475887-fj2f8 -n openshift-logging -- curl -tlsv1.2 --insecure -H "Authorization: Bearer ${token}" "https://172.30.183.229:9200/_cat/health"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    29  100    29    0     0    108      0 --:--:-- --:--:-- --:--:--   108
The Red Hat OpenShift Logging and Elasticsearch Operators must be installed.

You must have access to the project to be able to access to the logs.


To expose  the log store externally:

Change to the openshift-logging project:

Extract the CA certificate from the log store and write to the admin-ca file:

Create the route for the log store service as a YAML file:

Check that the Elasticsearch service is exposed:
Removing unused components if you do not use the default Elasticsearch log store
As an administrator, in the rare case that you forward logs to a third-party log store and do not use the default Elasticsearch log store, you can remove several unused components from your logging cluster.

In other words, if you do not use the default Elasticsearch log store, you can remove the internal Elasticsearch logStore and Kibana visualization components from the ClusterLogging custom resource (CR). Removing these components is optional but saves resources.

Verify that your log forwarder does not send log data to the default internal Elasticsearch cluster. Inspect the ClusterLogForwarder CR YAML file that you used to configure log forwarding. Verify that it does not have an outputRefs element that specifies default. For example:


Suppose the ClusterLogForwarder CR forwards log data to the internal Elasticsearch cluster, and you remove the logStore component from the ClusterLogging CR. In that case, the internal Elasticsearch cluster will not be present to store the log data. This absence can cause data loss.
Edit the ClusterLogging custom resource (CR) in the openshift-logging project:

If they are present, remove the logStore and visualization stanzas from the ClusterLogging CR.

Preserve the collection stanza of the ClusterLogging CR. The result should look similar to the following example:

Verify that the collector pods are redeployed: