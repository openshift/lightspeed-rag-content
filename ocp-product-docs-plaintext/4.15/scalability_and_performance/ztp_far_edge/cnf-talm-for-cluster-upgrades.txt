Updating managed clusters with the Topology Aware Lifecycle Manager

You can use the Topology Aware Lifecycle Manager (TALM) to manage the software lifecycle of multiple clusters. TALM uses Red Hat Advanced Cluster Management (RHACM) policies to perform changes on the target clusters.
About the Topology Aware Lifecycle Manager configuration
The Topology Aware Lifecycle Manager (TALM) manages the deployment of Red Hat Advanced Cluster Management (RHACM) policies for one or more "Red Hat OpenShift Container Platform" clusters. Using TALM in a large network of clusters allows the phased rollout of policies to the clusters in limited batches. This helps to minimize possible service disruptions when updating. With TALM, you can control the following actions:

The timing of the update

The number of RHACM-managed clusters

The subset of managed clusters to apply the policies to

The update order of the clusters

The set of policies remediated to the cluster

The order of policies remediated to the cluster

The assignment of a canary cluster


For single-node OpenShift, the Topology Aware Lifecycle Manager (TALM) offers the following features:

Create a backup of a deployment before an upgrade

Pre-caching images for clusters with limited bandwidth


TALM supports the orchestration of the "Red Hat OpenShift Container Platform" y-stream and z-stream updates, and day-two operations on y-streams and z-streams.
About managed policies used with Topology Aware Lifecycle Manager
The Topology Aware Lifecycle Manager (TALM) uses RHACM policies for cluster updates.

TALM can be used to manage the rollout of any policy CR where the remediationAction field is set to inform. Supported use cases include the following:

Manual user creation of policy CRs

Automatically generated policies from the PolicyGenTemplate custom resource definition (CRD)


For policies that update an Operator subscription with manual approval, TALM provides additional functionality that approves the installation of the updated Operator.

For more information about managed policies, see Policy Overview in the RHACM documentation.

For more information about the PolicyGenTemplate CRD, see the "About the PolicyGenTemplate CRD" section in "Configuring managed clusters with policies and PolicyGenTemplate resources".
Installing the Topology Aware Lifecycle Manager by using the web console
You can use the "Red Hat OpenShift Container Platform" web console to install the Topology Aware Lifecycle Manager.

Install the latest version of the RHACM Operator.

Set up a hub cluster with disconnected regitry.

Log in as a user with cluster-admin privileges.


In the "Red Hat OpenShift Container Platform" web console, navigate to Operators -> OperatorHub.

Search for the Topology Aware Lifecycle Manager from the list of available Operators, and then click Install.

Keep the default selection of Installation mode ["All namespaces on the cluster (default)"] and Installed Namespace ("openshift-operators") to ensure that the Operator is installed properly.

Click Install.


To confirm that the installation is successful:

Navigate to the Operators -> Installed Operators page.

Check that the Operator is installed in the All Namespaces namespace and its status is Succeeded.


If the Operator is not installed successfully:

Navigate to the Operators -> Installed Operators page and inspect the Status column for any errors or failures.

Navigate to the Workloads -> Pods page and check the logs in any containers in the cluster-group-upgrades-controller-manager pod that are reporting issues.
Installing the Topology Aware Lifecycle Manager by using the CLI
You can use the OpenShift CLI (oc) to install the Topology Aware Lifecycle Manager (TALM).

Install the OpenShift CLI (oc).

Install the latest version of the RHACM Operator.

Set up a hub cluster with disconnected registry.

Log in as a user with cluster-admin privileges.


Create a Subscription CR:


Verify that the installation succeeded by inspecting the CSV resource:

Verify that the TALM is up and running:
About the ClusterGroupUpgrade CR
The Topology Aware Lifecycle Manager (TALM) builds the remediation plan from the ClusterGroupUpgrade CR for a group of clusters. You can define the following specifications in a ClusterGroupUpgrade CR:

Clusters in the group

Blocking ClusterGroupUpgrade CRs

Applicable list of managed policies

Number of concurrent updates

Applicable canary updates

Actions to perform before and after the update

Update timing


You can control the start time of an update using the enable field in the ClusterGroupUpgrade CR. For example, if you have a scheduled maintenance window of four hours, you can prepare a ClusterGroupUpgrade CR with the enable field set to false.

You can set the timeout by configuring the spec.remediationStrategy.timeout setting as follows:

spec
  remediationStrategy:
          maxConcurrency: 1
          timeout: 240
You can use the batchTimeoutAction to determine what happens if an update fails for a cluster. You can specify continue to skip the failing cluster and continue to upgrade other clusters, or abort to stop policy remediation for all clusters. Once the timeout elapses, TALM removes all enforce policies to ensure that no further updates are made to clusters.

To apply the changes, you set the enabled field to true.

For more information see the "Applying update policies to managed clusters" section.

As TALM works through remediation of the policies to the specified clusters, the ClusterGroupUpgrade CR can report true or false statuses for a number of conditions.

After TALM completes a cluster update, the cluster does not update again under the control of the same ClusterGroupUpgrade CR. You must create a new ClusterGroupUpgrade CR in the following cases:

When you need to update the cluster again

When the cluster changes to non-compliant with the inform policy after being updated
Selecting clusters
TALM builds a remediation plan and selects clusters based on the following fields:

The clusterLabelSelector field specifies the labels of the clusters that you want to update. This consists of a list of the standard label selectors from k8s.io/apimachinery/pkg/apis/meta/v1. Each selector in the list uses either label value pairs or label expressions. Matches from each selector are added to the final list of clusters along with the matches from the clusterSelector field and the cluster field.

The clusters field specifies a list of clusters to update.

The canaries field specifies the clusters for canary updates.

The maxConcurrency field specifies the number of clusters to update in a batch.

The actions field specifies beforeEnable actions that TALM takes as it begins the update process, and afterCompletion actions that TALM takes as it completes policy remediation for each cluster.


You can use the clusters, clusterLabelSelector, and clusterSelector fields together to create a combined list of clusters.

The remediation plan starts with the clusters listed in the canaries field. Each canary cluster forms a single-cluster batch.

apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
Spec:
  actions:
    afterCompletion: 1
      addClusterLabels:
        upgrade-done: ""
      deleteClusterLabels:
        upgrade-running: ""
      deleteObjects: true
    beforeEnable: 2
      addClusterLabels:
        upgrade-running: ""
  backup: false
  clusters: 3
    - spoke1
  enable: false 4
  managedPolicies: 5
    - talm-policy
  preCaching: false
  remediationStrategy: 6
    canaries: 7
        - spoke1
    maxConcurrency: 2 8
    timeout: 240
  clusterLabelSelectors: 9
    - matchExpressions:
      - key: label1
      operator: In
      values:
        - value1a
        - value1b
  batchTimeoutAction: 10
status: 11
    computedMaxConcurrency: 2
    conditions:
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: 'True'
        type: ClustersSelected 12
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: Completed validation
        reason: ValidationCompleted
        status: 'True'
        type: Validated 13
      - lastTransitionTime: '2022-11-18T16:37:16Z'
        message: Not enabled
        reason: NotEnabled
        status: 'False'
        type: Progressing
    managedPoliciesForUpgrade:
      - name: talm-policy
        namespace: talm-namespace
    managedPoliciesNs:
      talm-policy: talm-namespace
    remediationPlan:
      - - spoke1
      - - spoke2
        - spoke3
    status:
Specifies the action that TALM takes when it completes policy remediation for each cluster.

Specifies the action that TALM takes as it begins the update process.

Defines the list of clusters to update.

The enable field is set to false.

Lists the user-defined set of policies to remediate.

Defines the specifics of the cluster updates.

Defines the clusters for canary updates.

Defines the maximum number of concurrent updates in a batch. The number of remediation batches is the number of canary clusters, plus the number of clusters, except the canary clusters, divided by the maxConcurrency value. The clusters that are already compliant with all the managed policies are excluded from the remediation plan.

Displays the parameters for selecting clusters.

Controls what happens if a batch times out. Possible values are abort or continue. If unspecified, the default is continue.

Displays information about the status of the updates.

The ClustersSelected condition shows that all selected clusters are valid.

The Validated condition shows that all selected clusters have been validated.


Any failures during the update of a canary cluster stops the update process.
When the remediation plan is successfully created, you can you set the enable field to true and TALM starts to update the non-compliant clusters with the specified managed policies.

You can only make changes to the spec fields if the enable field of the ClusterGroupUpgrade CR is set to false.
Validating
TALM checks that all specified managed policies are available and correct, and uses the Validated condition to report the status and reasons as follows:

true

false
Pre-caching
Clusters might have limited bandwidth to access the container image registry, which can cause a timeout before the updates are completed. On single-node OpenShift clusters, you can use pre-caching to avoid this. The container image pre-caching starts when you create a ClusterGroupUpgrade CR with the preCaching field set to true. TALM compares the available disk space with the estimated "Red Hat OpenShift Container Platform" image size to ensure that there is enough space. If a cluster has insufficient space, TALM cancels pre-caching for that cluster and does not remediate policies on it.

TALM uses the PrecacheSpecValid condition to report status information as follows:

true

false


TALM uses the PrecachingSucceeded condition to report status information as follows:

true

false


For more information see the "Using the container image pre-cache feature" section.
Creating a backup
For single-node OpenShift, TALM can create a backup of a deployment before an update. If the update fails, you can recover the previous version and restore a cluster to a working state without requiring a reprovision of applications. To use the backup feature you first create a ClusterGroupUpgrade CR with the backup field set to true. To ensure that the contents of the backup are up to date, the backup is not taken until you set the enable field in the ClusterGroupUpgrade CR to true.

TALM uses the BackupSucceeded condition to report the status and reasons as follows:

true

false


For more information, see the "Creating a backup of cluster resources before upgrade" section.
Updating clusters
TALM enforces the policies following the remediation plan. Enforcing the policies for subsequent batches starts immediately after all the clusters of the current batch are compliant with all the managed policies. If the batch times out, TALM moves on to the next batch. The timeout value of a batch is the spec.timeout field divided by the number of batches in the remediation plan.

TALM uses the Progressing condition to report the status and reasons as follows:

true

false


The managed policies apply in the order that they are listed in the managedPolicies field in the ClusterGroupUpgrade CR. One managed policy is applied to the specified clusters at a time. When a cluster complies with the current policy, the next managed policy is applied to it.
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
Spec:
  actions:
    afterCompletion:
      deleteObjects: true
    beforeEnable: {}
  backup: false
  clusters:
    - spoke1
  enable: true
  managedPolicies:
    - talm-policy
  preCaching: true
  remediationStrategy:
    canaries:
        - spoke1
    maxConcurrency: 2
    timeout: 240
  clusterLabelSelectors:
    - matchExpressions:
      - key: label1
      operator: In
      values:
        - value1a
        - value1b
  batchTimeoutAction:
status:
    clusters:
      - name: spoke1
        state: complete
    computedMaxConcurrency: 2
    conditions:
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: 'True'
        type: ClustersSelected
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: Completed validation
        reason: ValidationCompleted
        status: 'True'
        type: Validated
      - lastTransitionTime: '2022-11-18T16:37:16Z'
        message: Remediating non-compliant policies
        reason: InProgress
        status: 'True'
        type: Progressing 1
    managedPoliciesForUpgrade:
      - name: talm-policy
        namespace: talm-namespace
    managedPoliciesNs:
      talm-policy: talm-namespace
    remediationPlan:
      - - spoke1
      - - spoke2
        - spoke3
    status:
      currentBatch: 2
      currentBatchRemediationProgress:
        spoke2:
          state: Completed
        spoke3:
          policyIndex: 0
          state: InProgress
      currentBatchStartedAt: '2022-11-18T16:27:16Z'
      startedAt: '2022-11-18T16:27:15Z'
The Progressing fields show that TALM is in the process of remediating policies.
Update status
TALM uses the Succeeded condition to report the status and reasons as follows:

true

false


    apiVersion: ran.openshift.io/v1alpha1
    kind: ClusterGroupUpgrade
    metadata:
      name: cgu-upgrade-complete
      namespace: default
    spec:
      clusters:
      - spoke1
      - spoke4
      enable: true
      managedPolicies:
      - policy1-common-cluster-version-policy
      - policy2-common-pao-sub-policy
      remediationStrategy:
        maxConcurrency: 1
        timeout: 240
    status: 3
      clusters:
        - name: spoke1
          state: complete
        - name: spoke4
          state: complete
      conditions:
      - message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: "True"
        type: ClustersSelected
      - message: Completed validation
        reason: ValidationCompleted
        status: "True"
        type: Validated
      - message: All clusters are compliant with all the managed policies
        reason: Completed
        status: "False"
        type: Progressing 1
      - message: All clusters are compliant with all the managed policies
        reason: Completed
        status: "True"
        type: Succeeded 2
      managedPoliciesForUpgrade:
      - name: policy1-common-cluster-version-policy
        namespace: default
      - name: policy2-common-pao-sub-policy
        namespace: default
      remediationPlan:
      - - spoke1
      - - spoke4
      status:
        completedAt: '2022-11-18T16:27:16Z'
        startedAt: '2022-11-18T16:27:15Z'
In the Progressing fields, the status is false as the update has completed; clusters are compliant with all the managed policies.

The Succeeded fields show that the validations completed successfully.

The status field includes a list of clusters and their respective statuses. The status of a cluster can be complete or timedout.


apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
spec:
  actions:
    afterCompletion:
      deleteObjects: true
    beforeEnable: {}
  backup: false
  clusters:
    - spoke1
    - spoke2
  enable: true
  managedPolicies:
    - talm-policy
  preCaching: false
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
status:
  clusters:
    - name: spoke1
      state: complete
    - currentPolicy: 1
        name: talm-policy
        status: NonCompliant
      name: spoke2
      state: timedout
  computedMaxConcurrency: 2
  conditions:
    - lastTransitionTime: '2022-11-18T16:27:15Z'
      message: All selected clusters are valid
      reason: ClusterSelectionCompleted
      status: 'True'
      type: ClustersSelected
    - lastTransitionTime: '2022-11-18T16:27:15Z'
      message: Completed validation
      reason: ValidationCompleted
      status: 'True'
      type: Validated
    - lastTransitionTime: '2022-11-18T16:37:16Z'
      message: Policy remediation took too long
      reason: TimedOut
      status: 'False'
      type: Progressing
    - lastTransitionTime: '2022-11-18T16:37:16Z'
      message: Policy remediation took too long
      reason: TimedOut
      status: 'False'
      type: Succeeded 2
  managedPoliciesForUpgrade:
    - name: talm-policy
      namespace: talm-namespace
  managedPoliciesNs:
    talm-policy: talm-namespace
  remediationPlan:
    - - spoke1
      - spoke2
  status:
        startedAt: '2022-11-18T16:27:15Z'
        completedAt: '2022-11-18T20:27:15Z'
If a clusterâ€™s state is timedout, the currentPolicy field shows the name of the policy and the policy status.

The status for succeeded is false and the message indicates that policy remediation took too long.
Blocking ClusterGroupUpgrade CRs
You can create multiple ClusterGroupUpgrade CRs and control their order of application.

For example, if you create ClusterGroupUpgrade CR C that blocks the start of ClusterGroupUpgrade CR A, then ClusterGroupUpgrade CR A cannot start until the status of ClusterGroupUpgrade CR C becomes UpgradeComplete.

One ClusterGroupUpgrade CR can have multiple blocking CRs. In this case, all the blocking CRs must complete before the upgrade for the current CR can start.

Install the Topology Aware Lifecycle Manager (TALM).

Provision one or more managed clusters.

Log in as a user with cluster-admin privileges.

Create RHACM policies in the hub cluster.


Save the content of the ClusterGroupUpgrade CRs in the cgu-a.yaml, cgu-b.yaml, and cgu-c.yaml files.

Create the ClusterGroupUpgrade CRs by running the following command for each relevant CR:

Start the update process by running the following command for each relevant CR:
Update policies on managed clusters
The Topology Aware Lifecycle Manager (TALM) remediates a set of inform policies for the clusters specified in the ClusterGroupUpgrade CR. TALM remediates inform policies by making enforce copies of the managed RHACM policies. Each copied policy has its own corresponding RHACM placement rule and RHACM placement binding.

One by one, TALM adds each cluster from the current batch to the placement rule that corresponds with the applicable managed policy. If a cluster is already compliant with a policy, TALM skips applying that policy on the compliant cluster. TALM then moves on to applying the next policy to the non-compliant cluster. After TALM completes the updates in a batch, all clusters are removed from the placement rules associated with the copied policies. Then, the update of the next batch starts.

If a spoke cluster does not report any compliant state to RHACM, the managed policies on the hub cluster can be missing status information that TALM needs. TALM handles these cases in the following ways:

If a policy's status.compliant field is missing, TALM ignores the policy and adds a log entry. Then, TALM continues looking at the policy's status.status field.

If a policy's status.status is missing, TALM produces an error.

If a cluster's compliance status is missing in the policy's status.status field, TALM considers that cluster to be non-compliant with that policy.


The ClusterGroupUpgrade CR's batchTimeoutAction determines what happens if an upgrade fails for a cluster. You can specify continue to skip the failing cluster and continue to upgrade other clusters, or specify abort to stop the policy remediation for all clusters. Once the timeout elapses, TALM removes all enforce policies to ensure that no further updates are made to clusters.

apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: ocp-4."4.15".4
  namespace: platform-upgrade
spec:
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: upgrade
      spec:
        namespaceselector:
          exclude:
          - kube-*
          include:
          - '*'
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: config.openshift.io/v1
            kind: ClusterVersion
            metadata:
              name: version
            spec:
              channel: stable-"4.15"
              desiredUpdate:
                version: 4."4.15".4
              upstream: https://api.openshift.com/api/upgrades_info/v1/graph
            status:
              history:
                - state: Completed
                  version: 4."4.15".4
        remediationAction: inform
        severity: low
  remediationAction: inform
For more information about RHACM policies, see Policy overview.

For more information about the PolicyGenTemplate CRD, see About the PolicyGenTemplate CRD.

Configuring Operator subscriptions for managed clusters that you install with TALM
Topology Aware Lifecycle Manager (TALM) can only approve the install plan for an Operator if the Subscription custom resource (CR) of the Operator contains the status.state.AtLatestKnown field.

Add the status.state.AtLatestKnown field to the Subscription CR of the Operator:

Apply the changed Subscription policy to your managed clusters with a ClusterGroupUpgrade CR.
Applying update policies to managed clusters
You can update your managed clusters by applying your policies.

Install the Topology Aware Lifecycle Manager (TALM).

Provision one or more managed clusters.

Log in as a user with cluster-admin privileges.

Create RHACM policies in the hub cluster.


Save the contents of the ClusterGroupUpgrade CR in the cgu-1.yaml file.

Create the ClusterGroupUpgrade CR by running the following command:

Change the value of the spec.enable field to true by running the following command:


Check the status of the update again by running the following command:

If the policies include Operator subscriptions, you can check the installation progress directly on the single-node cluster.

If one of the managed policies includes a ClusterVersion CR, check the status of platform updates in the current batch by running the following command against the spoke cluster:

Check the Operator subscription by running the following command:

Check the install plans present on the single-node cluster that is associated with the desired subscription by running the following command:

Check if the cluster service version for the Operator of the policy that the ClusterGroupUpgrade is installing reached the Succeeded phase by running the following command:
Creating a backup of cluster resources before upgrade
For single-node OpenShift, the Topology Aware Lifecycle Manager (TALM) can create a backup of a deployment before an upgrade. If the upgrade fails, you can recover the previous version and restore a cluster to a working state without requiring a reprovision of applications.

To use the backup feature you first create a ClusterGroupUpgrade CR with the backup field set to true. To ensure that the contents of the backup are up to date, the backup is not taken until you set the enable field in the ClusterGroupUpgrade CR to true.

TALM uses the BackupSucceeded condition to report the status and reasons as follows:

true

false


If the backup of a cluster fails and enters the BackupTimeout or UnrecoverableError state, the cluster update does not proceed for that cluster. Updates to other clusters are not affected and continue.
Creating a ClusterGroupUpgrade CR with backup
You can create a backup of a deployment before an upgrade on single-node OpenShift clusters. If the upgrade fails you can use the upgrade-recovery.sh script generated by Topology Aware Lifecycle Manager (TALM) to return the system to its preupgrade state. The backup consists of the following items:


Cluster backup
A snapshot of etcd and static pod manifests.
Content backup
Backups of folders, for example, /etc, /usr/local, /var/lib/kubelet.
Changed files backup
Any files managed by machine-config that have been changed.
Deployment
A pinned ostree deployment.
Images (Optional)
Any container images that are in use.


Install the Topology Aware Lifecycle Manager (TALM).

Provision one or more managed clusters.

Log in as a user with cluster-admin privileges.

Install Red Hat Advanced Cluster Management (RHACM).


It is highly recommended that you create a recovery partition. The following is an example SiteConfig custom resource (CR) for a recovery partition of 50 GB:

nodes:
    - hostName: "node-1.example.com"
    role: "master"
    rootDeviceHints:
        hctl: "0:2:0:0"
        deviceName: /dev/disk/by-id/scsi-3600508b400105e210000900000490000
...
    #Disk /dev/disk/by-id/scsi-3600508b400105e210000900000490000:
    #893.3 GiB, 959119884288 bytes, 1873281024 sectors
    diskPartition:
        - device: /dev/disk/by-id/scsi-3600508b400105e210000900000490000
        partitions:
        - mount_point: /var/recovery
            size: 51200
            start: 800000
Save the contents of the ClusterGroupUpgrade CR with the backup and enable fields set to true in the clustergroupupgrades-group-du.yaml file:

To start the update, apply the ClusterGroupUpgrade CR by running the following command:


Check the status of the upgrade in the hub cluster by running the following command:
Recovering a cluster after a failed upgrade
If an upgrade of a cluster fails, you can manually log in to the cluster and use the backup to return the cluster to its preupgrade state. There are two stages:


Rollback
If the attempted upgrade included a change to the platform OS deployment, you must roll back to the previous version before running the recovery script.


A rollback is only applicable to upgrades from TALM and single-node OpenShift. This process does not apply to rollbacks from any other upgrade type.

Recovery
The recovery shuts down containers and uses files from the backup partition to relaunch containers and restore clusters.


Install the Topology Aware Lifecycle Manager (TALM).

Provision one or more managed clusters.

Install Red Hat Advanced Cluster Management (RHACM).

Log in as a user with cluster-admin privileges.

Run an upgrade that is configured for backup.


Delete the previously created ClusterGroupUpgrade custom resource (CR) by running the following command:

Log in to the cluster that you want to recover.

Check the status of the platform OS deployment by running the following command:

To trigger a rollback of the platform OS deployment, run the following command:

The first phase of the recovery shuts down containers and restores files from the backup partition to the targeted directories. To begin the recovery, run the following command:

When prompted, reboot the cluster by running the following command:

After the reboot, restart the recovery by running the following command:


If the recovery utility fails, you can retry with the --restart option:

$ /var/recovery/upgrade-recovery.sh --restart
To check the status of the recovery run the following command:
Using the container image pre-cache feature
Single-node OpenShift clusters might have limited bandwidth to access the container image registry, which can cause a timeout before the updates are completed.

The time of the update is not set by TALM. You can apply the ClusterGroupUpgrade CR at the beginning of the update by manual application or by external automation.
The container image pre-caching starts when the preCaching field is set to true in the ClusterGroupUpgrade CR.

TALM uses the PrecacheSpecValid condition to report status information as follows:

true

false


TALM uses the PrecachingSucceeded condition to report status information as follows:

true

false


After a successful pre-caching process, you can start remediating policies. The remediation actions start when the enable field is set to true. If there is a pre-caching failure on a cluster, the upgrade fails for that cluster. The upgrade process continues for all other clusters that have a successful pre-cache.

The pre-caching process can be in the following statuses:

NotStarted

PreparingToStart

Starting

Active

Succeeded

PrecacheTimeout

UnrecoverableError


Using the container image pre-cache filter
The pre-cache feature typically downloads more images than a cluster needs for an update. You can control which pre-cache images are downloaded to a cluster. This decreases download time, and saves bandwidth and storage.

You can see a list of all images to be downloaded using the following command:

$ oc adm release info <ocp-version>
The following ConfigMap example shows how you can exclude images using the excludePrecachePatterns field.

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-group-upgrade-overrides
data:
  excludePrecachePatterns: |
    azure 1
    aws
    vsphere
    alibaba
TALM excludes all images with names that include any of the patterns listed here.
Creating a ClusterGroupUpgrade CR with pre-caching
For single-node OpenShift, the pre-cache feature allows the required container images to be present on the spoke cluster before the update starts.

For pre-caching, TALM uses the spec.remediationStrategy.timeout value from the ClusterGroupUpgrade CR. You must set a timeout value that allows sufficient time for the pre-caching job to complete. When you enable the ClusterGroupUpgrade CR after pre-caching has completed, you can change the timeout value to a duration that is appropriate for the update.
Install the Topology Aware Lifecycle Manager (TALM).

Provision one or more managed clusters.

Log in as a user with cluster-admin privileges.


Save the contents of the ClusterGroupUpgrade CR with the preCaching field set to true in the clustergroupupgrades-group-du.yaml file:

When you want to start pre-caching, apply the ClusterGroupUpgrade CR by running the following command:


Check if the ClusterGroupUpgrade CR exists in the hub cluster by running the following command:

Check the status of the pre-caching task by running the following command:

Check the status of the pre-caching job by running the following command on the spoke cluster:

Check the status of the ClusterGroupUpgrade CR by running the following command:
Troubleshooting the Topology Aware Lifecycle Manager
The Topology Aware Lifecycle Manager (TALM) is an "Red Hat OpenShift Container Platform" Operator that remediates RHACM policies. When issues occur, use the oc adm must-gather command to gather details and logs and to take steps in debugging the issues.

For more information about related topics, see the following documentation:

Red Hat Advanced Cluster Management for Kubernetes 2.4 Support Matrix

Red Hat Advanced Cluster Management Troubleshooting

The "Troubleshooting Operator issues" section


General troubleshooting
You can determine the cause of the problem by reviewing the following questions:

Is the configuration that you are applying supported?

Which of the following components is causing the problem?


To ensure that the ClusterGroupUpgrade configuration is functional, you can do the following:

Create the ClusterGroupUpgrade CR with the spec.enable field set to false.

Wait for the status to be updated and go through the troubleshooting questions.

If everything looks as expected, set the spec.enable field to true in the ClusterGroupUpgrade CR.


After you set the spec.enable field to true in the ClusterUpgradeGroup CR, the update procedure starts and you cannot edit the CR's spec fields anymore.
Cannot modify the ClusterUpgradeGroup CR

Issue
You cannot edit the ClusterUpgradeGroup CR after enabling the update.
Resolution
Restart the procedure by performing the following steps:
Managed policies


Issue
You want to check if you have the correct managed policies on the system.
Resolution
Run the following command:




Issue
You want to check if the remediationAction field is set to inform in the spec of the managed policies.
Resolution
Run the following command:




Issue
You want to check the compliance state of policies.
Resolution
Run the following command:
Clusters


Issue
You want to check if the clusters in the ClusterGroupUpgrade CR are managed clusters.
Resolution
Run the following command:




Issue
You want to check if the managed clusters specified in the ClusterGroupUpgrade CR are available.
Resolution
Run the following command:




Issue
You want to check if the clusterLabelSelector field specified in the ClusterGroupUpgrade CR matches at least one of the managed clusters.
Resolution
Run the following command:




Issue
You want to check if the canary clusters are present in the list of clusters.
Resolution
Run the following commands:


A cluster can be present in spec.clusters and also be matched by the spec.clusterLabelSelector label.

Check the status of pre-caching by running the following command on the spoke cluster:
Remediation Strategy


Issue
You want to check if the remediationStrategy is present in the ClusterGroupUpgrade CR.
Resolution
Run the following command:




Issue
You want to check if the maxConcurrency is specified in the ClusterGroupUpgrade CR.
Resolution
Run the following command:
Topology Aware Lifecycle Manager


Issue
You want to check the value of the status.conditions field in the ClusterGroupUpgrade CR.
Resolution
Run the following command:




Issue
You want to check if every policy from status.managedPoliciesForUpgrade has a corresponding policy in status.copiedPolicies.
Resolution
Run the following command:




Issue
You want to check if status.remediationPlan is computed.
Resolution
Run the following command:




Issue
You want to check the logs of the manager container of TALM.
Resolution
Run the following command:




Issue
The policy compliance status that TALM uses to decide if remediation is needed has not yet fully updated for all clusters.
This may be because:
Resolution
Create and apply a new ClusterGroupUpdate CR with the same specification.




Issue
If there are no policies for the managed cluster when the cluster becomes Ready, a ClusterGroupUpgrade CR with no policies is auto-created.
Upon completion of the ClusterGroupUpgrade CR, the managed cluster is labeled as ztp-done.
If the PolicyGenTemplate CRs were not pushed to the Git repository within the required time after SiteConfig resources were pushed, this might result in no policies being available for the target cluster when the cluster became Ready.
Resolution
Verify that the policies you want to apply are available on the hub cluster, then create a ClusterGroupUpgrade CR with the required policies.


You can either manually create the ClusterGroupUpgrade CR or trigger auto-creation again. To trigger auto-creation of the ClusterGroupUpgrade CR, remove the ztp-done label from the cluster and delete the empty ClusterGroupUpgrade CR that was previously created in the zip-install namespace.



Issue
Pre-caching might fail for one of the following reasons:
Resolution



For information about troubleshooting, see OpenShift Container Platform Troubleshooting Operator Issues.

For more information about using Topology Aware Lifecycle Manager in the ZTP workflow, see Updating managed policies with Topology Aware Lifecycle Manager.

For more information about the PolicyGenTemplate CRD, see About the PolicyGenTemplate CRD