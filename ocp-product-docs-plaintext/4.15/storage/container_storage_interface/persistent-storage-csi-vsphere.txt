VMware vSphere CSI Driver Operator

Overview
"Red Hat OpenShift Container Platform" can provision persistent volumes (PVs) using the Container Storage Interface (CSI) VMware vSphere driver for Virtual Machine Disk (VMDK) volumes.

Familiarity with persistent storage and configuring CSI volumes is recommended when working with a CSI Operator and driver.

To create CSI-provisioned persistent volumes (PVs) that mount to vSphere storage assets, "Red Hat OpenShift Container Platform" installs the vSphere CSI Driver Operator and the vSphere CSI driver by default in the openshift-cluster-csi-drivers namespace.

vSphere CSI Driver Operator: The Operator provides a storage class, called thin-csi, that you can use to create persistent volumes claims (PVCs). The vSphere CSI Driver Operator supports dynamic volume provisioning by allowing storage volumes to be created on-demand, eliminating the need for cluster administrators to pre-provision storage. You can disable this default storage class if desired (see Managing the default storage class).

vSphere CSI driver: The driver enables you to create and mount vSphere PVs. In "Red Hat OpenShift Container Platform" 4.15, the driver version is 3.0.2. The vSphere CSI driver supports all of the file systems supported by the underlying Red Hat Core OS release, including XFS and Ext4. For more information about supported file systems, see Overview of available file systems.


For new installations, "Red Hat OpenShift Container Platform" 4.13 and later provides automatic migration for the vSphere in-tree volume plugin to its equivalent CSI driver. Updating to "Red Hat OpenShift Container Platform" 4.15 and later also provides automatic migration. For more information about updating and migration, see CSI automatic migration.

CSI automatic migration should be seamless. Migration does not change how you use all existing API objects, such as persistent volumes, persistent volume claims, and storage classes.
The vSphere CSI Driver supports dynamic and static provisioning. When using static provisioning in the PV specifications, do not use the key storage.kubernetes.io/csiProvisionerIdentity in csi.volumeAttributes because this key indicates dynamically provisioned PVs.
About CSI
Storage vendors have traditionally provided storage drivers as part of Kubernetes. With the implementation of the Container Storage Interface (CSI), third-party providers can instead deliver storage plugins using a standard interface without ever having to change the core Kubernetes code.

CSI Operators give "Red Hat OpenShift Container Platform" users storage options, such as volume snapshots, that are not possible with in-tree volume plugins.
vSphere storage policy
The vSphere CSI Driver Operator storage class uses vSphere's storage policy. "Red Hat OpenShift Container Platform" automatically creates a storage policy that targets datastore configured in cloud configuration:

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: thin-csi
provisioner: csi.vsphere.vmware.com
parameters:
  StoragePolicyName: "$openshift-storage-policy-xxxx"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false
reclaimPolicy: Delete
ReadWriteMany vSphere volume support
If the underlying vSphere environment supports the vSAN file service, then vSphere Container Storage Interface (CSI) Driver Operator installed by "Red Hat OpenShift Container Platform" supports provisioning of ReadWriteMany (RWX) volumes. If vSAN file service is not configured, then ReadWriteOnce (RWO) is the only access mode available. If you do not have vSAN file service configured, and you request RWX, the volume fails to get created and an error is logged.

For more information about configuring the vSAN file service in your environment, see vSAN File Service.

You can request RWX volumes by making the following persistent volume claim (PVC):

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
     - ReadWriteMany
  storageClassName: thin-csi
Requesting a PVC of the RWX volume type should result in provisioning of persistent volumes (PVs) backed by the vSAN file service.
VMware vSphere CSI Driver Operator requirements
To install the vSphere Container Storage Interface (CSI) Driver Operator, the following requirements must be met:

VMware vSphere version 7.0 Update 2 or later

vCenter 7.0 Update 2 or later

Virtual machines of hardware version 15 or later

No third-party vSphere CSI driver already installed in the cluster


If a third-party vSphere CSI driver is present in the cluster, "Red Hat OpenShift Container Platform" does not overwrite it. The presence of a third-party vSphere CSI driver prevents "Red Hat OpenShift Container Platform" from updating to "Red Hat OpenShift Container Platform" 4.13 or later.

The VMware vSphere CSI Driver Operator is supported only on clusters deployed with platform: vsphere in the installation manifest.
You can create a custom role for the Container Storage Interface (CSI) driver, the vSphere CSI Driver Operator, and the vSphere Problem Detector Operator. The custom role can include privilege sets that assign a minimum set of permissions to each vSphere object. This means that the CSI driver, the vSphere CSI Driver Operator, and the vSphere Problem Detector Operator can establish a basic interaction with these objects.

Installing an "Red Hat OpenShift Container Platform" cluster in a vCenter is tested against a full list of privileges as described in the "Required vCenter account privileges" section. By adhering to the full list of privileges, you can reduce the possibility of unexpected and unsupported behaviors that might occur when creating a custom role with a set of restricted privileges.
To remove a third-party CSI driver, see Removing a third-party vSphere CSI Driver.
Removing a third-party vSphere CSI Driver Operator
"Red Hat OpenShift Container Platform" 4.10, and later, includes a built-in version of the vSphere Container Storage Interface (CSI) Operator Driver that is supported by Red Hat. If you have installed a vSphere CSI driver provided by the community or another vendor, updates to the next major version of "Red Hat OpenShift Container Platform", such as 4.13, or later, might be disabled for your cluster.

"Red Hat OpenShift Container Platform" 4.12, and later, clusters are still fully supported, and updates to z-stream releases of 4.12, such as 4.12.z, are not blocked, but you must correct this state by removing the third-party vSphere CSI Driver before updates to next major version of "Red Hat OpenShift Container Platform" can occur. Removing the third-party vSphere CSI driver does not require deletion of associated persistent volume (PV) objects, and no data loss should occur.

These instructions may not be complete, so consult the vendor or community provider uninstall guide to ensure removal of the driver and components.
To uninstall the third-party vSphere CSI Driver:

Delete the third-party vSphere CSI Driver (VMware vSphere Container Storage Plugin) Deployment and Daemonset objects.

Delete the configmap and secret objects that were installed previously with the third-party vSphere CSI Driver.

Delete the third-party vSphere CSI driver CSIDriver object:


After you have removed the third-party vSphere CSI Driver from the "Red Hat OpenShift Container Platform" cluster, installation of Red Hat's vSphere CSI Driver Operator automatically resumes, and any conditions that could block upgrades to "Red Hat OpenShift Container Platform" 4.11, or later, are automatically removed. If you had existing vSphere CSI PV objects, their lifecycle is now managed by Red Hat's vSphere CSI Driver Operator.
vSphere persistent disks encryption
You can encrypt virtual machines (VMs) and dynamically provisioned persistent volumes (PVs) on "Red Hat OpenShift Container Platform" running on top of vSphere.

"Red Hat OpenShift Container Platform" does not support RWX-encrypted PVs. You cannot request RWX PVs out of a storage class that uses an encrypted storage policy.
You must encrypt VMs before you can encrypt PVs, which you can do during installation or postinstallation.

For information about encrypting VMs, see:

Requirements for encrypting virtual machines

During installation: Step 7 of Installing RHCOS and starting the "Red Hat OpenShift Container Platform" bootstrap process

Postinstallation enabling encryption on a vSphere cluster


After encrypting VMs, you can configure a storage class that supports dynamic encryption volume provisioning using the vSphere Container Storage Interface (CSI) driver. This can be accomplished in one of two ways using:

Datastore URL: This approach is not very flexible, and forces you to use a single datastore. It also does not support topology-aware provisioning.

Tag-based placement: Encrypts the provisioned volumes and uses tag-based placement to target specific datastores.


Using datastore URL
To encrypt using the datastore URL:

Find out the name of the default storage policy in your datastore that supports encryption.

Create a storage class that uses this storage policy:
Using tag-based placement
To encrypt using tag-based placement:

In vCenter create a category for tagging datastores that will be made available to this storage class. Also, ensure that StoragePod(Datastore clusters), Datastore, and Folder are selected as Associable Entities for the created category.

In vCenter, create a tag that uses the category created earlier.

Assign the previously created tag to each datastore that will be made available to the storage class. Make sure that datastores are shared with hosts participating in the "Red Hat OpenShift Container Platform" cluster.

In vCenter, from the main menu, click Policies and Profiles.

On the Policies and Profiles page, in the navigation pane, click VM Storage Policies.

Click CREATE.

Type a name for the storage policy.

Select Enable host based rules and Enable tag based placement rules.

In the Next tab:

Create the storage policy.

Create a storage class that uses the storage policy:
vSphere CSI topology overview
"Red Hat OpenShift Container Platform" provides the ability to deploy "Red Hat OpenShift Container Platform" for vSphere on different zones and regions, which allows you to deploy over multiple compute clusters and datacenters, thus helping to avoid a single point of failure.

This is accomplished by defining zone and region categories in vCenter, and then assigning these categories to different failure domains, such as a compute cluster, by creating tags for these zone and region categories. After you have created the appropriate categories, and assigned tags to vCenter objects, you can create additional machinesets that create virtual machines (VMs) that are responsible for scheduling pods in those failure domains.

The following example defines two failure domains with one region and two zones:


Creating vSphere storage topology during installation
Procedure
Specify the topology during installation. See the Configuring regions and zones for a VMware vCenter section.


No additional action is necessary and the default storage class that is created by "Red Hat OpenShift Container Platform" is topology aware and should allow provisioning of volumes in different failure domains.

Configuring regions and zones for a VMware vCenter
Creating vSphere storage topology postinstallation
Procedure
In the VMware vCenter vSphere client GUI, define appropriate zone and region catagories and tags.

In "Red Hat OpenShift Container Platform", create failure domains. See the Specifying multiple regions and zones for your cluster on vSphere section.

Create a tag to assign to datastores across failure domains:

As needed, create a storage policy that targets the tag-based datastores in each failure domain:

Create a new storage class that uses the new zoned storage policy:


Specifying multiple regions and zones for your cluster on vSphere

VMware vSphere tag documentation
Creating vSphere storage topology without an infra topology
"Red Hat OpenShift Container Platform" recommends using the infrastructure object for specifying failure domains in a topology aware setup. Specifying failure domains in the infrastructure object and specify topology-categories in the ClusterCSIDriver object at the same time is an unsupported operation.
Procedure
In the VMware vCenter vSphere client GUI, define appropriate zone and region catagories and tags.

To allow the container storage interface (CSI) driver to detect this topology, edit the clusterCSIDriver object YAML file driverConfig section:

Verify that CSINode object has topology keys by running the following commands:

Create a tag to assign to datastores across failure domains:

Create a storage policy that targets the tag-based datastores in each failure domain:

Create a new storage class that uses the new zoned storage policy:


VMware vSphere tag documentation
Results
Creating persistent volume claims (PVCs) and PVs from the topology aware storage class are truly zonal, and should use the datastore in their respective zone depending on how pods are scheduled:

~ $ oc get pv <pv-name> -o yaml
...
nodeAffinity:
  required:
    nodeSelectorTerms:
    - matchExpressions:
      - key: topology.csi.vmware.com/openshift-zone 1
        operator: In
        values:
        - <openshift-zone>
      -key: topology.csi.vmware.com/openshift-region 1
        operator: In
        values:
        - <openshift-region>
...
peristentVolumeclaimPolicy: Delete
storageClassName: <zoned-storage-class-name> 2
volumeMode: Filesystem
...
PV has zoned keys.

PV is using the zoned storage class.
Additional resources
Configuring CSI volumes