# Configuring a cross-cluster live migration network


Cross-cluster live migration requires that the clusters be connected in the same network. Specifically, virt-handler pods must be able to communicate.

[IMPORTANT]
----
Cross-cluster live migration is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----

# Configuration for a bridge secondary network

The following object describes the configuration parameters for the Bridge CNI plugin:




[NOTE]
----
The VLAN parameter configures the VLAN tag on the host end of the veth and also enables the vlan_filtering feature on the bridge interface.
----


[NOTE]
----
To configure an uplink for an L2 network, you must allow the VLAN on the uplink interface by using the following command:

```terminal
$  bridge vlan add vid VLAN_ID dev DEV
```

----

## Bridge CNI plugin configuration example

The following example configures a secondary network named bridge-net:


```json
{
  "cniVersion": "0.3.1",
  "name": "bridge-net",
  "type": "bridge",
  "isGateway": true,
  "vlan": 2,
  "ipam": {
    "type": "dhcp"
    }
}
```


# Configuring a dedicated secondary network for live migration

To configure a dedicated secondary network for live migration, you must first create a bridge network attachment definition (NAD) by using the CLI. Then, you add the name of the NetworkAttachmentDefinition object to the HyperConverged custom resource (CR).

* You installed the OpenShift CLI (oc).
* You logged in to the cluster as a user with the cluster-admin role.
* Each node has at least two Network Interface Cards (NICs).
* The NICs for live migration are connected to the same VLAN.

1. Create a NetworkAttachmentDefinition manifest according to the following example:
Example configuration file

```yaml
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: my-secondary-network 1
  namespace: openshift-cnv
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "migration-bridge",
    "type": "macvlan",
    "master": "eth1", 2
    "mode": "bridge",
    "ipam": {
      "type": "whereabouts", 3
      "range": "10.200.5.0/24" 4
    }
  }'
```

Specify the name of the NetworkAttachmentDefinition object.
Specify the name of the NIC to be used for live migration.
Specify the name of the CNI plugin that provides the network for the NAD.
Specify an IP address range for the secondary network. This range must not overlap the IP addresses of the main network.
2. Open the HyperConverged CR in your default editor by running the following command:

```terminal
$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
```

3. Add the name of the NetworkAttachmentDefinition object to the spec.liveMigrationConfig stanza of the HyperConverged CR:
Example HyperConverged manifest

```yaml
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  liveMigrationConfig:
    completionTimeoutPerGiB: 800
    network: <network> 1
    parallelMigrationsPerCluster: 5
    parallelOutboundMigrationsPerNode: 2
    progressTimeout: 150
# ...
```

Specify the name of the Multus NetworkAttachmentDefinition object to be used for live migrations.
4. Save your changes and exit the editor. The virt-handler pods restart and connect to the secondary network.

* When the node that the virtual machine runs on is placed into maintenance mode, the VM automatically migrates to another node in the cluster. You can verify that the migration occurred over the secondary network and not the default pod network by checking the target IP address in the virtual machine instance (VMI) metadata.

```terminal
$ oc get vmi <vmi_name> -o jsonpath='{.status.migrationState.targetNodeAddress}'
```
