# Creating a compute machine set on AWS


You can create a different compute machine set to serve a specific purpose in your Red Hat OpenShift Container Platform cluster on Amazon Web Services (AWS). For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.

[IMPORTANT]
----
You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
Clusters with the infrastructure platform type none cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
To view the platform type for your cluster, run the following command:

```terminal
$ oc get infrastructure cluster -o jsonpath='{.status.platform}'
```

----

# Sample YAML for a compute machine set custom resource on AWS

The sample YAML defines a compute machine set that runs in the us-east-1a Amazon Web Services (AWS) Local Zone and creates nodes that are labeled with
node-role.kubernetes.io/<role>: "".

In this sample, <infrastructure_id> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and
<role>
is the node label to add.


```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> 1
  name: <infrastructure_id>-<role>-<zone> 2
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>-<zone>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
        machine.openshift.io/cluster-api-machine-role: <role> 3
        machine.openshift.io/cluster-api-machine-type: <role>
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>-<zone>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/<role>: ""
      providerSpec:
        value:
          ami:
            id: ami-046fe691f52a953f9 4
          apiVersion: machine.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: <infrastructure_id>-worker-profile
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: <zone> 5
            region: <region> 6
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - <infrastructure_id>-node
            - filters:
                - name: tag:Name
                  values:
                    - <infrastructure_id>-lb
          subnet:
            filters:
              - name: tag:Name
                values:
                  - <infrastructure_id>-private-<zone> 7
          tags:
            - name: kubernetes.io/cluster/<infrastructure_id>
              value: owned
            - name: <custom_tag_name> 8
              value: <custom_tag_value>
          userDataSecret:
            name: worker-user-data
```


Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:

```terminal
$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
```

Specify the infrastructure ID, role node label, and zone.
Specify the role node label to add.
Specify a valid Red Hat Enterprise Linux CoreOS (RHCOS) Amazon
Machine Image (AMI) for your AWS zone for your Red Hat OpenShift Container Platform nodes. If you want to use an AWS Marketplace image, you must complete the Red Hat OpenShift Container Platform subscription from the AWS Marketplace to obtain an AMI ID for your region.

```terminal
$ oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.ami.id}{"\n"}' \
    get machineset/<infrastructure_id>-<role>-<zone>
```

Specify the zone name, for example, us-east-1a.
Specify the region, for example, us-east-1.
Specify the infrastructure ID and zone.
Optional: Specify custom tag data for your cluster. For example, you might add an admin contact email address by specifying a name:value pair of Email:admin-email@example.com.

[NOTE]
----
Custom tags can also be specified during installation in the install-config.yml file. If the install-config.yml file and the machine set include a tag with the same name data, the value for the tag from the machine set takes priority over the value for the tag in the install-config.yml file.
----

# Creating a compute machine set

In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.

* Deploy an Red Hat OpenShift Container Platform cluster.
* Install the OpenShift CLI (oc).
* Log in to oc as a user with cluster-admin permission.

1. Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <file_name>.yaml.

Ensure that you set the <clusterID> and <role> parameter values.
2. Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
1. To list the compute machine sets in your cluster, run the following command:

```terminal
$ oc get machinesets -n openshift-machine-api
```

Example output

```terminal
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
```

2. To view values of a specific compute machine set custom resource (CR), run the following command:

```terminal
$ oc get machineset <machineset_name> \
  -n openshift-machine-api -o yaml
```


```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> 1
  name: <infrastructure_id>-<role> 2
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
        machine.openshift.io/cluster-api-machine-role: <role>
        machine.openshift.io/cluster-api-machine-type: <role>
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
    spec:
      providerSpec: 3
        ...
```

The cluster infrastructure ID.
A default node label.

[NOTE]
----
For clusters that have user-provisioned infrastructure, a compute machine set can only create worker and infra type machines.
----
The values in the <providerSpec> section of the compute machine set CR are platform-specific. For more information about <providerSpec> parameters in the CR, see the sample compute machine set CR configuration for your provider.
3. Create a MachineSet CR by running the following command:

```terminal
$ oc create -f <file_name>.yaml
```

4. If you need compute machine sets in other availability zones, repeat this process to create more compute machine sets.

* View the list of compute machine sets by running the following command:

```terminal
$ oc get machineset -n openshift-machine-api
```

Example output

```terminal
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
```


When the new compute machine set is available, the DESIRED and CURRENT values match. If the compute machine set is not available, wait a few minutes and run the command again.

# Labeling GPU machine sets for the cluster autoscaler

You can use a machine set label to indicate which machines the cluster autoscaler can use to deploy GPU-enabled nodes.

* Your cluster uses a cluster autoscaler.

* On the machine set that you want to create machines for the cluster autoscaler to use to deploy GPU-enabled nodes, add a cluster-api/accelerator label:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: machine-set-name
spec:
  template:
    spec:
      metadata:
        labels:
          cluster-api/accelerator: <accelerator_name>
```

where:
<accelerator_name>:: Specifies a label of your choice that consists of alphanumeric characters, -, _, or . and starts and ends with an alphanumeric character.
For example, you might use nvidia-t4 to represent Nvidia T4 GPUs, or nvidia-a10g for A10G GPUs.

[NOTE]
----
You must specify the value of this label for the spec.resourceLimits.gpus.type parameter in your ClusterAutoscaler CR.
For more information, see "Cluster autoscaler resource definition".
----

* Cluster autoscaler resource definition

# Assigning machines to placement groups for Elastic Fabric Adapter instances by using machine sets

You can configure a machine set to deploy machines on Elastic Fabric Adapter (EFA) instances within an existing AWS placement group.

EFA instances do not require placement groups, and you can use placement groups for purposes other than configuring an EFA. This example uses both to demonstrate a configuration that can improve network performance for machines within the specified placement group.

* You created a placement group in the AWS console.

[NOTE]
----
Ensure that the rules and limitations for the type of placement group that you create are compatible with your intended use case.
----

1. In a text editor, open the YAML file for an existing machine set or create a new one.
2. Edit the following lines under the providerSpec field:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
# ...
spec:
  template:
    spec:
      providerSpec:
        value:
          instanceType: <supported_instance_type> 1
          networkInterfaceType: EFA 2
          placement:
            availabilityZone: <zone> 3
            region: <region> 4
          placementGroupName: <placement_group> 5
          placementGroupPartition: <placement_group_partition_number> 6
# ...
```

Specify an instance type that supports EFAs.
Specify the EFA network interface type.
Specify the zone, for example, us-east-1a.
Specify the region, for example, us-east-1.
Specify the name of the existing AWS placement group to deploy machines in.
Optional: Specify the partition number of the existing AWS placement group to deploy machines in.

* In the AWS console, find a machine that the machine set created and verify the following in the machine properties:
* The placement group field has the value that you specified for the placementGroupName parameter in the machine set.
* The partition number field has the value that you specified for the placementGroupPartition parameter in the machine set.
* The interface type field indicates that it uses an EFA.

# Machine set options for the Amazon EC2 Instance Metadata Service

You can use machine sets to create machines that use a specific version of the Amazon EC2 Instance Metadata Service (IMDS). Machine sets can create machines that allow the use of both IMDSv1 and IMDSv2 or machines that require the use of IMDSv2.


[NOTE]
----
To use IMDSv2 on AWS clusters that were created with Red Hat OpenShift Container Platform version 4.6 or earlier, you must update your boot image. For more information, see "Boot image management".
----

To deploy new compute machines with your preferred IMDS configuration, create a compute machine set YAML file with the appropriate values. You can also edit an existing machine set to create new machines with your preferred IMDS configuration when the machine set is scaled up.


[IMPORTANT]
----
Before configuring a machine set to create machines that require IMDSv2, ensure that any workloads that interact with the AWS metadata service support IMDSv2.
----

* Boot image management

## Configuring IMDS by using machine sets

You can specify whether to require the use of IMDSv2 by adding or editing the value of metadataServiceOptions.authentication in the machine set YAML file for your machines.

* To use IMDSv2, your AWS cluster must have been created with Red Hat OpenShift Container Platform version 4.7 or later.

* Add or edit the following lines under the providerSpec field:

```yaml
providerSpec:
  value:
    metadataServiceOptions:
      authentication: Required 1
```

To require IMDSv2, set the parameter value to Required. To allow the use of both IMDSv1 and IMDSv2, set the parameter value to Optional. If no value is specified, both IMDSv1 and IMDSv2 are allowed.

# Machine sets that deploy machines as Dedicated Instances

You can create a machine set running on AWS that deploys machines as Dedicated Instances. Dedicated Instances run in a virtual private cloud (VPC) on hardware that is dedicated to a single customer. These Amazon EC2 instances are physically isolated at the host hardware level. The isolation of Dedicated Instances occurs even if the instances belong to different AWS accounts that are linked to a single payer account. However, other instances that are not dedicated can share hardware with Dedicated Instances if they belong to the same AWS account.

Instances with either public or dedicated tenancy are supported by the Machine API. Instances with public tenancy run on shared hardware. Public tenancy is the default tenancy. Instances with dedicated tenancy run on single-tenant hardware.

## Creating Dedicated Instances by using machine sets

You can run a machine that is backed by a Dedicated Instance by using Machine API integration. Set the tenancy field in your machine set YAML file to launch a Dedicated Instance on AWS.

* Specify a dedicated tenancy under the providerSpec field:

```yaml
providerSpec:
  placement:
    tenancy: dedicated
```


# Machine sets that deploy machines as Spot Instances

You can save on costs by creating a compute machine set running on AWS that deploys machines as non-guaranteed Spot Instances. Spot Instances utilize unused AWS EC2 capacity and are less expensive than On-Demand Instances. You can use Spot Instances for workloads that can tolerate interruptions, such as batch or stateless, horizontally scalable workloads.

AWS EC2 can terminate a Spot Instance at any time. AWS gives a two-minute warning to the user when an interruption occurs. Red Hat OpenShift Container Platform begins to remove the workloads from the affected instances when AWS issues the termination warning.

Interruptions can occur when using Spot Instances for the following reasons:

* The instance price exceeds your maximum price
* The demand for Spot Instances increases
* The supply of Spot Instances decreases

When AWS terminates an instance, a termination handler running on the Spot Instance node deletes the machine resource. To satisfy the compute machine set replicas quantity, the compute machine set creates a machine that requests a Spot Instance.

## Creating Spot Instances by using compute machine sets

You can launch a Spot Instance on AWS by adding spotMarketOptions to your compute machine set YAML file.

* Add the following line under the providerSpec field:

```yaml
providerSpec:
  value:
    spotMarketOptions: {}
```


You can optionally set the spotMarketOptions.maxPrice field to limit the cost of the Spot Instance. For example you can set maxPrice: '2.50'.

If the maxPrice is set, this value is used as the hourly maximum spot price. If it is not set, the maximum price defaults to charge up to the On-Demand Instance price.

[NOTE]
----
It is strongly recommended to use the default On-Demand price as the maxPrice value and to not set the maximum price for Spot Instances.
----

# Configuring Capacity Reservations by using machine sets

Red Hat OpenShift Container Platform version 4.2 and later supports
Capacity Reservations on Amazon Web Services clusters, including On-Demand Capacity Reservations and Capacity Blocks for ML.

You can configure a machine set to deploy machines on any available resources that match the parameters of a capacity request that you define.
These parameters specify the
instance type,
region, and number of instances that you want to reserve.
If your
Capacity Reservation
can accommodate the capacity request, the deployment succeeds.

For more information, including limitations and suggested use cases for this
AWS offering, see On-Demand Capacity Reservations and Capacity Blocks for ML in the AWS documentation.

* You have access to the cluster with cluster-admin privileges.
* You installed the OpenShift CLI (`oc`).
* You purchased an On-Demand Capacity Reservation or Capacity Block for ML.
For more information, see On-Demand Capacity Reservations and Capacity Blocks for ML in the AWS documentation.

1. In a text editor, open the YAML file for an existing machine set or create a new one.
2. Edit the following section under the providerSpec field:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
# ...
spec:
  template:
    spec:
      providerSpec:
        value:
          capacityReservationId: <capacity_reservation> 1
          marketType: <market_type> 2
# ...
```

Specify the ID of the
Capacity Block for ML or On-Demand Capacity Reservation
that you want the machine set to deploy machines on.
Specify the market type to use.
The following values are valid:
CapacityBlock:: Use this market type with Capacity Blocks for ML.
OnDemand:: Use this market type with On-Demand Capacity Reservations.
Spot:: Use this market type with Spot Instances.
This option is not compatible with Capacity Reservations.

* To verify machine deployment, list the machines that the machine set created by running the following command:

```terminal
$ oc get machines.machine.openshift.io \
  -n openshift-machine-api \
  -l machine.openshift.io/cluster-api-machineset=<machine_set_name>
```


where <machine_set_name> is the name of the compute machine set.

In the output, verify that the characteristics of the listed machines match the parameters of your
Capacity Reservation.

# Adding a GPU node to an existing Red Hat OpenShift Container Platform cluster

You can copy and modify a default compute machine set configuration to create a GPU-enabled machine set and machines for the AWS EC2 cloud provider.

For more information about the supported instance types, see the following NVIDIA documentation:

* NVIDIA GPU Operator Community support matrix
* NVIDIA AI Enterprise support matrix

1. View the existing nodes, machines, and machine sets  by running the following command. Note that each node is an instance of a machine definition with a specific AWS region and Red Hat OpenShift Container Platform role.

```terminal
$ oc get nodes
```

Example output

```terminal
NAME                                        STATUS   ROLES                  AGE     VERSION
ip-10-0-52-50.us-east-2.compute.internal    Ready    worker                 3d17h   v1.33.4
ip-10-0-58-24.us-east-2.compute.internal    Ready    control-plane,master   3d17h   v1.33.4
ip-10-0-68-148.us-east-2.compute.internal   Ready    worker                 3d17h   v1.33.4
ip-10-0-68-68.us-east-2.compute.internal    Ready    control-plane,master   3d17h   v1.33.4
ip-10-0-72-170.us-east-2.compute.internal   Ready    control-plane,master   3d17h   v1.33.4
ip-10-0-74-50.us-east-2.compute.internal    Ready    worker                 3d17h   v1.33.4
```

2. View the machines and machine sets that exist in the openshift-machine-api namespace by running the following command. Each compute machine set is associated with a different availability zone within the AWS region. The installer automatically load balances compute machines across availability zones.

```terminal
$ oc get machinesets -n openshift-machine-api
```

Example output

```terminal
NAME                                        DESIRED   CURRENT   READY   AVAILABLE   AGE
preserve-dsoc12r4-ktjfc-worker-us-east-2a   1         1         1       1           3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b   2         2         2       2           3d11h
```

3. View the machines that exist in the openshift-machine-api namespace by running the following command. At this time, there is only one compute machine per machine set, though a compute machine set could be scaled to add a node in a particular region and zone.

```terminal
$ oc get machines -n openshift-machine-api | grep worker
```

Example output

```terminal
preserve-dsoc12r4-ktjfc-worker-us-east-2a-dts8r      Running   m5.xlarge   us-east-2   us-east-2a   3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b-dkv7w      Running   m5.xlarge   us-east-2   us-east-2b   3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b-k58cw      Running   m5.xlarge   us-east-2   us-east-2b   3d11h
```

4. Make a copy of one of the existing compute MachineSet definitions and output the result to a JSON file by running the following command. This will be the basis for the GPU-enabled compute machine set definition.

```terminal
$ oc get machineset preserve-dsoc12r4-ktjfc-worker-us-east-2a -n openshift-machine-api -o json > <output_file.json>
```

5. Edit the JSON file and make the following changes to the new MachineSet definition:
* Replace worker with gpu. This will be the name of the new machine set.
* Change the instance type of the new MachineSet definition to g4dn, which includes an NVIDIA Tesla T4 GPU.
To learn more about AWS g4dn instance types, see Accelerated Computing.

```terminal
$ jq .spec.template.spec.providerSpec.value.instanceType preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json

"g4dn.xlarge"
```


The <output_file.json> file is saved as preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json.
6. Update the following fields in preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json:
* .metadata.name to a name containing gpu.
* .spec.selector.matchLabels["machine.openshift.io/cluster-api-machineset"] to
match the new .metadata.name.
* .spec.template.metadata.labels["machine.openshift.io/cluster-api-machineset"]
to match the new .metadata.name.
* .spec.template.spec.providerSpec.value.instanceType to g4dn.xlarge.
7. To verify your changes, perform a diff of the original compute definition and the new GPU-enabled node definition by running the following command:

```terminal
$ oc -n openshift-machine-api get preserve-dsoc12r4-ktjfc-worker-us-east-2a -o json | diff preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json -
```

Example output

```terminal
10c10

< "name": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a",
---
> "name": "preserve-dsoc12r4-ktjfc-worker-us-east-2a",

21c21

< "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a"
---
> "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-us-east-2a"

31c31

< "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a"
---
> "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-us-east-2a"

60c60

< "instanceType": "g4dn.xlarge",
---
> "instanceType": "m5.xlarge",
```

8. Create the GPU-enabled compute machine set from the definition by running the following command:

```terminal
$ oc create -f preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json
```

Example output

```terminal
machineset.machine.openshift.io/preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a created
```


1. View the machine set you created by running the following command:

```terminal
$ oc -n openshift-machine-api get machinesets | grep gpu
```


The MachineSet replica count is set to 1 so a new Machine object is created automatically.
Example output

```terminal
preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a   1         1         1       1           4m21s
```

2. View the Machine object that the machine set created by running the following command:

```terminal
$ oc -n openshift-machine-api get machines | grep gpu
```

Example output

```terminal
preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a    running    g4dn.xlarge   us-east-2   us-east-2a  4m36s
```


Note that there is no need to specify a namespace for the node. The node definition is cluster scoped.

# Deploying the Node Feature Discovery Operator

After the GPU-enabled node is created, you need to discover the GPU-enabled node so it can be scheduled. To do this, install the Node Feature Discovery (NFD) Operator. The NFD Operator identifies hardware device features in nodes. It solves the general problem of identifying and cataloging hardware resources in the infrastructure nodes so they can be made available to Red Hat OpenShift Container Platform.

1. Install the Node Feature Discovery Operator from the software catalog in the Red Hat OpenShift Container Platform console.
2. After installing the NFD Operator, select Node Feature Discovery from the installed Operators list and select Create instance. This installs the nfd-master and nfd-worker pods, one nfd-worker pod for each compute node, in the openshift-nfd namespace.
3. Verify that the Operator is installed and running by running the following command:

```terminal
$ oc get pods -n openshift-nfd
```

Example output

```terminal
NAME                                       READY    STATUS     RESTARTS   AGE

nfd-controller-manager-8646fcbb65-x5qgk    2/2      Running 7  (8h ago)   1d
```

4. Browse to the installed Oerator in the console and select Create Node Feature Discovery.
5. Select Create to build a NFD custom resource. This creates NFD pods in the openshift-nfd namespace that poll the Red Hat OpenShift Container Platform nodes for hardware resources and catalogue them.

1. After a successful build, verify that a NFD pod is running on each nodes by running the following command:

```terminal
$ oc get pods -n openshift-nfd
```

Example output

```terminal
NAME                                       READY   STATUS      RESTARTS        AGE
nfd-controller-manager-8646fcbb65-x5qgk    2/2     Running     7 (8h ago)      12d
nfd-master-769656c4cb-w9vrv                1/1     Running     0               12d
nfd-worker-qjxb2                           1/1     Running     3 (3d14h ago)   12d
nfd-worker-xtz9b                           1/1     Running     5 (3d14h ago)   12d
```


The NFD Operator uses vendor PCI IDs to identify hardware in a node. NVIDIA uses the PCI ID 10de.
2. View the NVIDIA GPU discovered by the NFD Operator by running the following command:

```terminal
$ oc describe node ip-10-0-132-138.us-east-2.compute.internal | egrep 'Roles|pci'
```

Example output

```terminal
Roles: worker

feature.node.kubernetes.io/pci-1013.present=true

feature.node.kubernetes.io/pci-10de.present=true

feature.node.kubernetes.io/pci-1d0f.present=true
```


10de appears in the node feature list for the GPU-enabled node. This mean the NFD Operator correctly identified the node from the GPU-enabled MachineSet.