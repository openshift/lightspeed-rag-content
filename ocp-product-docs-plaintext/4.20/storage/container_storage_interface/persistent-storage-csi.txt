# Configuring CSI volumes


The Container Storage Interface (CSI) allows {product-title} to consume
storage from storage back ends that implement the
CSI interface
as persistent storage.

[NOTE]
----
{product-title} {product-version} supports version 1.6.0 of the CSI specification.
----

# CSI architecture

CSI drivers are typically shipped as container images. These containers
are not aware of {product-title} where they run. To use CSI-compatible
storage back end in {product-title}, the cluster administrator must deploy
several components that serve as a bridge between {product-title} and the
storage driver.

The following diagram provides a high-level overview about the components
running in pods in the {product-title} cluster.

![Architecture of CSI components]

It is possible to run multiple CSI drivers for different storage back ends.
Each driver needs its own external controllers deployment and daemon set
with the driver and CSI registrar.

## External CSI controllers

External CSI controllers is a deployment that deploys one or more pods
with five containers:

* The snapshotter container watches VolumeSnapshot and VolumeSnapshotContent objects and is responsible for the creation and deletion of VolumeSnapshotContent object.
* The resizer container is a sidecar container that watches for PersistentVolumeClaim updates and triggers ControllerExpandVolume operations against a CSI endpoint if you request more storage on PersistentVolumeClaim object.
* An external CSI attacher container translates attach and detach
calls from {product-title} to respective ControllerPublish and
ControllerUnpublish calls to the CSI driver.
* An external CSI provisioner container that translates provision and
delete calls from {product-title} to respective CreateVolume and
DeleteVolume calls to the CSI driver.
* A CSI driver container.

The CSI attacher and CSI provisioner containers communicate with the CSI
driver container using UNIX Domain Sockets, ensuring that no CSI
communication leaves the pod. The CSI driver is not accessible from
outside of the pod.


[NOTE]
----
The attach, detach, provision, and delete operations typically require
the CSI driver to use credentials to the storage backend. Run the CSI
controller pods on infrastructure nodes so the credentials are never leaked
to user processes, even in the event of a catastrophic security breach
on a compute node.
----


[NOTE]
----
The external attacher must also run for CSI drivers that do not support
third-party attach or detach operations. The external attacher will
not issue any ControllerPublish or ControllerUnpublish operations to
the CSI driver. However, it still must run to implement the necessary
{product-title} attachment API.
----

## CSI driver daemon set

The CSI driver daemon set runs a pod on every node that allows
{product-title} to mount storage provided by the CSI driver to the node
and use it in user workloads (pods) as persistent volumes (PVs). The pod
with the CSI driver installed contains the following containers:

* A CSI driver registrar, which registers the CSI driver into the
openshift-node service running on the node. The openshift-node process
running on the node then directly connects with the CSI driver using the
UNIX Domain Socket available on the node.
* A CSI driver.

The CSI driver deployed on the node should have as few credentials to the
storage back end as possible. {product-title} will only use the node plugin
set of CSI calls such as NodePublish/NodeUnpublish and
NodeStage/NodeUnstage, if these calls are implemented.

# CSI drivers supported by {product-title}

{product-title} installs certain CSI drivers by default, giving users storage options that are not possible with in-tree volume plugins.

To create CSI-provisioned persistent volumes that mount to these supported storage assets, {product-title} installs the necessary CSI driver Operator, the CSI driver, and the required storage class by default. For more details about the default namespace of the Operator and driver, see the documentation for the specific CSI Driver Operator.


[IMPORTANT]
----
The AWS EFS and GCP Filestore CSI drivers are not installed by default, and must be installed manually. For instructions on installing the AWS EFS CSI driver, see Setting up AWS Elastic File Service CSI Driver Operator. For instructions on installing the GCP Filestore CSI driver, see Google Compute Platform Filestore CSI Driver Operator.
----

The following table describes the CSI drivers that are
installed with {product-title},
supported by {product-title}, and which CSI features they support, such as volume snapshots and resize.


[IMPORTANT]
----
If your CSI driver is not listed in the following table, you must follow the installation instructions provided by your CSI storage vendor to use their supported CSI features.
----

For a list of third-party-certified CSI drivers, see the Red Hat ecosystem portal under Additional Resources.



1.

[IMPORTANT]
----
CSI volume group snapshots is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----
2.
* Cloning is not supported on hyperdisk-balanced disks with storage pools.
3.
* Does not support offline snapshots or resize. Volume must be attached to a running pod.
4.
* Azure File cloning does not supports NFS protocol. It supports the azurefile-csi storage class, which uses SMB protocol.
* Azure File cloning and snapshot are Technology Preview features:

[IMPORTANT]
----
Azure File CSI cloning and snapshot is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----
5.
* Requires vSphere version 7.0 Update 3 or later for both vCenter Server and ESXi.
* Does not support fileshare volumes.
6.
* Online expansion is supported from vSphere version 7.0 Update 2 and later.

* Red Hat ecosystem portal
* Third-party support policy

# Dynamic provisioning

Dynamic provisioning of persistent storage depends on the capabilities of
the CSI driver and underlying storage back end. The provider of the CSI
driver should document how to create a storage class in {product-title} and
the parameters available for configuration.

The created storage class can be configured to enable dynamic provisioning.

* Create a default storage class that ensures all PVCs that do not require
any special storage class are provisioned by the installed CSI driver.

```shell
# oc create -f - << EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: <storage-class> 1
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: <provisioner-name> 2
parameters:
  csi.storage.k8s.io/fstype: xfs  3
EOF
```

The name of the storage class that will be created.
The name of the CSI driver that has been installed.
The vSphere CSI driver supports all of the file systems supported by the underlying Red Hat Core operating system release, including XFS and Ext4.

# Example using the CSI driver

The following example installs a default MySQL template without any
changes to the template.

* The CSI driver has been deployed.
* A storage class has been created for dynamic provisioning.

* Create the MySQL template:

```terminal
# oc new-app mysql-persistent
```

Example output

```terminal
--> Deploying template "openshift/mysql-persistent" to project default
...
```


```terminal
# oc get pvc
```

Example output

```terminal
NAME              STATUS    VOLUME                                   CAPACITY
ACCESS MODES   STORAGECLASS   AGE
mysql             Bound     kubernetes-dynamic-pv-3271ffcb4e1811e8   1Gi
RWO            cinder         3s
```


# Volume populators

Volume populators use the datasource field in a persistent volume claim (PVC) spec to create pre-populated volumes.

Volume population is currently enabled, and supported as a Technology Preview feature. However, {product-title} does not ship with any volume populators.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----

For more information about volume populators, see Kubernetes volume populators.