# Storing and recording data for core platform monitoring


Store and record your metrics and alerting data, configure logs to specify which activities are recorded, control how long Prometheus retains stored data, and set the maximum amount of disk space for the data. These actions help you protect your data and use them for troubleshooting.

# Configuring persistent storage

Run cluster monitoring with persistent storage to gain the following benefits:

* Protect your metrics and alerting data from data loss by storing them in a persistent volume (PV). As a result, they can survive pods being restarted or recreated.
* Avoid getting duplicate notifications and losing silences for alerts when the Alertmanager pods are restarted.

For production environments, it is highly recommended to configure persistent storage.


[IMPORTANT]
----
In multi-node clusters, you must configure persistent storage for Prometheus, Alertmanager, and Thanos Ruler to ensure high availability.
----

## Persistent storage prerequisites

* Dedicate sufficient persistent storage to ensure that the disk does not become full.
* Use Filesystem as the storage type value for the volumeMode parameter when you configure the persistent volume.

[IMPORTANT]
----
* Do not use a raw block volume, which is described with volumeMode: Block in the PersistentVolume resource. Prometheus cannot use raw block volumes.
* Prometheus does not support file systems that are not POSIX compliant.
For example, some NFS file system implementations are not POSIX compliant.
If you want to use an NFS file system for storage, verify with the vendor that their NFS implementation is fully POSIX compliant.
----

## Configuring a persistent volume claim

To use a persistent volume (PV) for monitoring components, you must configure a persistent volume claim (PVC).

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the {configmap-name} config map in the {namespace-name} project:

```terminal
$ oc -n {namespace-name} edit configmap {configmap-name}
```

2. Add your PVC configuration for the component under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    <component>: 1
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class> 2
          resources:
            requests:
              storage: <amount_of_storage> 3
```

Specify the monitoring component for which you want to configure the PVC.
Specify an existing storage class. If a storage class is not specified, the default storage class is used.
Specify the amount of required storage.

The following example configures a PVC that claims persistent storage for
Prometheus:
Example PVC configuration

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {component}:
      volumeClaimTemplate:
        spec:
          storageClassName: my-storage-class
          resources:
            requests:
              storage: 40Gi
```

3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed and the new storage configuration is applied.

[WARNING]
----
When you update the config map with a PVC configuration, the affected StatefulSet object is recreated, resulting in a temporary service outage.
----

* Understanding persistent storage
* PersistentVolumeClaims (Kubernetes documentation)

## Resizing a persistent volume

You can resize a persistent volume (PV) for monitoring components, such as Prometheus or Alertmanager.
You need to manually expand a persistent volume claim (PVC), and then update the config map in which the component is configured.


[IMPORTANT]
----
You can only expand the size of the PVC. Shrinking the storage size is not possible.
----

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have configured at least one PVC for core {product-title} monitoring components.
* You have installed the OpenShift CLI (`oc`).

1. Manually expand a PVC with the updated storage request. For more information, see "Expanding persistent volume claims (PVCs) with a file system" in Expanding persistent volumes.
2. Edit the {configmap-name} config map in the {namespace-name} project:

```terminal
$ oc -n {namespace-name} edit configmap {configmap-name}
```

3. Add a new storage size for the PVC configuration for the component under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    <component>: 1
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: <amount_of_storage> 2
```

The component for which you want to change the storage size.
Specify the new size for the storage volume. It must be greater than the previous value.

The following example sets the new PVC request to
100 gigabytes for the Prometheus instance:
Example storage configuration for {component}

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {component}:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 100Gi
```

4. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.

[WARNING]
----
When you update the config map with a new storage size, the affected StatefulSet object is recreated, resulting in a temporary service outage.
----

* Prometheus database storage requirements
* Expanding persistent volume claims (PVCs) with a file system

# Modifying retention time and size for Prometheus metrics data

By default, Prometheus retains metrics data for
15 days for core platform monitoring.
You can modify the retention time for the Prometheus instance to change when the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses.


[NOTE]
----
Data compaction occurs every two hours. Therefore, a persistent volume (PV) might fill up before compaction, potentially exceeding the retentionSize limit. In such cases, the KubePersistentVolumeFillingUp alert fires until the space on a PV is lower than the retentionSize limit.
----

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the {configmap-name} config map in the {namespace-name} project:

```terminal
$ oc -n {namespace-name} edit configmap {configmap-name}
```

2. Add the retention time and size configuration under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {component}:
      retention: <time_specification> 1
      retentionSize: <size_specification> 2
```

The retention time: a number directly followed by ms (milliseconds), s (seconds), m (minutes), h (hours), d (days), w (weeks), or y (years). You can also combine time values for specific times, such as 1h30m15s.
The retention size: a number directly followed by B (bytes), KB (kilobytes), MB (megabytes), GB (gigabytes), TB (terabytes), PB (petabytes), and EB (exabytes).

The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance:
Example of setting retention time for Prometheus

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {component}:
      retention: 24h
      retentionSize: 10GB
```

3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.

* Retention time and size for Prometheus metrics
* Preparing to configure core platform monitoring stack
* Prometheus database storage requirements
* Recommended configurable storage technology
* Understanding persistent storage
* Optimizing storage

# Configuring audit logs for Metrics Server

You can configure audit logs for Metrics Server to help you troubleshoot issues with the server.
Audit logs record the sequence of actions in a cluster. It can record user, application, or control plane activities.

You can configure audit log rules to record specific events and a subset of associated data. The following audit profiles define configuration rules:

* Metadata (default): This profile logs event metadata including user, timestamps, resource, and verb. It does not record request and response bodies.
* Request: This profile logs event metadata and request body, but it does not record response body. This configuration does not apply to non-resource requests.
* RequestResponse: This profile logs event metadata, and request and response bodies. This configuration does not apply to non-resource requests.
* None: None of the previously described events are recorded.

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the cluster-monitoring-config config map in the openshift-monitoring project:

```terminal
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
```

2. Add audit log configuration for Metrics Server under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    metricsServer:
      audit:
        profile: <audit_log_profile> 1
```

Specify the audit profile for Metrics Server.
3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
4. Verify that the audit profile is applied:

```terminal
$ oc -n openshift-monitoring get deploy metrics-server -o yaml | grep -- '--audit-policy-file=*'
```

Example output

```terminal
        - --audit-policy-file=/etc/audit/request-profile.yaml
```


# Setting log levels for monitoring components

You can configure the log level for Alertmanager, Prometheus Operator, Prometheus, and {component-name} and log verbosity for Metrics Server.
You can use these settings for troubleshooting and to gain better insight into how the components are functioning.

The following log levels can be applied to the relevant component in the {configmap-name} ConfigMap object:

* debug. Log debug, informational, warning, and error messages.
* info (default). Log informational, warning, and error messages.
* warn. Log warning and error messages only.
* error. Log error messages only.

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the {configmap-name} config map in the {namespace-name} project:

```terminal
$ oc -n {namespace-name} edit configmap {configmap-name}
```

2. Add log configuration for a component under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    <component>: 1
      logLevel: <log_level> 2
    metricsServer:
      verbosity: <value> 3
    # ...
```

Specify the monitoring stack component for which you are setting a log level.
Available component values are {prometheus}, {alertmanager}, prometheusOperator, and {thanos}.
Specify the log level for the component.
The available values are error, warn, info, and debug.
The default value is info.
Specify the verbosity for Metrics Server.
Valid values are positive integers.
Increasing the number increases the amount of logged events, values over 10 are usually unnecessary.
The default value is 0.
3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
4. Verify that the log configuration is applied by reviewing the deployment or pod configuration in the related project.
* The following example checks the log level for the prometheus-operator deployment:

```terminal
$ oc -n {namespace-name} get deploy prometheus-operator -o yaml | grep "log-level"
```

Example output

```terminal
        - --log-level=debug
```

* The following example checks the log verbosity for the metrics-server deployment:

```terminal
$ oc -n openshift-monitoring get deploy metrics-server -o yaml | grep -- '--v='
```

Example output

```terminal
        - --v=3
```

5. Verify that the pods for the component are running:

```terminal
$ oc -n {namespace-name} get pods
```


[NOTE]
----
If an unrecognized logLevel value is included in the ConfigMap object, the pods for the component might not restart successfully.
----

# Enabling the query log file for Prometheus

You can configure Prometheus to write all queries that have been run by the engine to a log file.


[IMPORTANT]
----
Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the ConfigMap object to enable the feature.
----

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the {configmap-name} config map in the {namespace-name} project:

```terminal
$ oc -n {namespace-name} edit configmap {configmap-name}
```

2. Add the queryLogFile parameter for Prometheus under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {component}:
      queryLogFile: <path> 1
```

Add the full path to the file in which queries will be logged.
3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
4. Verify that the pods for the component are running. The following sample command lists the status of pods:

```terminal
$ oc -n {namespace-name} get pods
```

Example output

```terminal
...
prometheus-operator-567c9bc75c-96wkj   2/2     Running   0          62m
prometheus-k8s-0                       6/6     Running   1          57m
prometheus-k8s-1                       6/6     Running   1          57m
thanos-querier-56c76d7df4-2xkpc        6/6     Running   0          57m
thanos-querier-56c76d7df4-j5p29        6/6     Running   0          57m
...
```

5. Read the query log:

```terminal
$ oc -n {namespace-name} exec {pod} -- cat <path>
```


[IMPORTANT]
----
Revert the setting in the config map after you have examined the logged query information.
----

* Preparing to configure core platform monitoring stack

# Enabling query logging for Thanos Querier

For default platform monitoring in the openshift-monitoring project, you can enable the Cluster Monitoring Operator (CMO) to log all queries run by Thanos Querier.


[IMPORTANT]
----
Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the ConfigMap object to enable the feature.
----

* You have installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.

You can enable query logging for Thanos Querier in the openshift-monitoring project:

1. Edit the cluster-monitoring-config ConfigMap object in the openshift-monitoring project:

```terminal
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
```

2. Add a thanosQuerier section under data/config.yaml and add values as shown in the following example:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    thanosQuerier:
      enableRequestLogging: <value> 1
      logLevel: <value> 2
```

Set the value to true to enable logging and false to disable logging. The default value is false.
Set the value to debug, info, warn, or error. If no value exists for logLevel, the log level defaults to error.
3. Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.

1. Verify that the Thanos Querier pods are running. The following sample command lists the status of pods in the openshift-monitoring project:

```terminal
$ oc -n openshift-monitoring get pods
```

2. Run a test query using the following sample commands as a model:

```terminal
$ token=`oc create token prometheus-k8s -n openshift-monitoring`
$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -k -H "Authorization: Bearer $token" 'https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?query=cluster_version'
```

3. Run the following command to read the query log:

```terminal
$ oc -n openshift-monitoring logs <thanos_querier_pod_name> -c thanos-query
```


[NOTE]
----
Because the thanos-querier pods are highly available (HA) pods, you might be able to see logs in only one pod.
----
4. After you examine the logged query information, disable query logging by changing the enableRequestLogging value to false in the config map.