# {product-title} {product-version} release notes


Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.
Built on Red Hat Enterprise Linux (RHEL) and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

# About this release

{product-title} (RHSA-202X:XXXXX) is now available. This release uses Kubernetes 1.33 with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the Red Hat Hybrid Cloud Console, you can deploy {product-title} clusters to either on-premises or cloud environments.

You must use RHCOS machines for the control plane and for the compute machines.

Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including x86_64, 64-bit ARM (aarch64), IBM Power&#174; (ppc64le), and IBM Z&#174; (s390x) architectures. Beyond this, Red&#160;Hat also offers a 12-month additional EUS add-on, denoted as Additional EUS Term 2, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the Red Hat {product-title} Life Cycle Policy.

{product-title} is designed for FIPS. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, {product-title} core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.

For more information about the NIST validation program, see Cryptographic Module Validation Program. For the latest NIST status for the individual versions of RHEL cryptographic libraries that have been submitted for validation, see Compliance Activities and Government Standards.

# {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the Red Hat {product-title} Life Cycle Policy.

# New features and enhancements

This release adds improvements related to the following components and concepts:

## API server

### Extended loopback certificate validity to three years for kube-apiserver

Before this update, the self-signed loopback certificate for the Kubernetes API Server expired after one year. With this release, the expiration date of the certificate is extended to three years.

### Dry-run option is connected to 'oc delete istag'

Before this update, deleting an istag resource with the --dry-run=server option unintentionally caused actual deletion of the image from the server. This unexpected deletion occurred due to the dry-run option being implemented incorrectly in the oc delete istag command. With this release, the dry-run option is wired to the oc delete istag command. As a result, the accidental deletion of image objects is prevented and the istag object remains intact when using the --dry-run=server option.

## Authentication and authorization



## Documentation



## Edge computing

### NetworkPolicy support for the LVM Storage Operator

The LVM Storage Operator now applies Kubernetes NetworkPolicy objects during installation to restrict network communication to only the required components. This feature enforces default network isolation for LVM Storage deployments on {product-title} clusters.

### Support for hostname labelling for persistent volumes created by using the LVM Storage Operator

When you create a persistent volume (PV) by using the LVM Storage Operator, the PV now includes the kubernetes.io/hostname label. This label shows which node the PV is located on, making it easier to identify the node associated with a workload. This change only applies to newly created PVs. Existing PVs are not modified.

### Default namespace for the LVM Storage Operator

The default namespace for the LVM Storage Operator is now openshift-lvm-storage. You can still install LVM Storage in a custom namespace.

### SiteConfig CR to ClusterInstance CR migration tool

{product-title} {product-version} introduces the siteconfig-converter tool to help migrate managed clusters from using a SiteConfig custom resource (CR) to a ClusterInstance CR. Using a SiteConfig CR to define a managed cluster is deprecated and will be removed in a future release. The ClusterInstance CR provides a more unified and generic approach to defining clusters and is the preferred method for managing cluster deployments in the GitOps ZTP workflow.

Using the siteconfig-converter tool, you can convert SiteConfig CRs to ClusterInstance CRs and then incrementally migrate one or more clusters at a time. Existing and new pipelines run in parallel, so you can migrate clusters in a controlled, phased manner and without downtime.


[NOTE]
----
The siteconfig-converter tool does not convert SiteConfig CRs that use the deprecated spec.clusters.extraManifestPath field.
----

For more information, see Migrating from SiteConfig CRs to ClusterInstance CRs.

## Extensions (OLM v1)



## Hosted control planes

Because hosted control planes releases asynchronously from {product-title}, it has its own release notes. For more information, see Hosted control planes release notes.

## IBM Power

The IBM Power&#174; release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on IBM Power:

## IBM Z and IBM LinuxONE

The IBM Z&#174; and IBM&#174; LinuxONE release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on IBM Z&#174; and IBM&#174; LinuxONE:

## IBM Power, IBM Z, and IBM LinuxONE support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the IBM Power&#174; and the IBM Z&#174; platform. For more information, see the OpenShift EUS Overview.











1. Persistent shared storage must be provisioned by using either Red Hat OpenShift Data Foundation or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.

## Insights Operator



## Installation and update

### Installing a cluster on VMware vSphere with multiple network interface controllers (Generally Available)

{product-title} 4.18 enabled you to install a VMware vSphere cluster with multiple network interface controllers (NICs) for a node as a Technology Preview feature. This feature is now Generally Available.

For more information, see Configuring multiple NICs.

For an existing vSphere cluster, you can add multiple subnets by using compute machine sets.

### Installing a cluster on Google Cloud Platform into a shared VPC specifying a DNS private zone in a third project

With this release, you can specify the location of a DNS private zone when installing a cluster on GCP into a shared VPC. The private zone can be located in a service project that is distinct from the host project or main service project.

For more information, see Additional GCP configuration parameters.

### Installing a cluster on Microsoft Azure with virtual network encryption

With this release, you can install a cluster on Azure using encrypted virtual networks. You are required to use Azure virtual machines that have the premiumIO parameter set to true. See Microsoft&#8217;s documentation about Creating a virtual network with encryption and Requirements and Limitations for more information.

### Firewall requirements when installing a cluster that uses IBM Cloud Paks

With this release, if you install a cluster using IBM Cloud Paks, you must allow outbound access to icr.io and cp.icr.io on port 443. This access is required for IBM Cloud Pak container images. For more information, see Configuring your firewall.

### Installing a cluster on Microsoft Azure using Intel TDX Confidential VMs

With this release, you can install a cluster on Azure using Intel-based Confidential VMs. The following machine sizes are now supported:

* DCesv5-series
* DCedsv5-series
* ECesv5-series
* ECedsv5-series

For more information, see Enabling confidential VMs.

### Multi-architecture support for bare metal

With this release, you can install a bare-metal environment that supports multi-architecture capabilities. You can provision both x86_64 and aarch64 architectures from an existing x86_64 cluster by using virtual media, meaning you can manage a diverse hardware environment more efficiently.

For more information, see Configuring your cluster with multi-architecture compute machines.

## Machine Config Operator

### Updated boot images for vSphere now supported (Technology Preview)

Updated boot images is now supported as a Technology Preview feature for VMware vSphere clusters. This feature allows you configure your cluster to update the node boot image whenever you update your cluster. By default, the boot image in your cluster is not updated along with your cluster. For more information, see Updated boot images.

### On-cluster image mode reboot improvements

The following machine configuration changes no longer cause a reboot of nodes with on-cluster custom layered images:

* Modifying the configuration files in the /var or /etc directory
* Adding or modifying a systemd service
* Changing SSH keys
* Removing mirroring rules from ICSP, ITMS, and IDMS objects
* Changing the trusted CA, by updating the user-ca-bundle configmap in the openshift-config namespace

For more information, see On-cluster image mode known limitations.

### On-cluster image mode status reporting improvements

When image mode for OpenShift is configured, there are improvements to error reporting including the following changes:

* In certain scenarios after the custom layered image has been built and pushed, errors could cause the build process to fail. If this happens, the MCO now reports the errors and the machineosbuild object and builder pod are reported as failed.
* The oc describe mcp output has a new ImageBuildDegraded status field that reports if a custom layered image build has failed.

### Setting the kernel type parameter is now supported on on-cluster image mode nodes

You can now use the kernelType parameter in a MachineConfig object on nodes with on-cluster custom layered images in order to install a realtime kernel on the node. Previously, on nodes with on-cluster custom layered images the kernelType parameter was ignored. For information, see Adding a real-time kernel to nodes.

### Pinning images to nodes

In clusters with slow, unreliable connections to an image registry, you can use a PinnedImageSet object to pull the images in advance, before they are needed, then associate those images with a machine config pool. This ensures that the images are available to the nodes in that pool when needed. The must-gather for the Machine Config Operator includes all PinnedImageSet objects in the cluster. For more information, see Pinning images to nodes.

### Improved MCO state reporting is now generally available

The machine config nodes custom resource, which you can use to monitor the progress of machine configuration updates to nodes, is now generally available.

You can now view the status of updates to custom machine config pools in addition to the control plane and worker pools. The functionality for the feature has not changed. However, some of the information in the command output and in the status fields in the MachineConfigNode object has been updated. The must-gather for the Machine Config Operator now includes all MachineConfigNodes objects in the cluster. For more information, see About checking machine config node status.

## Machine management

### Additional AWS Capacity Reservation configuration options

On clusters that manage machines with the Cluster API, you can specify additional constraints to determine whether your compute machines use AWS capacity reservations. For more information, see Capacity Reservation configuration options.

### Cluster autoscaler scale up delay

You can now configure a delay before the cluster autoscaler recognizes newly pending pods and schedules the pods to a new node by using the spec.scaleUp.newPodScaleUpDelay parameter in the ClusterAutoscaler CR. If the node remains unscheduled after the delay, the cluster autoscaler can scale up a new node. This delay gives the cluster autoscaler additional time to locate an appropriate node or it can wait for space on an existing pod to become available. For more information, see Configuring the cluster autoscaler.

## Monitoring



## Networking

### Support for the BGP routing protocol

The Cluster Network Operator (CNO) now supports enabling Border Gateway Protocol (BGP) routing. With BGP, you can import and export routes to the underlying provider network and use multi-homing, link redundancy, and fast convergence. BGP configuration is managed with the FRRConfiguration custom resource (CR).

When upgrading from an earlier version of {product-title} in which you installed the MetalLB Operator, you must manually migrate your custom frr-k8s configurations from the metallb-system namespace to the openshift-frr-k8s namespace. To move these CRs, enter the following commands:

1. To create the openshift-frr-k8s namespace, enter the following command:

```terminal
$ oc create namespace openshift-frr-k8s
```

2. To automate the migration, create a migrate.sh file with the following content:

```bash
#!/bin/bash
OLD_NAMESPACE="metallb-system"
NEW_NAMESPACE="openshift-frr-k8s"
FILTER_OUT="metallb-"
oc get frrconfigurations.frrk8s.metallb.io -n "${OLD_NAMESPACE}" -o json |\
  jq -r '.items[] | select(.metadata.name | test("'"${FILTER_OUT}"'") | not)' |\
  jq -r '.metadata.namespace = "'"${NEW_NAMESPACE}"'"' |\
  oc create -f -
```

3. To run the migration script, enter the following command:

```terminal
$ bash migrate.sh
```

4. To verify that the migration succeeded, enter the following command:

```terminal
$ oc get frrconfigurations.frrk8s.metallb.io -n openshift-frr-k8s
```


After the migration is complete, you can remove the FRR-K8s custom resources from the metallb-system namespace.

For more information, see About BGP routing.

### Support for route advertisements for cluster user-defined networks (CUDNs) with Border Gateway Protocol (BGP)

With route advertisements enabled, the OVN-Kubernetes network plugin supports the direct advertisement of routes for pods and services associated with cluster user-defined networks (CUDNs) to the provider network. This feature enables some of the following benefits:

* Learns routes to pods dynamically
* Advertises routes dynamically
* Enables layer 3 notifications of EgressIP failovers in addition to the layer 2 ones based on gratuitous ARPs.
* Supports external route reflectors, which reduces the number of BGP connections required in large networks

For more information, see About route advertisements.

### Configuring enhanced PTP logging

You can now configure enhanced log reduction for the PTP Operator to reduce the volume of logs generated by the linuxptp-daemon.

This feature provides a periodic summary of filtered logs, which is not available with basic log reduction. Optionally, you can set a specific interval for the summary logs and a threshold in nanoseconds for the master offset logs.

For more information, see Configuring enhanced PTP logging.

### PTP ordinary clocks with added redundancy on AArch64 nodes (Technology Preview)

With this release, you can configure PTP ordinary clocks with added redundancy on AArch64 architecture nodes that use the following dual-port NICs only:

* NVIDIA ConnectX-7 series
* NVIDIA BlueField-3 series, in NIC mode

This feature is available as a Technology Preview. For more information, see Using dual-port NICs to improve redundancy for PTP ordinary clocks.

### Load balancing configuration with bond CNI plugin (Technology Preview)

In this release you can now specify the transmit hash policy for load balancing across the aggregated interfaces with the xmitHashPolicy as part of bond CNI plugin configuration. This feature is available as a Technology Preview.

For more information, see Configuration for a Bond CNI secondary network.

### SR-IOV network management in application namespaces

With {product-title} {product-version}, you can now create and manage SR-IOV networks directly within your application namespaces. This new feature provides greater control over your network configurations and helps simplify your workflow.

Previously, creating an SR-IOV network required a cluster administrator to configure it for you. Now, you can manage these resources directly in your own namespace, which offers several key benefits:

* Increased autonomy and control: You can now create your own SriovNetwork objects, removing the need to involve a cluster administrator for network configuration tasks.
* Enhanced security: Managing resources within your own namespace improves security by providing better separation between applications and helps prevent unintentional misconfigurations.
* Simplified permissions: You can now simplify permissions and reduce operational overhead by using namespaced SR-IOV networks.

For more information, see Configuring namespaced SR-IOV resources.

## Nodes

### sigstore support is now generally available

Support for sigstore ClusterImagePolicy and ImagePolicy objects is now generally available. The API version is now config.openshift.io/v1. For more information, see Manage secure signatures with sigstore.


[NOTE]
----
The default openshift cluster image policy is Technology Preview and is active only in clusters that have enabled Technology Preview features.
----

## Support for sigstore bring your own PKI (BYOPKI) image validation

You can now use sigstore ClusterImagePolicy and ImagePolicy objects to generate BYOPKI config to the policy.json file, enabling you to verify image signatures with BYOPKI. For more information, see About cluster and image policy parameters.

### Linux user namespace support is now generally available

Support for deploying pods and containers into Linux user namespaces is now generally available and enabled by default. Running pods and containers in individual user namespaces can mitigate several vulnerabilities that a compromised container can pose to other pods and the node itself. This change also includes two new security context constraints, restricted-v3 and nested-container, that are specifically designed for use with user namespaces. You can also configure the /proc file system in pods as unmasked. For more information, see Running pods in Linux user namespaces.

### Adjust pod resource levels without pod disruption

By using the in-place pod resizing feature, you can apply a resize policy to change the CPU and memory resources for containers within a running pod without re-creating or restarting the pod. For more information, see Manually adjust pod resource levels.

### Mounting an OCI image into a pod

You can you use an image volume to mount an Open Container Initiative (OCI)-compliant container image or artifact directly into a pod.

### Allocating specific GPUs to pods (Technology Preview)

You can now enable pods to request GPUs based on specific device attributes, such as product name, GPU memory capacity, compute capability, vendor name, and driver version. These attributes are exposed by the by using a third-party DRA resource driver that you install.

## OpenShift CLI (oc)

### Introducing the oc adm upgrade recommend command (General Availability)

Formerly Technology Preview and now Generally Available, the oc adm upgrade recommend command allows system administrators to perform a pre-update check on their {product-title} clusters using the command line interface (CLI). The pre-update check helps identify potential issues, enabling users to address them before initiating an update. By running the precheck command and inspecting the output, users can prepare for updating their cluster and make informed decisions about when to start an update.

For more information, see Updating a cluster by using the CLI.

## Operator development

### Supported Operator base images

With this release, the following base images for Operator projects are updated for compatibility with {product-title} {product-version}. The runtime functionality and configuration APIs for these base images are supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For more information, see Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later (Red&#160;Hat Knowledgebase).

## Postinstallation configuration



## Red Hat Enterprise Linux CoreOS (RHCOS)

### Investigate kernel crashes with kdump (General Availability)

With this update, kdump is now Generally Available for all supported architectures, including x86_64, arm64, s390x, and ppc64le. This enhancement enables users to diagnose and resolve kernel problems more efficiently.

## Scalability and performance

### Configuring NUMA-aware scheduler replicas and high availability (Technology Preview)

In {product-title} {product-version}, the NUMA Resources Operator automatically enables high availability (HA) mode by default. In this mode, the NUMA Resources Operator creates one scheduler replica for each control-plane node in the cluster to ensure redundancy. This default behavior occurs if the spec.replicas field is not specified in the NUMAResourcesScheduler Custom Resource (CR). Alternatively, you can explicitly set a specific number of scheduler replicas to override the default HA behavior or disable the scheduler entirely by setting the spec.replicas field to 0.

For more information, see Managing high availability (HA) for the NUMA-aware scheduler.

### Receive Packet Steering (RPS) is now disabled by default

With this release, Receive Packet Steering (RPS) is no longer configured when Performance Profile is applied. The RPS configuration affects containers that perform networking system calls, such as send, directly within latency-sensitive threads. To avoid latency impacts when RPS is not configured, move networking calls to helper threads or processes.

The previous RPS configuration resolved latency issues at the expense of overall pod kernel networking performance. The current default configuration promotes transparency by requiring developers to address the underlying application design instead of obscuring performance impacts.

To revert to the previous behavior, add the performance.openshift.io/enable-rps annotation to the PerformanceProfile manifest:


```yaml
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: example-performanceprofile
  annotations:
    performance.openshift.io/enable-rps: "enable"
```



[NOTE]
----
This action restores the prior functionality at the cost of globally reducing networking performance for all pods.
----

## Security



## Storage



## Web console



# Notable technical changes

## MachineOSConfig naming changes

The name of the MachineOSConfig object used with on-cluster image mode must now be the same as the machine config pool where you want to deploy the custom layered image. Previously, you could use any name. This change was made to prevent attempts to use multiple MachineOSConfig objects with each machine config pool.

## oc-mirror plugin v2 verifies credentials and certificates before mirroring operations

With this update, the oc-mirror plugin v2 now verifies information such as registry credentials, DNS name, and SSL certificates before populating the cache and beginning mirroring operations.
This prevents users from discovering certain problems only after the cache is populated and mirroring has begun.

# Deprecated and removed features

## Images deprecated and removed features



## Installation deprecated and removed features



## Networking deprecated and removed features



## Node deprecated and removed features



## OpenShift CLI (oc) deprecated and removed features



## Operator lifecycle and development deprecated and removed features



## Specialized hardware and driver enablement deprecated and removed features



## Storage deprecated and removed features



## Updating clusters deprecated and removed features



## Web console deprecated and removed features



## Workloads deprecated and removed features



## Deprecated features



## Removed features



# Bug fixes

## API Server and Authentication



## Bare Metal Hardware Provisioning



## Cloud Compute



## Cluster Autoscaler



## Cluster Resource Override Admission Operator



## Cluster Version Operator



## ImageStreams



## Installer



## Machine Config Operator



## Management Console



## Monitoring



## Networking



## Node



## Node Tuning Operator (NTO)



## Observability



## oc-mirror



## OpenShift CLI (oc)



## Operator Lifecycle Manager (OLM)



## Operator Controller Manager



## Performance Addon Operator



## Samples Operator



## Storage



## Red Hat Enterprise Linux CoreOS (RHCOS)



# Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red&#160;Hat Customer Portal for these features:

Technology Preview Features Support Scope

In the following tables, features are marked with the following statuses:

* Not Available
* Technology Preview
* General Availability
* Deprecated
* Removed

## Authentication and authorization Technology Preview features



## Edge computing Technology Preview features



## Extensions Technology Preview features



## Installation Technology Preview features



## Machine Config Operator Technology Preview features



## Machine management Technology Preview features



## Monitoring Technology Preview features



## Multi-Architecture Technology Preview features



## Networking Technology Preview features



## Node Technology Preview features



## OpenShift CLI (oc) Technology Preview features



## Operator lifecycle and development Technology Preview features



## Red Hat OpenStack Platform (RHOSP) Technology Preview features



## Scalability and performance Technology Preview features



## Specialized hardware and driver enablement Technology Preview features



## Storage Technology Preview features



## Web console Technology Preview features



# Known issues



# Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red&#160;Hat Network. All {product-title} {product-version} errata is available on the Red Hat Customer Portal. See the {product-title} Life Cycle for more information about asynchronous errata.

Red&#160;Hat Customer Portal users can enable errata notifications in the account settings for Red&#160;Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.


[NOTE]
----
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
----

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.


[IMPORTANT]
----
For any {product-title} release, always review the instructions on updating your cluster properly.
----

## RHSA-202X:XXXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: DD MMM YYYY

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the RHSA-202X:XXXXX advisory. The RPM packages that are included in the update are provided by the RHEA-202X:XXXX advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:


```terminal
$ oc adm release info 4.20.0 --pullspecs
```


### Updating

To update an {product-title} 4.20 cluster to this latest release, see Updating a cluster using the CLI.