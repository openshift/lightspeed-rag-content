# {product-title} {product-version} release notes


Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.
Built on Red Hat Enterprise Linux (RHEL) and Kubernetes, {product-title} provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

# About this release

{product-title} (RHSA-202X:XXXXX) is now available. This release uses Kubernetes 1.33 with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

{product-title} {product-version} clusters are available at https://console.redhat.com/openshift. From the Red Hat Hybrid Cloud Console, you can deploy {product-title} clusters to either on-premises or cloud environments.

You must use RHCOS machines for the control plane and for the compute machines.

Starting from {product-title} 4.14, the Extended Update Support (EUS) phase for even-numbered releases increases the total available lifecycle to 24 months on all supported architectures, including x86_64, 64-bit ARM (aarch64), IBM Power&#174; (ppc64le), and IBM Z&#174; (s390x) architectures. Beyond this, Red&#160;Hat also offers a 12-month additional EUS add-on, denoted as Additional EUS Term 2, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of {product-title}. For more information about support for all versions, see the Red Hat {product-title} Life Cycle Policy.

{product-title} is designed for FIPS. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, {product-title} core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.

For more information about the NIST validation program, see Cryptographic Module Validation Program. For the latest NIST status for the individual versions of RHEL cryptographic libraries that have been submitted for validation, see Compliance Activities and Government Standards.

# {product-title} layered and dependent component support and compatibility

The scope of support for layered and dependent components of {product-title} changes independently of the {product-title} version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the Red Hat {product-title} Life Cycle Policy.

# New features and enhancements

This release adds improvements related to the following components and concepts:

## Authentication and authorization



## Documentation



## Edge computing

### NetworkPolicy support for the LVM Storage Operator

The LVM Storage Operator now applies Kubernetes NetworkPolicy objects during installation to restrict network communication to only the required components. This feature enforces default network isolation for LVM Storage deployments on {product-title} clusters.

### Support for hostname labelling for persistent volumes created by using the LVM Storage Operator

When you create a persistent volume (PV) by using the LVM Storage Operator, the PV now includes the kubernetes.io/hostname label. This label shows which node the PV is located on, making it easier to identify the node associated with a workload. This change only applies to newly created PVs. Existing PVs are not modified.

### Default namespace for the LVM Storage Operator

The default namespace for the LVM Storage Operator is now openshift-lvm-storage. You can still install LVM Storage in a custom namespace.

### SiteConfig CR to ClusterInstance CR migration tool

{product-title} {product-version} introduces the siteconfig-converter tool to help migrate managed clusters from using a SiteConfig custom resource (CR) to a ClusterInstance CR. Using a SiteConfig CR to define a managed cluster is deprecated and will be removed in a future release. The ClusterInstance CR provides a more unified and generic approach to defining clusters and is the preferred method for managing cluster deployments in the GitOps ZTP workflow.

Using the siteconfig-converter tool, you can convert SiteConfig CRs to ClusterInstance CRs and then incrementally migrate one or more clusters at a time. Existing and new pipelines run in parallel, so you can migrate clusters in a controlled, phased manner and without downtime.


[NOTE]
----
The siteconfig-converter tool does not convert SiteConfig CRs that use the deprecated spec.clusters.extraManifestPath field.
----

For more information, see Migrating from SiteConfig CRs to ClusterInstance CRs.

## Extensions (OLM v1)



## Hosted control planes

Because hosted control planes releases asynchronously from {product-title}, it has its own release notes. For more information, see Hosted control planes release notes.

## IBM Power

The IBM Power&#174; release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on IBM Power:

## IBM Z and IBM LinuxONE

The IBM Z&#174; and IBM&#174; LinuxONE release on {product-title} {product-version} adds improvements and new capabilities to {product-title} components.

This release introduces support for the following features on IBM Z&#174; and IBM&#174; LinuxONE:

## IBM Power, IBM Z, and IBM LinuxONE support matrix

Starting in {product-title} 4.14, Extended Update Support (EUS) is extended to the IBM Power&#174; and the IBM Z&#174; platform. For more information, see the OpenShift EUS Overview.











1. Persistent shared storage must be provisioned by using either Red Hat OpenShift Data Foundation or other supported storage protocols.
2. Persistent non-shared storage must be provisioned by using local storage, such as iSCSI, FC, or by using LSO with DASD, FCP, or EDEV/FBA.

## Insights Operator



## Installation and update

### Installing a cluster on Google Cloud Platform into a shared VPC specifying a DNS private zone in a third project

With this release, you can specify the location of a DNS private zone when installing a cluster on GCP into a shared VPC. The private zone can be located in a service project that is distinct from the host project or main service project.

For more information, see Additional GCP configuration parameters.

### Installing a cluster on Microsoft Azure with virtual network encryption

With this release, you can install a cluster on Azure using encrypted virtual networks. You are required to use Azure virtual machines that have the premiumIO parameter set to true. See Microsoft&#8217;s documentation about Creating a virtual network with encryption and Requirements and Limitations for more information.

### Firewall requirements when installing a cluster that uses IBM Cloud Paks

With this release, if you install a cluster using IBM Cloud Paks, you must allow outbound access to icr.io and cp.icr.io on port 443. This access is required for IBM Cloud Pak container images. For more information, see Configuring your firewall.

### Installing a cluster on Microsoft Azure using Intel TDX Confidential VMs

With this release, you can install a cluster on Azure using Intel-based Confidential VMs. The following machine sizes are now supported:

* DCesv5-series
* DCedsv5-series
* ECesv5-series
* ECedsv5-series

For more information, see Enabling confidential VMs.

## Machine Config Operator



## Machine management

### Additional AWS Capacity Reservation configuration options

On clusters that manage machines with the Cluster API, you can specify additional constraints to determine whether your compute machines use AWS capacity reservations. For more information, see Capacity Reservation configuration options.

## Monitoring



## Networking

### Support for the BGP routing protocol

The Cluster Network Operator (CNO) now supports enabling Border Gateway Protocol (BGP) routing. With BGP, you can import and export routes to the underlying provider network and use multi-homing, link redundancy, and fast convergence. BGP configuration is managed with the FRRConfiguration custom resource (CR).

When upgrading from an earlier version of {product-title} in which you installed the MetalLB Operator, you must manually migrate your custom frr-k8s configurations from the metallb-system namespace to the openshift-frr-k8s namespace. To move these CRs, enter the following commands:

1. To create the openshift-frr-k8s namespace, enter the following command:

```terminal
$ oc create namespace openshift-frr-k8s
```

2. To automate the migration, create a migrate.sh file with the following content:

```bash
#!/bin/bash
OLD_NAMESPACE="metallb-system"
NEW_NAMESPACE="openshift-frr-k8s"
FILTER_OUT="metallb-"
oc get frrconfigurations.frrk8s.metallb.io -n "${OLD_NAMESPACE}" -o json |\
  jq -r '.items[] | select(.metadata.name | test("'"${FILTER_OUT}"'") | not)' |\
  jq -r '.metadata.namespace = "'"${NEW_NAMESPACE}"'"' |\
  oc create -f -
```

3. To run the migration script, enter the following command:

```terminal
$ bash migrate.sh
```

4. To verify that the migration succeeded, enter the following command:

```terminal
$ oc get frrconfigurations.frrk8s.metallb.io -n openshift-frr-k8s
```


After the migration is complete, you can remove the FRR-K8s custom resources from the metallb-system namespace.

For more information, see About BGP routing.

### Support for route advertisements for cluster user-defined networks (CUDNs) with Border Gateway Protocol (BGP)

With route advertisements enabled, the OVN-Kubernetes network plugin supports the direct advertisement of routes for pods and services associated with cluster user-defined networks (CUDNs) to the provider network. This feature enables some of the following benefits:

* Learns routes to pods dynamically
* Advertises routes dynamically
* Enables layer 3 notifications of EgressIP failovers in addition to the layer 2 ones based on gratuitous ARPs.
* Supports external route reflectors, which reduces the number of BGP connections required in large networks

For more information, see About route advertisements.

### Configuring enhanced PTP logging

You can now configure enhanced log reduction for the PTP Operator to reduce the volume of logs generated by the linuxptp-daemon.

This feature provides a periodic summary of filtered logs, which is not available with basic log reduction. Optionally, you can set a specific interval for the summary logs and a threshold in nanoseconds for the master offset logs.

For more information, see Configuring enhanced PTP logging.

### PTP ordinary clocks with added redundancy on AArch64 nodes (Technology Preview)

With this release, you can configure PTP ordinary clocks with added redundancy on AArch64 architecture nodes that use the following dual-port NICs only:

* NVIDIA ConnectX-7 series
* NVIDIA BlueField-3 series, in NIC mode

This feature is available as a Technology Preview. For more information, see Using dual-port NICs to improve redundancy for PTP ordinary clocks.

### Load balancing configuration with bond CNI plugin (Technology Preview)

In this release you can now specify the transmit hash policy for load balancing across the aggregated interfaces with the xmitHashPolicy as part of bond CNI plugin configuration. This feature is available as a Technology Preview.

For more information, see Configuration for a Bond CNI secondary network.

### SR-IOV network management in application namespaces

With {product-title} {product-version}, you can now create and manage SR-IOV networks directly within your application namespaces. This new feature provides greater control over your network configurations and helps simplify your workflow.

Previously, creating an SR-IOV network required a cluster administrator to configure it for you. Now, you can manage these resources directly in your own namespace, which offers several key benefits:

* Increased autonomy and control: You can now create your own SriovNetwork objects, removing the need to involve a cluster administrator for network configuration tasks.
* Enhanced security: Managing resources within your own namespace improves security by providing better separation between applications and helps prevent unintentional misconfigurations.
* Simplified permissions: You can now simplify permissions and reduce operational overhead by using namespaced SR-IOV networks.

For more information, see Configuring namespaced SR-IOV resources.

## Nodes



## OpenShift CLI (oc)

### Introducing the oc adm upgrade recommend command (General Availability)

Formerly Technology Preview and now Generally Available, the oc adm upgrade recommend command allows system administrators to perform a pre-update check on their {product-title} clusters using the command line interface (CLI). The pre-update check helps identify potential issues, enabling users to address them before initiating an update. By running the precheck command and inspecting the output, users can prepare for updating their cluster and make informed decisions about when to start an update.

For more information, see Updating a cluster by using the CLI.

## Operator development

### Supported Operator base images

With this release, the following base images for Operator projects are updated for compatibility with {product-title} {product-version}. The runtime functionality and configuration APIs for these base images are supported for bug fixes and for addressing CVEs.

* The base image for Ansible-based Operator projects
* The base image for Helm-based Operator projects

For more information, see Updating the base image for existing Ansible- or Helm-based Operator projects for {product-title} 4.19 and later (Red&#160;Hat Knowledgebase).

## Postinstallation configuration



## Red Hat Enterprise Linux CoreOS (RHCOS)



## Scalability and performance

### Configuring NUMA-aware scheduler replicas and high availability (Technology Preview)

In {product-title} {product-version}, the NUMA Resources Operator automatically enables high availability (HA) mode by default. In this mode, the NUMA Resources Operator creates one scheduler replica for each control-plane node in the cluster to ensure redundancy. This default behavior occurs if the spec.replicas field is not specified in the NUMAResourcesScheduler Custom Resource (CR). Alternatively, you can explicitly set a specific number of scheduler replicas to override the default HA behavior or disable the scheduler entirely by setting the spec.replicas field to 0.

For more information, see Managing high availability (HA) for the NUMA-aware scheduler.

### Receive Packet Steering (RPS) is now disabled by default

With this release, Receive Packet Steering (RPS) is no longer configured when Performance Profile is applied. The RPS configuration affects containers that perform networking system calls, such as send, directly within latency-sensitive threads. To avoid latency impacts when RPS is not configured, move networking calls to helper threads or processes.

The previous RPS configuration resolved latency issues at the expense of overall pod kernel networking performance. The current default configuration promotes transparency by requiring developers to address the underlying application design instead of obscuring performance impacts.

To revert to the previous behavior, add the performance.openshift.io/enable-rps annotation to the PerformanceProfile manifest:


```yaml
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: example-performanceprofile
  annotations:
    performance.openshift.io/enable-rps: "enable"
```



[NOTE]
----
This action restores the prior functionality at the cost of globally reducing networking performance for all pods.
----

## Security



## Storage



## Web console



# Notable technical changes

## oc-mirror plugin v2 verifies credentials and certificates before mirroring operations

With this update, the oc-mirror plugin v2 now verifies information such as registry credentials, DNS name, and SSL certificates before populating the cache and beginning mirroring operations.
This prevents users from discovering certain problems only after the cache is populated and mirroring has begun.

# Deprecated and removed features

## Images deprecated and removed features



## Installation deprecated and removed features



## Networking deprecated and removed features



## Node deprecated and removed features



## OpenShift CLI (oc) deprecated and removed features



## Operator lifecycle and development deprecated and removed features



## Specialized hardware and driver enablement deprecated and removed features



## Storage deprecated and removed features



## Updating clusters deprecated and removed features



## Web console deprecated and removed features



## Workloads deprecated and removed features



## Deprecated features



## Removed features



# Bug fixes

## API Server and Authentication



## Bare Metal Hardware Provisioning



## Cloud Compute



## Cluster Autoscaler



## Cluster Resource Override Admission Operator



## Cluster Version Operator



## ImageStreams



## Installer



## Machine Config Operator



## Management Console



## Monitoring



## Networking



## Node



## Node Tuning Operator (NTO)



## Observability



## oc-mirror



## OpenShift CLI (oc)



## Operator Lifecycle Manager (OLM)



## Operator Controller Manager



## Performance Addon Operator



## Samples Operator



## Storage



## Red Hat Enterprise Linux CoreOS (RHCOS)



# Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red&#160;Hat Customer Portal for these features:

Technology Preview Features Support Scope

In the following tables, features are marked with the following statuses:

* Not Available
* Technology Preview
* General Availability
* Deprecated
* Removed

## Authentication and authorization Technology Preview features



## Edge computing Technology Preview features



## Extensions Technology Preview features



## Installation Technology Preview features



## Machine Config Operator Technology Preview features



## Machine management Technology Preview features



## Monitoring Technology Preview features



## Multi-Architecture Technology Preview features



## Networking Technology Preview features



## Node Technology Preview features



## OpenShift CLI (oc) Technology Preview features



## Operator lifecycle and development Technology Preview features



## Red Hat OpenStack Platform (RHOSP) Technology Preview features



## Scalability and performance Technology Preview features



## Specialized hardware and driver enablement Technology Preview features



## Storage Technology Preview features



## Web console Technology Preview features



# Known issues



# Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} {product-version} are released as asynchronous errata through the Red&#160;Hat Network. All {product-title} {product-version} errata is available on the Red Hat Customer Portal. See the {product-title} Life Cycle for more information about asynchronous errata.

Red&#160;Hat Customer Portal users can enable errata notifications in the account settings for Red&#160;Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.


[NOTE]
----
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
----

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} {product-version}. Versioned asynchronous releases, for example with the form {product-title} {product-version}.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.


[IMPORTANT]
----
For any {product-title} release, always review the instructions on updating your cluster properly.
----

## RHSA-202X:XXXXX - {product-title} {product-version}.0 image release, bug fix, and security update advisory

Issued: DD MMM YYYY

{product-title} release {product-version}.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the RHSA-202X:XXXXX advisory. The RPM packages that are included in the update are provided by the RHEA-202X:XXXX advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:


```terminal
$ oc adm release info 4.20.0 --pullspecs
```


### Updating

To update an {product-title} 4.20 cluster to this latest release, see Updating a cluster using the CLI.