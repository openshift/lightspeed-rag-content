# About PTP in OpenShift cluster nodes


Precision Time Protocol (PTP) is used to synchronize clocks in a network. When used in conjunction with hardware support, PTP is capable of sub-microsecond accuracy, and is more accurate than Network Time Protocol (NTP).

[IMPORTANT]
----
If your openshift-sdn cluster with PTP uses the User Datagram Protocol (UDP) for hardware time stamping and you migrate to the OVN-Kubernetes plugin, the hardware time stamping cannot be applied to primary interface devices, such as an Open vSwitch (OVS) bridge. As a result, UDP version 4 configurations cannot work with a br-ex interface.
----
You can configure linuxptp services and use PTP-capable hardware in {product-title} cluster nodes.
Use the {product-title} web console or OpenShift CLI (oc) to install PTP by deploying the PTP Operator. The PTP Operator creates and manages the linuxptp services and provides the following features:
* Discovery of the PTP-capable devices in the cluster.
* Management of the configuration of linuxptp services.
* Notification of PTP clock events that negatively affect the performance and reliability of your application with the PTP Operator cloud-event-proxy sidecar.

[NOTE]
----
The PTP Operator works with PTP-capable devices on clusters provisioned only on bare-metal infrastructure.
----

# Elements of a PTP domain

PTP is used to synchronize multiple nodes connected in a network, with clocks for each node.
The clocks synchronized by PTP are organized in a leader-follower hierarchy.
The hierarchy is created and updated automatically by the best master clock (BMC) algorithm, which runs on every clock.
Follower clocks are synchronized to leader clocks, and follower clocks can themselves be the source for other downstream clocks.

![Diagram showing a PTP grandmaster clock]

The three primary types of PTP clocks are described below.

Grandmaster clock:: The grandmaster clock provides standard time information to other clocks across the network and ensures accurate and stable synchronisation. It writes time stamps and responds to time requests from other clocks. Grandmaster clocks synchronize to a Global Navigation Satellite System (GNSS) time source. The Grandmaster clock is the authoritative source of time in the network and is responsible for providing time synchronization to all other devices.
Boundary clock:: The boundary clock has ports in two or more communication paths and can be a source and a destination to other destination clocks at the same time. The boundary clock works as a destination clock upstream. The destination clock receives the timing message, adjusts for delay, and then creates a new source time signal to pass down the network. The boundary clock produces a new timing packet that is still correctly synced with the source clock and can reduce the number of connected devices reporting directly to the source clock.
Ordinary clock:: The ordinary clock has a single port connection that can play the role of source or destination clock, depending on its position in the network. The ordinary clock can read and write timestamps.

## Advantages of PTP over NTP

One of the main advantages that PTP has over NTP is the hardware support present in various network interface controllers (NIC) and network switches. The specialized hardware allows PTP to account for delays in message transfer and improves the accuracy of time synchronization. To achieve the best possible accuracy, it is recommended that all networking components between PTP clocks are PTP hardware enabled.

Hardware-based PTP provides optimal accuracy, since the NIC can timestamp the PTP packets at the exact moment they are sent and received. Compare this to software-based PTP, which requires additional processing of the PTP packets by the operating system.


[IMPORTANT]
----
Before enabling PTP, ensure that NTP is disabled for the required nodes. You can disable the chrony time service (chronyd) using a MachineConfig custom resource. For more information, see Disabling chrony time service.
----

# Overview of linuxptp and gpsd in {product-title} nodes

{product-title} uses the PTP Operator with linuxptp and gpsd packages for high precision network synchronization.
The linuxptp package provides tools and daemons for PTP timing in networks.
Cluster hosts with Global Navigation Satellite System (GNSS) capable NICs use gpsd to interface with GNSS clock sources.

The linuxptp package includes the ts2phc, pmc, ptp4l, and phc2sys programs for system clock synchronization.

ts2phc:: ts2phc synchronizes the PTP hardware clock (PHC) across PTP devices with a high degree of precision.
ts2phc is used in grandmaster clock configurations.
It receives the precision timing signal a high precision clock source such as Global Navigation Satellite System (GNSS).
GNSS provides an accurate and reliable source of synchronized time for use in large distributed networks.
GNSS clocks typically provide time information with a precision of a few nanoseconds.

The ts2phc system daemon sends timing information from the grandmaster clock to other PTP devices in the network by reading time information from the grandmaster clock and converting it to PHC format.
PHC time is used by other devices in the network to synchronize their clocks with the grandmaster clock.
pmc:: pmc implements a PTP management client (pmc) according to IEEE standard 1588.1588.
pmc provides basic management access for the ptp4l system daemon.
pmc reads from standard input and sends the output over the selected transport, printing any replies it receives.
ptp4l:: ptp4l implements the PTP boundary clock and ordinary clock and runs as a system daemon.
ptp4l does the following:
* Synchronizes the PHC to the source clock with hardware time stamping
* Synchronizes the system clock to the source clock with software time stamping
phc2sys:: phc2sys synchronizes the system clock to the PHC on the network interface controller (NIC).
The phc2sys system daemon continuously monitors the PHC for timing information.
When it detects a timing error, the PHC corrects the system clock.

The gpsd package includes the ubxtool, gspipe, gpsd, programs for GNSS clock synchronization with the host clock.

ubxtool:: ubxtool CLI allows you to communicate with a u-blox GPS system. The ubxtool CLI uses the u-blox binary protocol to communicate with the GPS.
gpspipe:: gpspipe connects to gpsd output and pipes it to stdout.
gpsd:: gpsd is a service daemon that monitors one or more GPS or AIS receivers connected to the host.

# Overview of GNSS timing for PTP grandmaster clocks

{product-title} supports receiving precision PTP timing from Global Navigation Satellite System (GNSS) sources and grandmaster clocks (T-GM) in the cluster.


[IMPORTANT]
----
{product-title} supports PTP timing from GNSS sources with Intel E810 Westport Channel NICs only.
----

![GNSS and T-GM system architecture]

Global Navigation Satellite System (GNSS):: GNSS is a satellite-based system used to provide positioning, navigation, and timing information to receivers around the globe.
In PTP, GNSS receivers are often used as a highly accurate and stable reference clock source.
These receivers receive signals from multiple GNSS satellites, allowing them to calculate precise time information.
The timing information obtained from GNSS is used as a reference by the PTP grandmaster clock.

By using GNSS as a reference, the grandmaster clock in the PTP network can provide highly accurate timestamps to other devices, enabling precise synchronization across the entire network.
Digital Phase-Locked Loop (DPLL):: DPLL provides clock synchronization between different PTP nodes in the network.
DPLL compares the phase of the local system clock signal with the phase of the incoming synchronization signal, for example, PTP messages from the PTP grandmaster clock.
The DPLL continuously adjusts the local clock frequency and phase to minimize the phase difference between the local clock and the reference clock.

## Handling leap second events in GNSS-synced PTP grandmaster clocks

A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) to keep it synchronized with International Atomic Time (TAI).
UTC leap seconds are unpredictable.
Internationally agreed leap seconds are listed in leap-seconds.list.
This file is regularly updated by the International Earth Rotation and Reference Systems Service (IERS).
An unhandled leap second can have a significant impact on far edge RAN networks.
It can cause the far edge RAN application to immediately disconnect voice calls and data sessions.

# About PTP and clock synchronization error events

Cloud native applications such as virtual RAN (vRAN) require access to notifications about hardware timing events that are critical to the functioning of the overall network.
PTP clock synchronization errors can negatively affect the performance and reliability of your low-latency application, for example, a vRAN application running in a distributed unit (DU).

Loss of PTP synchronization is a critical error for a RAN network.
If synchronization is lost on a node, the radio might be shut down and the network Over the Air (OTA) traffic might be shifted to another node in the wireless network.
Fast event notifications mitigate against workload errors by allowing cluster nodes to communicate PTP clock sync status to the vRAN application running in the DU.

Event notifications are available to vRAN applications running on the same DU node.
A publish/subscribe REST API passes events notifications to the messaging bus.
Publish/subscribe messaging, or pub-sub messaging, is an asynchronous service-to-service communication architecture where any message published to a topic is immediately received by all of the subscribers to the topic.

The PTP Operator generates fast event notifications for every PTP-capable network interface.
The consumer application can subscribe to PTP events by using the PTP events REST API {ptp-events-rest-api}.


[NOTE]
----
PTP fast event notifications are available for network interfaces configured to use PTP ordinary clocks, PTP grandmaster clocks, or PTP boundary clocks.
----

# 2-card E810 NIC configuration reference

{product-title} supports single and dual-NIC Intel E810 hardware for PTP timing in grandmaster clocks (T-GM) and boundary clocks (T-BC).

Dual NIC grandmaster clock:: 
You can use a cluster host that has dual-NIC hardware as PTP grandmaster clock.
One NIC receives timing information from the global navigation satellite system (GNSS).
The second NIC receives the timing information from the first using the SMA1 Tx/Rx connections on the E810 NIC faceplate.
The system clock on the cluster host is synchronized from the NIC that is connected to the GNSS satellite.
Dual NIC grandmaster clocks are a feature of distributed RAN (D-RAN) configurations where the Remote Radio Unit (RRU) and Baseband Unit (BBU) are located at the same radio cell site.
D-RAN distributes radio functions across multiple sites, with backhaul connections linking them to the core network.
![Dual NIC PTP grandmaster clock connected to GNSS timing source and downstream PTP boundary and ordinary clocks]

[NOTE]
----
In a dual-NIC T-GM configuration, a single ts2phc program operate on two PTP hardware clocks (PHCs), one for each NIC.
----
Dual NIC boundary clock:: 
For 5G telco networks that deliver mid-band spectrum coverage, each virtual distributed unit (vDU) requires connections to 6 radio units (RUs). To make these connections, each vDU host requires 2 NICs configured as boundary clocks.
Dual NIC hardware allows you to connect each NIC to the same upstream leader clock with separate ptp4l instances for each NIC feeding the downstream clocks.
Highly available system clock with dual-NIC boundary clocks:: 
You can configure Intel E810-XXVDA4 Salem channel dual-NIC hardware as dual PTP boundary clocks that provide timing for a highly available system clock.
This configuration is useful when you have multiple time sources on different NICs.
High availability ensures that the node does not lose timing synchronization if one of the two timing sources is lost or disconnected.
Each NIC is connected to the same upstream leader clock.
Highly available boundary clocks use multiple PTP domains to synchronize with the target system clock.
When a T-BC is highly available, the host system clock can maintain the correct offset even if one or more ptp4l instances syncing the NIC PHC clock fails.
If any single SFP port or cable failure occurs, the boundary clock stays in sync with the leader clock.
Boundary clock leader source selection is done using the A-BMCA algorithm. For more information, see ITU-T recommendation G.8275.1.

# Using dual-port NICs to improve redundancy for PTP ordinary clocks

{product-title} supports single-port networking interface cards (NICs) as ordinary clocks for PTP timing. To improve redundancy, you can configure a dual-port NIC with one port as active and the other as standby.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----

In this configuration, the ports in a dual-port NIC operate as follows:

* The active port functions as an ordinary clock in the Following port state.
* The standby port remains in the Listening port state.
* If the active port fails, the standby port transitions to active to ensure continued PTP timing synchronization.
* If both ports become faulty, the clock state moves to the HOLDOVER state, then the FREERUN state when the holdover timeout expires, before resyncing to a leader clock.

## Hardware requirements

You can configure PTP ordinary clocks with added redundancy on x86_64 or AArch64 architecture nodes.

For x86_64 architecture nodes, the nodes must feature dual-port NICs that support PTP and expose a single PTP hardware clock (PHC) per NIC, such as the Intel E810.

For AArch64 architecture nodes, you can use the following dual-port NICs only:

* NVIDIA ConnectX-7 series
* NVIDIA BlueField-3 series, in NIC mode
* You must configure the NVIDIA BlueField-3 series DPU in NIC mode before configuring the interface as an ordinary clock with improved redundancy. For further information about configuring NIC mode, see NIC Mode for BlueField-3 (NVIDIA documentation), BlueField Management (NVIDIA documentation), and Configuring NIC Mode on BlueField-3 from Host BIOS HII UEFI Menu (NVIDIA documentation).
* You must restart the card after changing to NIC mode. For more information about restarting the card, see NVIDIA BlueField Reset and Reboot Procedures (NVIDIA documentation).
* Use the latest supported NVIDIA drivers and firmware to ensure proper PTP support and to expose a single PHC per NIC.

# 3-card Intel E810 PTP grandmaster clock

{product-title} supports cluster hosts with 3 Intel E810 NICs as PTP grandmaster clocks (T-GM).

3-card grandmaster clock:: 
You can use a cluster host that has 3 NICs as PTP grandmaster clock.
One NIC receives timing information from the global navigation satellite system (GNSS).
The second and third NICs receive the timing information from the first by using the SMA1 Tx/Rx connections on the E810 NIC faceplate.
The system clock on the cluster host is synchronized from the NIC that is connected to the GNSS satellite.
3-card NIC grandmaster clocks can be used for distributed RAN (D-RAN) configurations where the Radio Unit (RU) is connected directly to the distributed unit (DU) without a front haul switch, for example, if the RU and DU are located in the same radio cell site.
D-RAN distributes radio functions across multiple sites, with backhaul connections linking them to the core network.
![3-card PTP grandmaster clock connected to GNSS timing source and downstream PTP boundary and ordinary clocks]

[NOTE]
----
In a 3-card T-GM configuration, a single ts2phc process reports as 3 ts2phc instances in the system.
----