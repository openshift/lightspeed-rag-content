# Deploying hosted control planes on non-bare-metal agent machines


You can deploy hosted control planes by configuring a cluster to function as a hosting cluster. The hosting cluster is an {product-title} cluster where the control planes are hosted. The hosting cluster is also known as the management cluster.

[IMPORTANT]
----
Hosted control planes on non-bare-metal agent machines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----

[NOTE]
----
The management cluster is not the same thing as the managed cluster. A managed cluster is a cluster that the hub cluster manages.
----
The hosted control planes feature is enabled by default.
The multicluster engine Operator supports only the default local-cluster managed hub cluster. On Red Hat Advanced Cluster Management (RHACM) 2.10, you can use the local-cluster managed hub cluster as the hosting cluster.
A hosted cluster is an {product-title} cluster with its API endpoint and control plane that are hosted on the hosting cluster. The hosted cluster includes the control plane and its corresponding data plane. You can use the multicluster engine Operator console or the hcp command-line interface (CLI) to create a hosted cluster.
The hosted cluster is automatically imported as a managed cluster. If you want to disable this automatic import feature, see "Disabling the automatic import of hosted clusters into multicluster engine Operator".

# Preparing to deploy hosted control planes on non-bare-metal agent machines

As you prepare to deploy hosted control planes on bare metal, consider the following information:

* You can add agent machines as a worker node to a hosted cluster by using the Agent platform. Agent machine represents a host booted with a Discovery Image and ready to be provisioned as an {product-title} node. The Agent platform is part of the central infrastructure management service. For more information, see Enabling the central infrastructure management service.
* All hosts that are not bare metal require a manual boot with a Discovery Image ISO that the central infrastructure management provides.
* When you scale up the node pool, a machine is created for every replica. For every machine, the Cluster API provider finds and installs an Agent that is approved, is passing validations, is not currently in use, and meets the requirements that are specified in the node pool specification. You can monitor the installation of an Agent by checking its status and conditions.
* When you scale down a node pool, Agents are unbound from the corresponding cluster. Before you can reuse the Agents, you must restart them by using the Discovery image.
* When you configure storage for hosted control planes, consider the recommended etcd practices. To ensure that you meet the latency requirements, dedicate a fast storage device to all hosted control planes etcd instances that run on each control-plane node. You can use LVM storage to configure a local storage class for hosted etcd pods. For more information, see "Recommended etcd practices" and "Persistent storage using logical volume manager storage" in the {product-title} documentation.

## Prerequisites for deploying hosted control planes on non-bare-metal agent machines

Before you deploy hosted control planes on non-bare-metal agent machines, ensure you meet the following prerequisites:

* You must have multicluster engine for Kubernetes Operator 2.5 or later installed on an {product-title} cluster. You can install the multicluster engine Operator as an Operator from the {product-title} software catalog.
* You must have at least one managed {product-title} cluster for the multicluster engine Operator. The local-cluster management cluster is automatically imported. For more information about the local-cluster, see Advanced configuration in the Red Hat Advanced Cluster Management documentation. You can check the status of your management cluster by running the following command:

```terminal
$ oc get managedclusters local-cluster
```

* You have enabled central infrastructure management. For more information, see Enabling the central infrastructure management service in the Red Hat Advanced Cluster Management documentation.
* You have installed the hcp command-line interface.
* Your hosted cluster has a cluster-wide unique name.
* You are running the management cluster and workers on the same infrastructure.

* Advanced configuration
* Enabling the central infrastructure management service

## Firewall, port, and service requirements for non-bare-metal agent machines

You must meet the firewall and port requirements so that ports can communicate between the management cluster, the control plane, and hosted clusters.


[NOTE]
----
Services run on their default ports. However, if you use the NodePort publishing strategy, services run on the port that is assigned by the NodePort service.
----

Use firewall rules, security groups, or other access controls to restrict access to only required sources. Avoid exposing ports publicly unless necessary. For production deployments, use a load balancer to simplify access through a single IP address.

A hosted control plane exposes the following services on non-bare-metal agent machines:

* APIServer
* The APIServer service runs on port 6443 by default and requires ingress access for communication between the control plane components.
* If you use MetalLB load balancing, allow ingress access to the IP range that is used for load balancer IP addresses.
* OAuthServer
* The OAuthServer service runs on port 443 by default when you use the route and ingress to expose the service.
* If you use the NodePort publishing strategy, use a firewall rule for the OAuthServer service.
* Konnectivity
* The Konnectivity service runs on port 443 by default when you use the route and ingress to expose the service.
* The Konnectivity agent establishes a reverse tunnel to allow the control plane to access the network for the hosted cluster. The agent uses egress to connect to the Konnectivity server. The server is exposed by using either a route on port 443 or a manually assigned NodePort.
* If the cluster API server address is an internal IP address, allow access from the workload subnets to the IP address on port 6443.
* If the address is an external IP address, allow egress on port 6443 to that external IP address from the nodes.
* Ignition
* The Ignition service runs on port 443 by default when you use the route and ingress to expose the service.
* If you use the NodePort publishing strategy, use a firewall rule for the Ignition service.

You do not need the following services on non-bare-metal agent machines:

* OVNSbDb
* OIDC

## Infrastructure requirements for non-bare-metal agent machines

The Agent platform does not create any infrastructure, but it has the following infrastructure requirements:

* Agents: An Agent represents a host that is booted with a discovery image and is ready to be provisioned as an {product-title} node.
* DNS: The API and ingress endpoints must be routable.

* Recommended etcd practices
* Persistent storage using logical volume manager storage
* Disabling the automatic import of hosted clusters into multicluster engine Operator
* Manually enabling the hosted control planes feature
* Disabling the hosted control planes feature
* Configuring Ansible Automation Platform jobs to run on hosted clusters

# Configuring DNS on non-bare-metal agent machines

The API Server for the hosted cluster is exposed as a NodePort service. A DNS entry must exist for api.<hosted_cluster_name>.<basedomain> that points to destination where the API Server can be reached.

The DNS entry can be as simple as a record that points to one of the nodes in the managed cluster that is running the hosted control plane. The entry can also point to a load balancer that is deployed to redirect incoming traffic to the ingress pods.

* If you are configuring DNS for a connected environment on an IPv4 network, see the following example DNS configuration:

```text
api.example.krnl.es.        IN A 192.168.122.20
api.example.krnl.es.        IN A 192.168.122.21
api.example.krnl.es.        IN A 192.168.122.22
api-int.example.krnl.es.    IN A 192.168.122.20
api-int.example.krnl.es.    IN A 192.168.122.21
api-int.example.krnl.es.    IN A 192.168.122.22
`*`.apps.example.krnl.es.   IN A 192.168.122.23
```

* If you are configuring DNS for a disconnected environment on an IPv6 network, see the following example DNS configuration:

```text
api.example.krnl.es.        IN A 2620:52:0:1306::5
api.example.krnl.es.        IN A 2620:52:0:1306::6
api.example.krnl.es.        IN A 2620:52:0:1306::7
api-int.example.krnl.es.    IN A 2620:52:0:1306::5
api-int.example.krnl.es.    IN A 2620:52:0:1306::6
api-int.example.krnl.es.    IN A 2620:52:0:1306::7
`*`.apps.example.krnl.es.   IN A 2620:52:0:1306::10
```

* If you are configuring DNS for a disconnected environment on a dual stack network, be sure to include DNS entries for both IPv4 and IPv6. See the following example DNS configuration:

```text
host-record=api-int.hub-dual.dns.base.domain.name,192.168.126.10
host-record=api.hub-dual.dns.base.domain.name,192.168.126.10
address=/apps.hub-dual.dns.base.domain.name/192.168.126.11
dhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,192.168.126.20
dhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,192.168.126.21
dhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,192.168.126.22
dhcp-host=aa:aa:aa:aa:10:06,ocp-installer,192.168.126.25
dhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,192.168.126.26

host-record=api-int.hub-dual.dns.base.domain.name,2620:52:0:1306::2
host-record=api.hub-dual.dns.base.domain.name,2620:52:0:1306::2
address=/apps.hub-dual.dns.base.domain.name/2620:52:0:1306::3
dhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,[2620:52:0:1306::5]
dhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,[2620:52:0:1306::6]
dhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,[2620:52:0:1306::7]
dhcp-host=aa:aa:aa:aa:10:06,ocp-installer,[2620:52:0:1306::8]
dhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,[2620:52:0:1306::9]
```


## Defining a custom DNS name

As a cluster administrator, you can create a hosted cluster with an external API DNS name that differs from the internal endpoint that gets used for node bootstraps and control plane communication. You might want to define a different DNS name for the following reasons:

* To replace the user-facing TLS certificate with one from a public CA without breaking the control plane functions that bind to the internal root CA.
* To support split-horizon DNS and NAT scenarios.
* To ensure a similar experience to standalone control planes, where you can use functions, such as the Show Login Command function, with the correct kubeconfig and DNS configuration.

You can define a DNS name either during your initial setup or during postinstallation operations, by entering a domain name in the kubeAPIServerDNSName parameter of a HostedCluster object.

* You have a valid TLS certificate that covers the DNS name that you set in the kubeAPIServerDNSName parameter.
* You have a resolvable DNS name URI that can reach and point to the correct address.

* In the specification for the HostedCluster object, add the kubeAPIServerDNSName parameter and the address for the domain and specify which certificate to use, as shown in the following example:

```yaml
#...
spec:
  configuration:
    apiServer:
      servingCerts:
        namedCertificates:
        - names:
          - xxx.example.com
          - yyy.example.com
          servingCertificate:
            name: <my_serving_certificate>
  kubeAPIServerDNSName: <custom_address> 1
```

The value for the kubeAPIServerDNSName parameter must be a valid and addressable domain.

After you define the kubeAPIServerDNSName parameter and specify the certificate, the Control Plane Operator controllers create a kubeconfig file named custom-admin-kubeconfig, where the file gets stored in the HostedControlPlane namespace. The generation of certificates happen from the root CA, and the HostedControlPlane namespace manages their expiration and renewal.

The Control Plane Operator reports a new kubeconfig file named CustomKubeconfig in the HostedControlPlane namespace. That file uses the defined new server in the kubeAPIServerDNSName parameter.

A reference for the custom kubeconfig file exists in the status parameter as CustomKubeconfig of the HostedCluster object. The CustomKubeConfig parameter is optional, and you can add the parameter only if the kubeAPIServerDNSName parameter is not empty. After you set the CustomKubeConfig parameter, the parameter triggers the generation of a secret named <hosted_cluster_name>-custom-admin-kubeconfig in the HostedCluster namespace. You can use the secret to access the HostedCluster API server. If you remove the CustomKubeConfig parameter during postinstallation operations, deletion of all related secrets and status references occur.


[NOTE]
----
Defining a custom DNS name does not directly impact the data plane, so no expected rollouts occur. The HostedControlPlane namespace receives the changes from the HyperShift Operator and deletes the corresponding parameters.
----

If you remove the kubeAPIServerDNSName parameter from the specification for the HostedCluster object, all newly generated secrets and the CustomKubeconfig reference are removed from the cluster and from the status parameter.

# Creating a hosted cluster on non-bare-metal agent machines by using the CLI

When you create a hosted cluster with the Agent platform, the HyperShift Operator installs the Agent Cluster API provider in the hosted control plane namespace. You can create a hosted cluster on bare metal or import one.

As you create a hosted cluster, review the following guidelines:

* Each hosted cluster must have a cluster-wide unique name. A hosted cluster name cannot be the same as any existing managed cluster in order for multicluster engine Operator to manage it.
* Do not use clusters as a hosted cluster name.
* A hosted cluster cannot be created in the namespace of a multicluster engine Operator managed cluster.

1. Create the hosted control plane namespace by entering the following command:

```terminal
$ oc create ns <hosted_cluster_namespace>-<hosted_cluster_name> 1
```

Replace <hosted_cluster_namespace> with your hosted cluster namespace name, for example, clusters. Replace <hosted_cluster_name> with your hosted cluster name.
2. Create a hosted cluster by entering the following command:

```terminal
$ hcp create cluster agent \
  --name=<hosted_cluster_name> \1
  --pull-secret=<path_to_pull_secret> \2
  --agent-namespace=<hosted_control_plane_namespace> \3
  --base-domain=<basedomain> \4
  --api-server-address=api.<hosted_cluster_name>.<basedomain> \5
  --etcd-storage-class=<etcd_storage_class> \6
  --ssh-key  <path_to_ssh_key> \7
  --namespace <hosted_cluster_namespace> \8
  --control-plane-availability-policy HighlyAvailable \9
  --release-image=quay.io/openshift-release-dev/ocp-release:<ocp_release> \10
  --node-pool-replicas <node_pool_replica_count> 11
```

Specify the name of your hosted cluster, for instance, example.
Specify the path to your pull secret, for example, /user/name/pullsecret.
Specify your hosted control plane namespace, for example, clusters-example. Ensure that agents are available in this namespace by using the oc get agent -n <hosted-control-plane-namespace> command.
Specify your base domain, for example, krnl.es.
The --api-server-address flag defines the IP address that is used for the Kubernetes API communication in the hosted cluster. If you do not set the --api-server-address flag, you must log in to connect to the management cluster.
Verify that you have a default storage class configured for your cluster. Otherwise, you might end up with pending PVCs. Specify the etcd storage class name, for example, lvm-storageclass.
Specify the path to your SSH public key. The default file path is ~/.ssh/id_rsa.pub.
Specify your hosted cluster namespace.
Specify the availability policy for the hosted control plane components. Supported options are SingleReplica and HighlyAvailable. The default value is HighlyAvailable.
Specify the supported {product-title} version that you want to use, for example, 4.19.0-multi.
Specify the node pool replica count, for example, 3. You must specify the replica count as 0 or greater to create the same number of replicas. Otherwise, no node pools are created.

* After a few moments, verify that your hosted control plane pods are up and running by entering the following command:

```terminal
$ oc -n <hosted_cluster_namespace>-<hosted_cluster_name> get pods
```

Example output

```terminal
NAME                                             READY   STATUS    RESTARTS   AGE
catalog-operator-6cd867cc7-phb2q                 2/2     Running   0          2m50s
control-plane-operator-f6b4c8465-4k5dh           1/1     Running   0          4m32s
```


* Manually importing a hosted cluster

## Creating a hosted cluster on non-bare-metal agent machines by using the web console

You can create a hosted cluster on non-bare-metal agent machines by using the {product-title} web console.

* You have access to the cluster with cluster-admin privileges.
* You have access to the {product-title} web console.

1. Open the {product-title} web console and log in by entering your administrator credentials.
2. In the console header, select All Clusters.
3. Click Infrastructure -> Clusters.
4. Click Create cluster  Host inventory -> Hosted control plane.

The Create cluster page is displayed.
5. On the Create cluster page, follow the prompts to enter details about the cluster, node pools, networking, and automation.

As you enter details about the cluster, you might find the following tips useful:

* If you want to use predefined values to automatically populate fields in the console, you can create a host inventory credential. For more information, see Creating a credential for an on-premises environment.
* On the Cluster details page, the pull secret is your {product-title} pull secret that you use to access {product-title} resources. If you selected a host inventory credential, the pull secret is automatically populated.
* On the Node pools page, the namespace contains the hosts for the node pool. If you created a host inventory by using the console, the console creates a dedicated namespace.
* On the Networking page, you select an API server publishing strategy. The API server for the hosted cluster can be exposed either by using an existing load balancer or as a service of the NodePort type. A DNS entry must exist for the api.<hosted_cluster_name>.<basedomain> setting that points to the destination where the API server can be reached. This entry can be a record that points to one of the nodes in the management cluster or a record that points to a load balancer that redirects incoming traffic to the Ingress pods.
1. Review your entries and click Create.

The Hosted cluster view is displayed.
1. Monitor the deployment of the hosted cluster in the Hosted cluster view. If you do not see information about the hosted cluster, ensure that All Clusters is selected, and click the cluster name. Wait until the control plane components are ready. This process can take a few minutes.
2. To view the node pool status, scroll to the NodePool section. The process to install the nodes takes about 10 minutes. You can also click Nodes to confirm whether the nodes joined the hosted cluster.

* To access the web console, see Accessing the web console.

## Creating a hosted cluster on non-bare-metal agent machines by using a mirror registry

You can use a mirror registry to create a hosted cluster on non-bare-metal agent machines by specifying the --image-content-sources flag in the hcp create cluster command.

1. Create a YAML file to define Image Content Source Policies (ICSP). See the following example:

```yaml
- mirrors:
  - brew.registry.redhat.io
  source: registry.redhat.io
- mirrors:
  - brew.registry.redhat.io
  source: registry.stage.redhat.io
- mirrors:
  - brew.registry.redhat.io
  source: registry-proxy.engineering.redhat.com
```

2. Save the file as icsp.yaml. This file contains your mirror registries.
3. To create a hosted cluster by using your mirror registries, run the following command:

```terminal
$ hcp create cluster agent \
    --name=<hosted_cluster_name> \1
    --pull-secret=<path_to_pull_secret> \2
    --agent-namespace=<hosted_control_plane_namespace> \3
    --base-domain=<basedomain> \4
    --api-server-address=api.<hosted_cluster_name>.<basedomain> \5
    --image-content-sources icsp.yaml  \6
    --ssh-key  <path_to_ssh_key> \7
    --namespace <hosted_cluster_namespace> \8
    --release-image=quay.io/openshift-release-dev/ocp-release:<ocp_release_image> 9
```

Specify the name of your hosted cluster, for instance, example.
Specify the path to your pull secret, for example, /user/name/pullsecret.
Specify your hosted control plane namespace, for example, clusters-example. Ensure that agents are available in this namespace by using the oc get agent -n <hosted-control-plane-namespace> command.
Specify your base domain, for example, krnl.es.
The --api-server-address flag defines the IP address that is used for the Kubernetes API communication in the hosted cluster. If you do not set the --api-server-address flag, you must log in to connect to the management cluster.
Specify the icsp.yaml file that defines ICSP and your mirror registries.
Specify the path to your SSH public key. The default file path is ~/.ssh/id_rsa.pub.
Specify your hosted cluster namespace.
Specify the supported {product-title} version that you want to use, for example, 4.19.0-multi. If you are using a disconnected environment, replace <ocp_release_image> with the digest image. To extract the {product-title} release image digest, see Extracting the {product-title} release image digest.

* To create credentials that you can reuse when you create a hosted cluster with the console, see Creating a credential for an on-premises environment.
* To access a hosted cluster, see Accessing the hosted cluster.
* To add hosts to the host inventory by using the Discovery Image, see Adding hosts to the host inventory by using the Discovery Image.
* To extract the {product-title} release image digest, see Extracting the {product-title} release image digest.

# Verifying hosted cluster creation on non-bare-metal agent machines

After the deployment process is complete, you can verify that the hosted cluster was created successfully. Follow these steps a few minutes after you create the hosted cluster.

1. Obtain the kubeconfig file for your new hosted cluster by entering the following command:

```terminal
$ oc extract -n <hosted_cluster_namespace> \
  secret/<hosted_cluster_name>-admin-kubeconfig --to=- \
  > kubeconfig-<hosted_cluster_name>
```

2. Use the kubeconfig file to view the cluster Operators of the hosted cluster. Enter the following command:

```terminal
$ oc get co --kubeconfig=kubeconfig-<hosted_cluster_name>
```

Example output

```terminal
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.10.26   True        False         False      2m38s
csi-snapshot-controller                    4.10.26   True        False         False      4m3s
dns                                        4.10.26   True        False         False      2m52s
```

3. View the running pods on your hosted cluster by entering the following command:

```terminal
$ oc get pods -A --kubeconfig=kubeconfig-<hosted_cluster_name>
```

Example output

```terminal
NAMESPACE                                          NAME                                                      READY   STATUS             RESTARTS        AGE
kube-system                                        konnectivity-agent-khlqv                                  0/1     Running            0               3m52s
openshift-cluster-samples-operator                 cluster-samples-operator-6b5bcb9dff-kpnbc                 2/2     Running            0               20m
openshift-monitoring                               alertmanager-main-0                                       6/6     Running            0               100s
openshift-monitoring                               openshift-state-metrics-677b9fb74f-qqp6g                  3/3     Running            0               104s
```


# Configuring a custom API server certificate in a hosted cluster

To configure a custom certificate for the API server, specify the certificate details in the spec.configuration.apiServer section of your HostedCluster configuration.

You can configure a custom certificate during either day-1 or day-2 operations. However, because the service publishing strategy is immutable after you set it during hosted cluster creation, you must know what the hostname is for the Kubernetes API server that you plan to configure.

* You created a Kubernetes secret that contains your custom certificate in the management cluster. The secret contains the following keys:
* tls.crt: The certificate
* tls.key: The private key
* If your HostedCluster configuration includes a service publishing strategy that uses a load balancer, ensure that the Subject Alternative Names (SANs) of the certificate do not conflict with the internal API endpoint (api-int). The internal API endpoint is automatically created and managed by your platform. If you use the same hostname in both the custom certificate and the internal API endpoint, routing conflicts can occur. The only exception to this rule is when you use AWS as the provider with either Private or PublicAndPrivate configurations. In those cases, the SAN conflict is managed by the platform.
* The certificate must be valid for the external API endpoint.
* The validity period of the certificate aligns with your cluster's expected life cycle.

1. Create a secret with your custom certificate by entering the following command:

```terminal
$ oc create secret tls sample-hosted-kas-custom-cert \
  --cert=path/to/cert.crt \
  --key=path/to/key.key \
  -n <hosted_cluster_namespace>
```

2. Update your HostedCluster configuration with the custom certificate details, as shown in the following example:

```yaml
spec:
  configuration:
    apiServer:
      servingCerts:
        namedCertificates:
        - names: 1
          - api-custom-cert-sample-hosted.sample-hosted.example.com
          servingCertificate: 2
            name: sample-hosted-kas-custom-cert
```

The list of DNS names that the certificate is valid for.
The name of the secret that contains the custom certificate.
3. Apply the changes to your HostedCluster configuration by entering the following command:

```terminal
$ oc apply -f <hosted_cluster_config>.yaml
```


* Check the API server pods to ensure that the new certificate is mounted.
* Test the connection to the API server by using the custom domain name.
* Verify the certificate details in your browser or by using tools such as openssl.