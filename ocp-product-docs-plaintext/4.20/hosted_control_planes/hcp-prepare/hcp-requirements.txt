# Requirements for hosted control planes


In the context of hosted control planes, a management cluster is an {product-title} cluster where the HyperShift Operator is deployed and where the control planes for hosted clusters are hosted.
The control plane is associated with a hosted cluster and runs as pods in a single namespace. When the cluster service consumer creates a hosted cluster, it creates a worker node that is independent of the control plane.
The following requirements apply to hosted control planes:
* In order to run the HyperShift Operator, your management cluster needs at least three worker nodes.
* You can run both the management cluster and the worker nodes on-premise, such as on a bare-metal platform or on OpenShift Virtualization. In addition, you can run both the management cluster and the worker nodes on cloud infrastructure, such as Amazon Web Services (AWS).
* If you use a mixed infrastructure, such as running the management cluster on AWS and your worker nodes on-premise, or running your worker nodes on AWS and your management cluster on-premise, you must use the PublicAndPrivate publishing strategy and follow the latency requirements in the support matrix.
* In Bare Metal Host (BMH) deployments, where the Bare Metal Operator starts machines, the hosted control plane must be able to reach baseboard management controllers (BMCs). If your security profile does not permit the Cluster Baremetal Operator to access the network where the BMHs have their BMCs in order to enable Redfish automation, you can use BYO ISO support. However, in BYO mode, {product-title} cannot automate the powering on of BMHs.

# Support matrix for hosted control planes

Because multicluster engine for Kubernetes Operator includes the HyperShift Operator, releases of hosted control planes align with releases of multicluster engine Operator. For more information, see OpenShift Operator Life Cycles.

## Management cluster support

Any supported standalone {product-title} cluster can be a management cluster.


[NOTE]
----
A single-node {product-title} cluster is not supported as a management cluster. If you have resource constraints, you can share infrastructure between a standalone {product-title} control plane and hosted control planes. For more information, see "Shared infrastructure between hosted and standalone control planes".
----

The following table maps multicluster engine Operator versions to the management cluster versions that support them:



## Hosted cluster support

For hosted clusters, no direct relationship exists between the management cluster version and the hosted cluster version. The hosted cluster version depends on the HyperShift Operator that is included with your multicluster engine Operator version.


[NOTE]
----
Ensure a maximum latency of 200 ms between the management cluster and hosted clusters. This requirement is especially important for mixed infrastructure deployments, such as when your management cluster is on AWS and your compute nodes are on-premise.
----

The following table shows the hosted cluster versions that you can create by using the HyperShift Operator that is associated with a version of multicluster engine Operator:


[NOTE]
----
Although the HyperShift Operator supports the hosted cluster versions in the following table, multicluster engine Operator supports only as far back as 2 versions earlier than the current version. For example, if the current hosted cluster version is 4.19, multicluster engine Operator supports as far back as version 4.17. If you want to use a hosted cluster version that is earlier than one of the versions that multicluster engine Operator supports, you can detach your hosted clusters from multicluster engine Operator to be unmanaged, or you can use an earlier version of multicluster engine Operator. For instructions to detach your hosted clusters from multicluster engine Operator, see Removing a cluster from management (RHACM documentation). For more information about multicluster engine Operator support, see The multicluster engine for Kubernetes operator 2.9 Support Matrix (Red Hat Knowledgebase).
----



## Hosted cluster platform support

A hosted cluster supports only one infrastructure platform. For example, you cannot create multiple node pools on different infrastructure platforms.

The following table indicates which {product-title} versions are supported for each platform of hosted control planes.


[IMPORTANT]
----
For IBM Power and IBM Z, you must run the control plane on machine types based on 64-bit x86 architecture, and node pools on IBM Power or IBM Z.
----

In the following table, the management cluster version is the {product-title} version where the multicluster engine Operator is enabled:



## Multi-architecture support

The following tables indicate the support status for hosted control planes on multiple architectures, organized by platform.













## Updates of multicluster engine Operator

When you update to another version of the multicluster engine Operator, your hosted cluster can continue to run if the HyperShift Operator that is included in the version of multicluster engine Operator supports the hosted cluster version. The following table shows which hosted cluster versions are supported on which updated multicluster engine Operator versions.


[NOTE]
----
Although the HyperShift Operator supports the hosted cluster versions in the following table, multicluster engine Operator supports only as far back as 2 versions earlier than the current version. For example, if the current hosted cluster version is 4.19, multicluster engine Operator supports as far back as version 4.17. If you want to use a hosted cluster version that is earlier than one of the versions that multicluster engine Operator supports, you can detach your hosted clusters from multicluster engine Operator to be unmanaged, or you can use an earlier version of multicluster engine Operator. For instructions to detach your hosted clusters from multicluster engine Operator, see Removing a cluster from management (RHACM documentation). For more information about multicluster engine Operator support, see The multicluster engine for Kubernetes operator 2.9 Support Matrix (Red Hat Knowledgebase).
----



For example, if you have an {product-title} 4.14 hosted cluster on the management cluster and you update from multicluster engine Operator 2.4 to 2.5, the hosted cluster can continue to run.

## Technology Preview features

The following list indicates features in this release that have a Technology Preview status:

* Hosted control planes on IBM Z in a disconnected environment
* Custom taints and tolerations for hosted control planes
* NVIDIA GPU devices on hosted control planes for OpenShift Virtualization
* Hosted control planes on Red Hat OpenStack Platform (RHOSP)

# FIPS-enabled hosted clusters

The binaries for hosted control planes are FIPs-compliant, with the exception of the hosted control planes command-line interface, hcp.

If you want to deploy a FIPS-enabled hosted cluster, you must use a FIPS-enabled management cluster. To enable FIPS mode for your management cluster, you must run the installation program from a Red Hat Enterprise Linux (RHEL) computer configured to operate in FIPS mode. For more information about configuring FIPS mode on RHEL, see Switching RHEL to FIPS mode.

When running RHEL or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, {product-title} core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.

After you set up your management cluster in FIPS mode, the hosted cluster creation process runs on that management cluster.

* The multicluster engine for Kubernetes Operator 2.9 support matrix
* Red Hat {product-title} Operator Update Information Checker
* Shared infrastructure between hosted and standalone control planes

# CIDR ranges for hosted control planes

For deploying hosted control planes on {product-title}, use the following required Classless Inter-Domain Routing (CIDR) subnet ranges:

* v4InternalSubnet: 100.65.0.0/16 (OVN-Kubernetes)
* clusterNetwork: 10.132.0.0/14 (pod network)
* serviceNetwork: 172.31.0.0/16

For more information about {product-title} CIDR range definitions, see "CIDR range definitions".

* CIDR range definitions