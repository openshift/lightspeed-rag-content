# Preparing to install a cluster on IBM Z and IBM LinuxONE using user-provisioned infrastructure


You prepare to install an {product-title} cluster on IBM Z(R) and IBM(R) LinuxONE by completing the following steps:
* Verifying internet connectivity for your cluster.
* Downloading the installation program.

[NOTE]
----
If you are installing in a disconnected environment, you extract the installation program from the mirrored content. For more information, see Mirroring images for a disconnected installation.
----
* Installing the OpenShift CLI (`oc`).

[NOTE]
----
If you are installing in a disconnected environment, install oc to the mirror host.
----
* Generating an SSH key pair. You can use this key pair to authenticate into the {product-title} cluster's nodes after it is deployed.
* Validating DNS resolution.

# Internet access for {product-title}

In {product-title} {product-version}, you require access to the internet to
install
your cluster.

You must have internet access to perform the following actions:

* Access OpenShift Cluster Manager to download the installation program and perform subscription management. If the cluster has internet access and you do not disable Telemetry, that service automatically entitles your cluster.
* Access Quay.io to obtain the packages that are required to install your cluster.
* Obtain the packages that are required to perform cluster updates.

# Obtaining the installation program

Before you install {product-title}, download the installation file on
 your provisioning machine.

* You have a machine that runs Linux, for example Red Hat Enterprise Linux 8, with 500 MB of local disk space.

1. Go to the Cluster Type page on the Red Hat Hybrid Cloud Console. If you have a Red Hat account, log in with your credentials. If you do not, create an account.

[TIP]
----
You can also download the binaries for a specific {product-title} release.
----
2. Select your infrastructure provider from the Run it yourself section of the page.
3. Select your host operating system and architecture from the dropdown menus under OpenShift Installer and click Download Installer.
4. Place the downloaded file in the directory where you want to store the installation configuration files.

[IMPORTANT]
----
* The installation program creates several files on the computer that you use to install your cluster. You must keep the installation program and the files that the installation program creates after you finish installing the cluster. Both of the files are required to delete the cluster.
* Deleting the files created by the installation program does not remove your cluster, even if the cluster failed during installation. To remove your cluster, complete the {product-title} uninstallation procedures for your specific cloud provider.
----
5. Extract the installation program. For example, on a computer that uses a Linux operating system, run the following command:

```terminal
$ tar -xvf openshift-install-linux.tar.gz
```

6. Download your installation pull secret from Red Hat OpenShift Cluster Manager. This pull secret allows you to authenticate with the services that are provided by the included authorities, including Quay.io, which serves the container images for {product-title} components.


[TIP]
----
Alternatively, you can retrieve the installation program from the Red Hat Customer Portal, where you can specify a version of the installation program to download.
However, you must have an active subscription to access this page.
----

# Installing the OpenShift CLI

You can install the OpenShift CLI (`oc`) to interact with {product-title} from a command-line interface. You can install oc on Linux, Windows, or macOS.


[IMPORTANT]
----
If you installed an earlier version of oc, you cannot use it to complete all of the commands in {product-title}.
Download and install the new version of oc.
----

## Installing the OpenShift CLI on Linux

You can install the OpenShift CLI (oc) binary on Linux by using the following procedure.

1. Navigate to the Download {product-title} page on the Red Hat Customer Portal.
2. Select the architecture from the Product Variant list.
3. Select the appropriate version from the Version list.
4. Click Download Now next to the OpenShift v{product-version} Linux Clients entry and save the file.
5. Unpack the archive:

```terminal
$ tar xvf <file>
```

6. Place the oc binary in a directory that is on your PATH.

To check your PATH, execute the following command:

```terminal
$ echo $PATH
```


* After you install the OpenShift CLI, it is available using the oc command:

```terminal
$ oc <command>
```


## Installing the OpenShift CLI on Windows

You can install the OpenShift CLI (oc) binary on Windows by using the following procedure.

1. Navigate to the Download {product-title} page on the Red Hat Customer Portal.
2. Select the appropriate version from the Version list.
3. Click Download Now next to the OpenShift v{product-version} Windows Client entry and save the file.
4. Unzip the archive with a ZIP program.
5. Move the oc binary to a directory that is on your PATH.

To check your PATH, open the command prompt and execute the following command:

```terminal
C:\> path
```


* After you install the OpenShift CLI, it is available using the oc command:

```terminal
C:\> oc <command>
```


## Installing the OpenShift CLI on macOS

You can install the OpenShift CLI (oc) binary on macOS by using the following procedure.

1. Navigate to the Download {product-title} page on the Red Hat Customer Portal.
2. Select the appropriate version from the Version drop-down list.
3. Click Download Now next to the OpenShift v{product-version} macOS Clients entry and save the file.

[NOTE]
----
For macOS arm64, choose the OpenShift v{product-version} macOS arm64 Client entry.
----
4. Unpack and unzip the archive.
5. Move the oc binary to a directory on your PATH.

To check your PATH, open a terminal and execute the following command:

```terminal
$ echo $PATH
```


* Verify your installation by using an oc command:

```terminal
$ oc <command>
```


# Generating a key pair for cluster node SSH access

During an {product-title} installation, you can provide an SSH public key to the installation program. The key is passed to the Red Hat Enterprise Linux CoreOS (RHCOS) nodes through their Ignition config files and is used to authenticate SSH access to the nodes. The key is added to the ~/.ssh/authorized_keys list for the core user on each node, which enables password-less authentication.

After the key is passed to the nodes, you can use the key pair to SSH in to the RHCOS nodes as the user core. To access the nodes through SSH, the private key identity must be managed by SSH for your local user.

If you want to SSH in to your cluster nodes to perform installation debugging or disaster recovery, you must provide the SSH public key during the installation process. The ./openshift-install gather command also requires the SSH public key to be in place on the cluster nodes.


[IMPORTANT]
----
Do not skip this procedure in production environments, where disaster recovery and debugging is required.
----

1. If you do not have an existing SSH key pair on your local machine to use for authentication onto your cluster nodes, create one. For example, on a computer that uses a Linux operating system, run the following command:

```terminal
$ ssh-keygen -t ed25519 -N '' -f <path>/<file_name> 1
```

Specify the path and file name, such as ~/.ssh/id_ed25519, of the new SSH key. If you have an existing key pair, ensure your public key is in the your ~/.ssh directory.

[NOTE]
----
If you plan to install an {product-title} cluster that uses the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures, do not create a key that uses the ed25519 algorithm. Instead, create a key that uses the rsa or ecdsa algorithm.
----
2. View the public SSH key:

```terminal
$ cat <path>/<file_name>.pub
```


For example, run the following to view the ~/.ssh/id_ed25519.pub public key:

```terminal
$ cat ~/.ssh/id_ed25519.pub
```

3. Add the SSH private key identity to the SSH agent for your local user, if it has not already been added. SSH agent management of the key is required for password-less SSH authentication onto your cluster nodes, or if you want to use the ./openshift-install gather command.

[NOTE]
----
On some distributions, default SSH private key identities such as ~/.ssh/id_rsa and ~/.ssh/id_dsa are managed automatically.
----
1. If the ssh-agent process is not already running for your local user, start it as a background task:

```terminal
$ eval "$(ssh-agent -s)"
```

Example output

```terminal
Agent pid 31874
```


[NOTE]
----
If your cluster is in FIPS mode, only use FIPS-compliant algorithms to generate the SSH key. The key must be either RSA or ECDSA.
----
4. Add your SSH private key to the ssh-agent:

```terminal
$ ssh-add <path>/<file_name> 1
```

Specify the path and file name for your SSH private key, such as ~/.ssh/id_ed25519
Example output

```terminal
Identity added: /home/<you>/<path>/<file_name> (<computer_name>)
```


* When you install {product-title}, provide the SSH public key to the installation program.

# Validating DNS resolution for user-provisioned infrastructure

You can validate your DNS configuration before installing {product-title} on user-provisioned infrastructure.


[IMPORTANT]
----
The validation steps detailed in this section must succeed before you install your cluster.
----

* You have configured the required DNS records for your user-provisioned infrastructure.

1. From your installation node, run DNS lookups against the record names of the Kubernetes API, the wildcard routes, and the cluster nodes. Validate that the IP addresses contained in the responses correspond to the correct components.
1. Perform a lookup against the Kubernetes API record name. Check that the result points to the IP address of the API load balancer:

```terminal
$ dig +noall +answer @<nameserver_ip> api.<cluster_name>.<base_domain> 1
```

Replace <nameserver_ip> with the IP address of the nameserver, <cluster_name> with your cluster name, and <base_domain> with your base domain name.
Example output

```terminal
api.ocp4.example.com.		604800	IN	A	192.168.1.5
```

2. Perform a lookup against the Kubernetes internal API record name. Check that the result points to the IP address of the API load balancer:

```terminal
$ dig +noall +answer @<nameserver_ip> api-int.<cluster_name>.<base_domain>
```

Example output

```terminal
api-int.ocp4.example.com.		604800	IN	A	192.168.1.5
```

3. Test an example *.apps.<cluster_name>.<base_domain> DNS wildcard lookup. All of the application wildcard lookups must resolve to the IP address of the application ingress load balancer:

```terminal
$ dig +noall +answer @<nameserver_ip> random.apps.<cluster_name>.<base_domain>
```

Example output

```terminal
random.apps.ocp4.example.com.		604800	IN	A	192.168.1.5
```


[NOTE]
----
In the example outputs, the same load balancer is used for the Kubernetes API and application ingress traffic. In production scenarios, you can deploy the API and application ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.
----

You can replace random with another wildcard value. For example, you can query the route to the {product-title} console:

```terminal
$ dig +noall +answer @<nameserver_ip> console-openshift-console.apps.<cluster_name>.<base_domain>
```

Example output

```terminal
console-openshift-console.apps.ocp4.example.com. 604800 IN	A 192.168.1.5
```

4. Run a lookup against the bootstrap DNS record name. Check that the result points to the IP address of the bootstrap node:

```terminal
$ dig +noall +answer @<nameserver_ip> bootstrap.<cluster_name>.<base_domain>
```

Example output

```terminal
bootstrap.ocp4.example.com.		604800	IN	A	192.168.1.96
```

5. Use this method to perform lookups against the DNS record names for the control plane and compute nodes. Check that the results correspond to the IP addresses of each node.
2. From your installation node, run reverse DNS lookups against the IP addresses of the load balancer and the cluster nodes. Validate that the record names contained in the responses correspond to the correct components.
1. Perform a reverse lookup against the IP address of the API load balancer. Check that the response includes the record names for the Kubernetes API and the Kubernetes internal API:

```terminal
$ dig +noall +answer @<nameserver_ip> -x 192.168.1.5
```

Example output

```terminal
5.1.168.192.in-addr.arpa. 604800	IN	PTR	api-int.ocp4.example.com. 1
5.1.168.192.in-addr.arpa. 604800	IN	PTR	api.ocp4.example.com. 2
```

Provides the record name for the Kubernetes internal API.
Provides the record name for the Kubernetes API.

[NOTE]
----
A PTR record is not required for the {product-title} application wildcard. No validation step is needed for reverse DNS resolution against the IP address of the application ingress load balancer.
----
2. Perform a reverse lookup against the IP address of the bootstrap node. Check that the result points to the DNS record name of the bootstrap node:

```terminal
$ dig +noall +answer @<nameserver_ip> -x 192.168.1.96
```

Example output

```terminal
96.1.168.192.in-addr.arpa. 604800	IN	PTR	bootstrap.ocp4.example.com.
```

3. Use this method to perform reverse lookups against the IP addresses for the control plane and compute nodes. Check that the results correspond to the DNS record names of each node.

* See About remote health monitoring for more information about the Telemetry service.