Scheduling NUMA-aware workloads

Learn about NUMA-aware scheduling and how you can use it to deploy high performance workloads in an Red Hat OpenShift Container Platform cluster.

The NUMA Resources Operator allows you to schedule high-performance workloads in the same NUMA zone. It deploys a node resources exporting agent that reports on available cluster node NUMA resources, and a secondary scheduler that manages the workloads.
About NUMA-aware scheduling

Non-Uniform Memory Access (NUMA) is a compute platform architecture that allows different CPUs to access different regions of memory at different speeds. NUMA resource topology refers to the locations of CPUs, memory, and PCI devices relative to each other in the compute node. Colocated resources are said to be in the same NUMA zone. For high-performance applications, the cluster needs to process pod workloads in a single NUMA zone.


NUMA architecture allows a CPU with multiple memory controllers to use any available memory across CPU complexes, regardless of where the memory is located. This allows for increased flexibility at the expense of performance. A CPU processing a workload using memory that is outside its NUMA zone is slower than a workload processed in a single NUMA zone. Also, for I/O-constrained workloads, the network interface on a distant NUMA zone slows down how quickly information can reach the application. High-performance workloads, such as telecommunications workloads, cannot operate to specification under these conditions.


NUMA-aware scheduling aligns the requested cluster compute resources (CPUs, memory, devices) in the same NUMA zone to process latency-sensitive or high-performance workloads efficiently. NUMA-aware scheduling also improves pod density per compute node for greater resource efficiency.


By integrating the Node Tuning Operator's performance profile with NUMA-aware scheduling, you can further configure CPU affinity to optimize performance for latency-sensitive workloads.


The default Red Hat OpenShift Container Platform pod scheduler scheduling logic considers the available resources of the entire compute node, not individual NUMA zones. If the most restrictive resource alignment is requested in the kubelet topology manager, error conditions can occur when admitting the pod to a node. Conversely, if the most restrictive resource alignment is not requested, the pod can be admitted to the node without proper resource alignment, leading to worse or unpredictable performance. For example, runaway pod creation with Topology Affinity Error statuses can occur when the pod scheduler makes suboptimal scheduling decisions for guaranteed pod workloads without knowing if the pod's requested resources are available. Scheduling mismatch decisions can cause indefinite pod startup delays. Also, depending on the cluster state and resource allocation, poor pod scheduling decisions can cause extra load on the cluster because of failed startup attempts.


The NUMA Resources Operator deploys a custom NUMA resources secondary scheduler and other resources to mitigate against the shortcomings of the default Red Hat OpenShift Container Platform pod scheduler. The following diagram provides a high-level overview of NUMA-aware pod scheduling.



NodeResourceTopology API
The NodeResourceTopology API describes the available NUMA zone resources in each compute node.
NUMA-aware scheduler
The NUMA-aware secondary scheduler receives information about the available NUMA zones from the NodeResourceTopology API and schedules high-performance workloads on a node where it can be optimally processed.
Node topology exporter
The node topology exporter exposes the available NUMA zone resources for each compute node to the NodeResourceTopology API. The node topology exporter daemon tracks the resource allocation from the kubelet by using the PodResources API.
PodResources API
The PodResources API is local to each node and exposes the resource topology and available resources to the kubelet.


For more information about running secondary pod schedulers in your cluster and how to deploy pods with a secondary pod scheduler, see Scheduling pods using a secondary scheduler.
Installing the NUMA Resources Operator
NUMA Resources Operator deploys resources that allow you to schedule NUMA-aware workloads and deployments. You can install the NUMA Resources Operator using the Red Hat OpenShift Container Platform CLI or the web console.

Installing the NUMA Resources Operator using the CLI
As a cluster administrator, you can install the Operator using the CLI.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Create a namespace for the NUMA Resources Operator:

Create the Operator group for the NUMA Resources Operator:

Create the subscription for the NUMA Resources Operator:


Verify that the installation succeeded by inspecting the CSV resource in the openshift-numaresources namespace. Run the following command:
Installing the NUMA Resources Operator using the web console
As a cluster administrator, you can install the NUMA Resources Operator using the web console.

Create a namespace for the NUMA Resources Operator:

Install the NUMA Resources Operator:

Optional: Verify that the NUMA Resources Operator installed successfully:
Scheduling NUMA-aware workloads
Clusters running latency-sensitive workloads typically feature performance profiles that help to minimize workload latency and optimize performance. The NUMA-aware scheduler deploys workloads based on available node NUMA resources and with respect to any performance profile settings applied to the node. The combination of NUMA-aware deployments, and the performance profile of the workload, ensures that workloads are scheduled in a way that maximizes performance.

For the NUMA Resources Operator to be fully operational, you must deploy the NUMAResourcesOperator custom resource and the NUMA-aware secondary pod scheduler.

Creating the NUMAResourcesOperator custom resource
When you have installed the NUMA Resources Operator, then create the NUMAResourcesOperator custom resource (CR) that instructs the NUMA Resources Operator to install all the cluster infrastructure needed to support the NUMA-aware scheduler, including daemon sets and APIs.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.

Install the NUMA Resources Operator.


Create the NUMAResourcesOperator custom resource:


Verify that the NUMA Resources Operator deployed successfully by running the following command:

After a few minutes, run the following command to verify that the required resources deployed successfully:
Deploying the NUMA-aware secondary pod scheduler
After you install the NUMA Resources Operator, do the following to deploy the NUMA-aware secondary pod scheduler:

Create the NUMAResourcesScheduler custom resource that deploys the NUMA-aware custom pod scheduler:

After a few seconds, run the following command to confirm the successful deployment of the required resources:
Configuring a single NUMA node policy
The NUMA Resources Operator requires a single NUMA node policy to be configured on the cluster. This can be achieved in two ways: by creating and applying a performance profile, or by configuring a KubeletConfig.

The preferred way to configure a single NUMA node policy is to apply a performance profile. You can use the Performance Profile Creator (PPC) tool to create the performance profile. If a performance profile is created on the cluster, it automatically creates other tuning components like KubeletConfig and the tuned profile.
For more information about creating a performance profile, see "About the Performance Profile Creator" in the "Additional Resources" section.

About the Performance Profile Creator
Sample performance profile
This example YAML shows a performance profile created by using the performance profile creator (PPC) tool:

apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: "3"
    reserved: 0-2
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/worker: "" 1
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  numa:
    topologyPolicy: single-numa-node 2
  realTimeKernel:
    enabled: true
  workloadHints:
    highPowerConsumption: true
    perPodPowerManagement: false
    realTime: true
This should match the MachineConfigPool that you want to configure the NUMA Resources Operator on. For example, you might have created a MachineConfigPool named worker-cnf that designates a set of nodes that run telecommunications workloads.

The topologyPolicy must be set to single-numa-node. Ensure that this is the case by setting the topology-manager-policy argument to single-numa-node when running the PPC tool.
Creating a KubeletConfig CRD
The recommended way to configure a single NUMA node policy is to apply a performance profile. Another way is by creating and applying a KubeletConfig custom resource (CR), as shown in the following procedure.

Create the KubeletConfig custom resource (CR) that configures the pod admittance policy for the machine profile:
Scheduling workloads with the NUMA-aware scheduler
Now that topo-aware-scheduler is installed, the NUMAResourcesOperator and NUMAResourcesScheduler CRs are applied and your cluster has a matching performance profile or kubeletconfig, you can schedule workloads with the NUMA-aware scheduler using deployment CRs that specify the minimum required resources to process the workload.

The following example deployment uses NUMA-aware scheduling for a sample workload.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Get the name of the NUMA-aware scheduler that is deployed in the cluster by running the following command:

Create a Deployment CR that uses scheduler named topo-aware-scheduler, for example:


Verify that the deployment was successful:

Verify that the topo-aware-scheduler is scheduling the deployed pod by running the following command:

Verify that the expected allocated resources are listed for the node.

Resource allocations for pods with a Best-effort or Burstable quality of service (qosClass) are not reflected in the NUMA node resources under noderesourcetopologies.topology.node.k8s.io. If a pod's consumed resources are not reflected in the node resource calculation, verify that the pod has qosClass of Guaranteed and the CPU request is an integer value, not a decimal value. You can verify the that the pod has a  qosClass of Guaranteed by running the following command:
Optional: Configuring polling operations for NUMA resources updates
The daemons controlled by the NUMA Resources Operator in their nodeGroup poll resources to retrieve updates about available NUMA resources. You can fine-tune polling operations for these daemons by configuring the spec.nodeGroups specification in the NUMAResourcesOperator custom resource (CR). This provides advanced control of polling operations. Configure these specifications to improve scheduling behaviour and troubleshoot suboptimal scheduling decisions.

The configuration options are the following:

infoRefreshMode: Determines the trigger condition for polling the kubelet. The NUMA Resources Operator reports the resulting information to the API server.

infoRefreshPeriod: Determines the duration between polling updates.

podsFingerprinting: Determines if point-in-time information for the current set of pods running on a node is exposed in polling updates.


Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.

Install the NUMA Resources Operator.


Configure the spec.nodeGroups specification in your NUMAResourcesOperator CR:


After you deploy the NUMA Resources Operator, verify that the node group configurations were applied by running the following command:
Troubleshooting NUMA-aware scheduling
To troubleshoot common problems with NUMA-aware pod scheduling, perform the following steps.

Install the Red Hat OpenShift Container Platform CLI (oc).

Log in as a user with cluster-admin privileges.

Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.


Verify that the noderesourcetopologies CRD is deployed in the cluster by running the following command:

Check that the NUMA-aware scheduler name matches the name specified in your NUMA-aware workloads by running the following command:

Verify that NUMA-aware scheduable nodes have the noderesourcetopologies CR applied to them. Run the following command:

Verify the NUMA zone granularity for all scheduable nodes by running the following command:


Reporting more exact resource availability
Enable the cacheResyncPeriod specification to help the NUMA Resources Operator report more exact resource availability by monitoring pending resources on nodes and synchronizing this information in the scheduler cache at a defined interval. This also helps to minimize Topology Affinity Error errors because of sub-optimal scheduling decisions. The lower the interval, the greater the network load. The cacheResyncPeriod specification is disabled by default.

Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Delete the currently running NUMAResourcesScheduler resource:

Save the following YAML in the file nro-scheduler-cacheresync.yaml. This example changes the log level to Debug:

Create the updated NUMAResourcesScheduler resource by running the following command:


Check that the NUMA-aware scheduler was successfully deployed:

Check that the logs for the scheduler show the increased log level:
Checking the NUMA-aware scheduler logs
Troubleshoot problems with the NUMA-aware scheduler by reviewing the logs. If required, you can increase the scheduler log level by modifying the spec.logLevel field of the NUMAResourcesScheduler resource. Acceptable values are Normal, Debug, and Trace, with Trace being the most verbose option.

To change the log level of the secondary scheduler, delete the running scheduler resource and re-deploy it with the changed log level. The scheduler is unavailable for scheduling new workloads during this downtime.
Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Delete the currently running NUMAResourcesScheduler resource:

Save the following YAML in the file nro-scheduler-debug.yaml. This example changes the log level to Debug:

Create the updated Debug logging NUMAResourcesScheduler resource by running the following command:


Check that the NUMA-aware scheduler was successfully deployed:

Check that the logs for the scheduler shows the increased log level:
Troubleshooting the resource topology exporter
Troubleshoot noderesourcetopologies objects where unexpected results are occurring by inspecting the corresponding resource-topology-exporter logs.

It is recommended that NUMA resource topology exporter instances in the cluster are named for nodes they refer to. For example, a worker node with the name worker should have a corresponding noderesourcetopologies object called worker.
Install the OpenShift CLI (oc).

Log in as a user with cluster-admin privileges.


Get the daemonsets managed by the NUMA Resources Operator. Each daemonset has a corresponding nodeGroup in the NUMAResourcesOperator CR. Run the following command:

Get the label for the daemonset of interest using the value for name from the previous step:

Get the pods using the resource-topology label by running the following command:

Examine the logs of the resource-topology-exporter container running on the worker pod that corresponds to the node you are troubleshooting. Run the following command:
Correcting a missing resource topology exporter config map
If you install the NUMA Resources Operator in a cluster with misconfigured cluster settings, in some circumstances, the Operator is shown as active but the logs of the resource topology exporter (RTE) daemon set pods show that the configuration for the RTE is missing, for example:

Info: couldn't find configuration in "/etc/resource-topology-exporter/config.yaml"
This log message indicates that the kubeletconfig with the required configuration was not properly applied in the cluster, resulting in a missing RTE configmap. For example, the following cluster is missing a numaresourcesoperator-worker configmap custom resource (CR):

$ oc get configmap
NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h
In a correctly configured cluster, oc get configmap also returns a numaresourcesoperator-worker configmap CR.

Install the Red Hat OpenShift Container Platform CLI (oc).

Log in as a user with cluster-admin privileges.

Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.


Compare the values for spec.machineConfigPoolSelector.matchLabels in kubeletconfig and
metadata.labels in the MachineConfigPool (mcp) worker CR using the following commands:

Edit the MachineConfigPool CR to include the missing label, for example:

Apply the label changes and wait for the cluster to apply the updated configuration. Run the following command:


Check that the missing numaresourcesoperator-worker configmap CR is applied:
Collecting NUMA Resources Operator data
You can use the oc adm must-gather CLI command to collect information about your cluster, including features and objects associated with the NUMA Resources Operator.

You have access to the cluster as a user with the cluster-admin role.

You have installed the OpenShift CLI (`oc`).


To collect NUMA Resources Operator data with must-gather, you must specify the NUMA Resources Operator must-gather image.