Replacing an unhealthy etcd member

This document describes the process to replace a single unhealthy etcd member.

This process depends on whether the etcd member is unhealthy because the machine is not running or the node is not ready, or whether it is unhealthy because the etcd pod is crashlooping.

If you have lost the majority of your control plane hosts, follow the disaster recovery procedure to restore to a previous cluster state instead of this procedure.

If the control plane certificates are not valid on the member being replaced, then you must follow the procedure to recover from expired control plane certificates instead of this procedure.

If a control plane node is lost and a new one is created, the etcd cluster Operator handles generating the new TLS certificates and adding the node as an etcd member.
Prerequisites
Take an etcd backup prior to replacing an unhealthy etcd member.
Identifying an unhealthy etcd member
You can identify if your cluster has an unhealthy etcd member.

Access to the cluster as a user with the cluster-admin role.


Check the status of the EtcdMembersAvailable status condition using the following command:

Review the output:
Determining the state of the unhealthy etcd member
The steps to replace an unhealthy etcd member depend on which of the following states your etcd member is in:

The machine is not running or the node is not ready

The etcd pod is crashlooping


This procedure determines which state your etcd member is in. This enables you to know which procedure to follow to replace the unhealthy etcd member.

If you are aware that the machine is not running or the node is not ready, but you expect it to return to a healthy state soon, then you do not need to perform a procedure to replace the etcd member. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.
You have access to the cluster as a user with the cluster-admin role.

You have identified an unhealthy etcd member.


Determine if the machine is not running:

Determine if the node is not ready.

Determine if the etcd pod is crashlooping.
Replacing the unhealthy etcd member
Depending on the state of your unhealthy etcd member, use one of the following procedures:

Replacing an unhealthy etcd member whose machine is not running or whose node is not ready

Replacing an unhealthy etcd member whose etcd pod is crashlooping

Replacing an unhealthy stopped baremetal etcd member


Replacing an unhealthy etcd member whose machine is not running or whose node is not ready
This procedure details the steps to replace an etcd member that is unhealthy either because the machine is not running or because the node is not ready.

If your cluster uses a control plane machine set, see "Recovering a degraded etcd Operator" in "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure.
You have identified the unhealthy etcd member.

You have verified that either the machine is not running or the node is not ready.

You have access to the cluster as a user with the cluster-admin role.

You have taken an etcd backup.


Remove the unhealthy member.

Turn off the quorum guard by entering the following command:

Delete the affected node by running the following command:

Remove the old secrets for the unhealthy etcd member that was removed.

Delete and re-create the control plane machine. After this machine is re-created, a new revision is forced and etcd scales up automatically.

Turn the quorum guard back on by entering the following command:

You can verify that the unsupportedConfigOverrides section is removed from the object by entering this command:

If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:


Verify that all etcd pods are running properly.

Verify that there are exactly three etcd members.


Recovering a degraded etcd Operator
Replacing an unhealthy etcd member whose etcd pod is crashlooping
This procedure details the steps to replace an etcd member that is unhealthy because the etcd pod is crashlooping.

You have identified the unhealthy etcd member.

You have verified that the etcd pod is crashlooping.

You have access to the cluster as a user with the cluster-admin role.

You have taken an etcd backup.


Stop the crashlooping etcd pod.

Remove the unhealthy member.

Turn off the quorum guard by entering the following command:

Remove the old secrets for the unhealthy etcd member that was removed.

Force etcd redeployment.

Turn the quorum guard back on by entering the following command:

You can verify that the unsupportedConfigOverrides section is removed from the object by entering this command:

If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:


Verify that the new member is available and healthy.
Replacing an unhealthy bare metal etcd member whose machine is not running or whose node is not ready
This procedure details the steps to replace a bare metal etcd member that is unhealthy either because the machine is not running or because the node is not ready.

If you are running installer-provisioned infrastructure or you used the Machine API to create your machines, follow these steps. Otherwise you must create the new control plane node using the same method that was used to originally create it.

You have identified the unhealthy bare metal etcd member.

You have verified that either the machine is not running or the node is not ready.

You have access to the cluster as a user with the cluster-admin role.

You have taken an etcd backup.


Verify and remove the unhealthy member.

Turn off the quorum guard by entering the following command:

Remove the old secrets for the unhealthy etcd member that was removed by running the following commands.

Delete the control plane machine.

Change the metadata.name field to a new name.

Ensure that the Bare Metal Operator is available by running the following command:

Remove the old BareMetalHost object by running the following command:

Delete the machine of the unhealthy member by running the following command:

Verify that the machine was deleted by running the following command:

Verify that the node has been deleted by running the following command:

Create the new BareMetalHost object and the secret to store the BMC credentials:

Verify the creation process using available BareMetalHost objects:

Turn the quorum guard back on by entering the following command:

You can verify that the unsupportedConfigOverrides section is removed from the object by entering this command:

If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:


Verify that all etcd pods are running properly.

View the member list:

Verify that all etcd members are healthy by running the following command:

Validate that all nodes are at the latest revision by running the following command:
Additional resources
Quorum protection with machine lifecycle hooks