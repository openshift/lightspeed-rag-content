# Restoring to a previous cluster state


To restore the cluster to a previous state, you must have previously backed up the etcd data by creating a snapshot. You will use this snapshot to restore the cluster state. For more information, see "Backing up etcd data".

# About restoring cluster state

You can use an etcd backup to restore your cluster to a previous state. This can be used to recover from the following situations:

* The cluster has lost the majority of control plane hosts (quorum loss).
* An administrator has deleted something critical and must restore to recover the cluster.


[WARNING]
----
Restoring to a previous cluster state is a destructive and destablizing action to take on a running cluster. This should only be used as a last resort.
If you are able to retrieve data using the Kubernetes API server, then etcd is available and you should not restore using an etcd backup.
----

Restoring etcd effectively takes a cluster back in time and all clients will experience a conflicting, parallel history. This can impact the behavior of watching components like kubelets, Kubernetes controller managers, SDN controllers, and persistent volume controllers.

It can cause Operator churn when the content in etcd does not match the actual content on disk, causing Operators for the Kubernetes API server, Kubernetes controller manager, Kubernetes scheduler, and etcd to get stuck when files on disk conflict with content in etcd. This can require manual actions to resolve the issues.

In extreme cases, the cluster can lose track of persistent volumes, delete critical workloads that no longer exist, reimage machines, and rewrite CA bundles with expired certificates.

# Restoring to a previous cluster state

You can use a saved etcd backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.


[NOTE]
----
If your cluster uses a control plane machine set, see "Recovering a degraded etcd Operator" in "Troubleshooting the control plane machine set" for an etcd recovery procedure.
----


[IMPORTANT]
----
When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an Red Hat OpenShift Container Platform 4.7.2 cluster must use an etcd backup that was taken from 4.7.2.
----

* Access to the cluster as a user with the cluster-admin role through a certificate-based kubeconfig file, like the one that was used during installation.
* A healthy control plane host to use as the recovery host.
* SSH access to control plane hosts.
* A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: snapshot_<datetimestamp>.db and static_kuberesources_<datetimestamp>.tar.gz.
* Nodes must be accessible or bootable.


[IMPORTANT]
----
For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and re-create other non-recovery, control plane machines, one by one.
----

1. Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.
2. Establish SSH connectivity to each of the control plane nodes, including the recovery host. Use a separate terminal to establish SSH connectivity for each control plane node.

The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes by using the oc debug method. For this reason, establish SSH connectivity to each control plane host in a separate terminal.

[IMPORTANT]
----
If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.
----
3. Copy the etcd backup directory to the recovery control plane host.

This procedure assumes that you copied the backup directory containing the etcd snapshot and the resources for the static pods to the /home/core/ directory of your recovery control plane host.
4. Stop the static pods on any other control plane nodes.

[NOTE]
----
You do not need to stop the static pods on the recovery host.
----
1. Access a control plane host that is not the recovery host.
2. Move the existing etcd pod file out of the kubelet manifest directory by running the following command:

```terminal
$ sudo mv -v /etc/kubernetes/manifests/etcd-pod.yaml /tmp
```

3. Verify that the etcd pods are stopped by running the following command:

```terminal
$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"
```


If the output of this command is not empty, wait a few minutes and check again.
4. Move the existing kube-apiserver file out of the kubelet manifest directory by running the following command:

```terminal
$ sudo mv -v /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp
```

5. Verify that the kube-apiserver containers are stopped by running the following command:

```terminal
$ sudo crictl ps | grep kube-apiserver | egrep -v "operator|guard"
```


If the output of this command is not empty, wait a few minutes and check again.
6. Move the existing kube-controller-manager file out of the kubelet manifest directory by running the following command:

```terminal
$ sudo mv -v /etc/kubernetes/manifests/kube-controller-manager-pod.yaml /tmp
```

7. Verify that the kube-controller-manager containers are stopped by running the following command:

```terminal
$ sudo crictl ps | grep kube-controller-manager | egrep -v "operator|guard"
```


If the output of this command is not empty, wait a few minutes and check again.
8. Move the existing kube-scheduler file out of the kubelet manifest directory by running the following command:

```terminal
$ sudo mv -v /etc/kubernetes/manifests/kube-scheduler-pod.yaml /tmp
```

9. Verify that the kube-scheduler containers are stopped by running the following command:

```terminal
$ sudo crictl ps | grep kube-scheduler | egrep -v "operator|guard"
```


If the output of this command is not empty, wait a few minutes and check again.
10. Move the etcd data directory to a different location with the following example:

```terminal
$ sudo mv -v /var/lib/etcd/ /tmp
```

11. If the /etc/kubernetes/manifests/keepalived.yaml file exists, complete the following steps. These steps are necessary to ensure that the API IP address is listening on the recovery host.
1. Move the /etc/kubernetes/manifests/keepalived.yaml file out of the kubelet manifest directory by running the following command:

```terminal
$ sudo mv -v /etc/kubernetes/manifests/keepalived.yaml /home/core/
```


[NOTE]
----
This file must be restored to its original location after the procedure is completed.
----
2. Verify that any containers managed by the keepalived daemon are stopped:

```terminal
$ sudo crictl ps --name keepalived
```


The output of this command should be empty. If it is not empty, wait a few minutes and check again.
3. Check if the control plane has any Virtual IPs (VIPs) assigned to it:

```terminal
$ ip -o address | egrep '<api_vip>|<ingress_vip>'
```

4. For each reported VIP, run the following command to remove it:

```terminal
$ sudo ip address del <reported_vip> dev <reported_vip_device>
```

12. Repeat this step on each of the other control plane hosts that is not the recovery host.
5. Access the recovery control plane host.
6. If the keepalived daemon is in use, verify that the recovery control plane node owns the VIP. Otherwise, repeat step 4.xi.

```terminal
$ ip -o address | grep <api_vip>
```


The address of the VIP is highlighted in the output if it exists. This command returns an empty string if the VIP is not set or configured incorrectly.
7. If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables.

[TIP]
----
You can check whether the proxy is enabled by reviewing the output of oc get proxy cluster -o yaml. The proxy is enabled if the httpProxy, httpsProxy, and noProxy fields have values set.
----
8. Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory:

```terminal
$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/assets/backup
```

Example script output

```terminal
...stopping kube-scheduler-pod.yaml
...stopping kube-controller-manager-pod.yaml
...stopping etcd-pod.yaml
...stopping kube-apiserver-pod.yaml
Waiting for container etcd to stop
.complete
Waiting for container etcdctl to stop
.............................complete
Waiting for container etcd-metrics to stop
complete
Waiting for container kube-controller-manager to stop
complete
Waiting for container kube-apiserver to stop
..........................................................................................complete
Waiting for container kube-scheduler to stop
complete
Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup
starting restore-etcd static pod
starting kube-apiserver-pod.yaml
static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml
starting kube-controller-manager-pod.yaml
static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml
starting kube-scheduler-pod.yaml
static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml
```


The cluster-restore.sh script must show that etcd, kube-apiserver, kube-controller-manager, and kube-scheduler pods are stopped and then started at the end of the restore process.

[NOTE]
----
The restore process can cause nodes to enter the NotReady state if the node certificates were updated after the last etcd backup.
----
9. Check the nodes to ensure they are in the Ready state. To check the nodes, you can use either the bastion host or the recovery host.
* If you use the recovery host, run the following commands:

```terminal
$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig
```


```terminal
$ oc get nodes -w
```

* If you use the bastion host, complete the following steps:
1. Run the following command:

```terminal
$ oc get nodes -w
```

Sample output

```terminal
NAME                STATUS  ROLES          AGE     VERSION
host-172-25-75-28   Ready   master         3d20h   v1.29.4
host-172-25-75-38   Ready   infra,worker   3d20h   v1.29.4
host-172-25-75-40   Ready   master         3d20h   v1.29.4
host-172-25-75-65   Ready   master         3d20h   v1.29.4
host-172-25-75-74   Ready   infra,worker   3d20h   v1.29.4
host-172-25-75-79   Ready   worker         3d20h   v1.29.4
host-172-25-75-86   Ready   worker         3d20h   v1.29.4
host-172-25-75-98   Ready   infra,worker   3d20h   v1.29.4
```


It can take several minutes for all nodes to report their state.
2. If any nodes are in the NotReady state, log in to the nodes and remove all of the PEM files from the /var/lib/kubelet/pki directory on each node. You can SSH into the nodes or use the terminal window in the web console.

```terminal
$  ssh -i <ssh-key-path> core@<master-hostname>
```

Sample pki directory

```terminal
sh-4.4# pwd
/var/lib/kubelet/pki
sh-4.4# ls
kubelet-client-2022-04-28-11-24-09.pem  kubelet-server-2022-04-28-11-24-15.pem
kubelet-client-current.pem              kubelet-server-current.pem
```

10. Restart the kubelet service on all control plane hosts.
1. From the recovery host, run the following command:

```terminal
$ sudo systemctl restart kubelet.service
```

2. Repeat this step on all other control plane hosts.
11. Approve the pending Certificate Signing Requests (CSRs):

[NOTE]
----
Clusters with no worker nodes, such as single-node clusters or clusters consisting of three schedulable control plane nodes, will not have any pending CSRs to approve. You can skip all the commands listed in this step.
----
1. Get the list of current CSRs by running the following command:

```terminal
$ oc get csr
```

Example output

```
NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:<node_name>                                                     Pending 1
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:<node_name>                                                     Pending 1
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending 2
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending 2
...
```

A pending kubelet serving CSR, requested by the node for the kubelet serving endpoint.
A pending kubelet client CSR, requested with the node-bootstrapper node bootstrap credentials.
2. Review the details of a CSR to verify that it is valid by running the following command:

```terminal
$ oc describe csr <csr_name> 1
```

<csr_name> is the name of a CSR from the list of current CSRs.
3. Approve each valid node-bootstrapper CSR by running the following command:

```terminal
$ oc adm certificate approve <csr_name>
```

4. For user-provisioned installations, approve each valid kubelet service CSR by running the following command:

```terminal
$ oc adm certificate approve <csr_name>
```

12. Verify that the single member control plane has started successfully.
1. From the recovery host, verify that the etcd container is running by entering the following command:

```terminal
$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"
```

Example output

```terminal
3ad41b7908e32       36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009                                                         About a minute ago   Running             etcd                                          0                   7c05f8af362f0
```

2. From the recovery host, verify that the etcd pod is running by entering the following command:

```terminal
$ oc -n openshift-etcd get pods -l k8s-app=etcd
```

Example output

```terminal
NAME                                             READY   STATUS      RESTARTS   AGE
etcd-ip-10-0-143-125.ec2.internal                1/1     Running     1          2m47s
```


If the status is Pending, or the output lists more than one running etcd pod, wait a few minutes and check again.
13. If you are using the OVNKubernetes network plugin, you must restart ovnkube-controlplane pods.
1. Delete all of the ovnkube-controlplane pods by running the following command:

```terminal
$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane
```

2. Verify that all of the ovnkube-controlplane pods were redeployed by running the following command:

```terminal
$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane
```

14. If you are using the OVN-Kubernetes network plugin, restart the Open Virtual Network (OVN) Kubernetes pods on all the nodes one by one. Use the following steps to restart OVN-Kubernetes pods on each node:

[IMPORTANT]
----
1. The recovery control plane host
2. The other control plane hosts (if available)
3. The other nodes
----

[NOTE]
----
Validating and mutating admission webhooks can reject pods. If you add any additional webhooks with the failurePolicy set to Fail, then they can reject pods and the restoration process can fail. You can avoid this by saving and deleting webhooks while restoring the cluster state. After the cluster state is restored successfully, you can enable the webhooks again.
Alternatively, you can temporarily set the failurePolicy to Ignore while restoring the cluster state. After the cluster state is restored successfully, you can set the failurePolicy to Fail.
----
1. Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run the following command:

```terminal
$ sudo rm -f /var/lib/ovn-ic/etc/*.db
```

2. Restart the OpenVSwitch services. Access the node by using Secure Shell (SSH) and run the following command:

```terminal
$ sudo systemctl restart ovs-vswitchd ovsdb-server
```

3. Delete the ovnkube-node pod on the node by running the following command, replacing <node> with the name of the node that you are restarting:

```terminal
$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-node --field-selector=spec.nodeName==<node>
```

4. Check the status of the OVN pods by running the following command:

```terminal
$ oc get po -n openshift-ovn-kubernetes
```

1. If any OVN pods are in the Terminating status, delete the node that is running that OVN pod by running the following command. Replace <node> with the name of the node you are deleting:

```terminal
$ oc delete node <node>
```

2. Use SSH to log in to the OVN pod node with the Terminating status by running the following command:

```terminal
$ ssh -i <ssh-key-path> core@<node>
```

3. Move all PEM files from the /var/lib/kubelet/pki directory by running the following command:

```terminal
$ sudo mv /var/lib/kubelet/pki/* /tmp
```

4. Restart the kubelet service by running the following command:

```terminal
$ sudo systemctl restart kubelet.service
```

5. Return to the recovery etcd machines by running the following command:

```terminal
$ oc get csr
```

Example output

```terminal
NAME        AGE    SIGNERNAME                         REQUESTOR                     CONDITION
csr-<uuid>   8m3s   kubernetes.io/kubelet-serving     system:node:<node_name>       Pending
```

6. Approve all new CSRs by running the following command, replacing csr-<uuid> with the name of the CSR:

```terminal
oc adm certificate approve csr-<uuid>
```

7. Verify that the node is back by running the following command:

```terminal
$ oc get nodes
```

5. Verify that the ovnkube-node pod is running again with:

```terminal
$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-node --field-selector=spec.nodeName==<node>
```


[NOTE]
----
It might take several minutes for the pods to restart.
----
15. Delete and re-create other non-recovery, control plane machines, one by one. After the machines are re-created, a new revision is forced and etcd automatically scales up.
* If you use a user-provisioned bare metal installation, you can re-create a control plane machine by using the same method that you used to originally create it. For more information, see "Installing a user-provisioned cluster on bare metal".

[WARNING]
----
Do not delete and re-create the machine for the recovery host.
----
* If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps:

[WARNING]
----
Do not delete and re-create the machine for the recovery host.
For bare metal installations on installer-provisioned infrastructure, control plane machines are not re-created. For more information, see "Replacing a bare-metal control plane node".
----
1. Obtain the machine for one of the lost control plane hosts.

In a terminal that has access to the cluster as a cluster-admin user, run the following command:

```terminal
$ oc get machines -n openshift-machine-api -o wide
```

Example output

```terminal
NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped 1
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running
```

This is the control plane machine for the lost control plane host, ip-10-0-131-183.ec2.internal.
2. Delete the machine of the lost control plane host by running:

```terminal
$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 1
```

Specify the name of the control plane machine for the lost control plane host.

A new machine is automatically provisioned after deleting the machine of the lost control plane host.
3. Verify that a new machine has been created by running:

```terminal
$ oc get machines -n openshift-machine-api -o wide
```

Example output

```terminal
NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-173-171.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8  running 1
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running
```

The new machine, clustername-8qw5l-master-3 is being created and is ready after the phase changes from Provisioning to Running.

It might take a few minutes for the new machine to be created. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.
4. Repeat these steps for each lost control plane host that is not the recovery host.
16. Turn off the quorum guard by running the following command:

```terminal
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'
```


This command ensures that you can successfully re-create secrets and roll out the static pods.
17. If not yet defined, in a separate terminal window within the recovery host, export the recovery kubeconfig file by running the following command:

```terminal
$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig
```

18. Force etcd redeployment.

In the same terminal window where you exported the recovery kubeconfig file, run the following command:

```terminal
$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge 1
```

The forceRedeploymentReason value must be unique, which is why a timestamp is appended.

The etcd redeployment starts.

When the etcd cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up.
19. Turn the quorum guard back on by running the following command:

```terminal
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'
```

20. You can verify that the unsupportedConfigOverrides section is removed from the object by running the following command:

```terminal
$ oc get etcd/cluster -oyaml
```

21. Verify all nodes are updated to the latest revision.

In a terminal that has access to the cluster as a cluster-admin user, run the following command:

```terminal
$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition for etcd to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 7 1
```

In this example, the latest revision number is 7.

If the output includes multiple revision numbers, such as 2 nodes are at revision 6; 1 nodes are at revision 7, this means that the update is still in progress. Wait a few minutes and try again.
22. After etcd is redeployed, force new rollouts for the control plane. kube-apiserver will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer.

In a terminal that has access to the cluster as a cluster-admin user, complete the following steps:
1. Force a new rollout for kube-apiserver:

```terminal
$ oc patch kubeapiserver cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
```


Verify all nodes are updated to the latest revision.

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 7 1
```

In this example, the latest revision number is 7.

If the output includes multiple revision numbers, such as 2 nodes are at revision 6; 1 nodes are at revision 7, this means that the update is still in progress. Wait a few minutes and try again.
2. Force a new rollout for the Kubernetes controller manager by running the following command:

```terminal
$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
```


Verify all nodes are updated to the latest revision by running the following command:

```terminal
$ oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 7 1
```

In this example, the latest revision number is 7.

If the output includes multiple revision numbers, such as 2 nodes are at revision 6; 1 nodes are at revision 7, this means that the update is still in progress. Wait a few minutes and try again.
3. Force a new rollout for the kube-scheduler by running the following command:

```terminal
$ oc patch kubescheduler cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
```


Verify all nodes are updated to the latest revision by running the following command:

```terminal
$ oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 7 1
```

In this example, the latest revision number is 7.

If the output includes multiple revision numbers, such as 2 nodes are at revision 6; 1 nodes are at revision 7, this means that the update is still in progress. Wait a few minutes and try again.
23. Monitor the platform Operators by running the following command:

```terminal
$ oc adm wait-for-stable-cluster
```


This process can take up to 15 minutes.
24. To ensure that all workloads return to normal operation following a recovery procedure, restart all the control plane nodes.

[NOTE]
----
On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using oc login might not immediately work until the OAuth server pods are restarted.
Consider using the system:admin kubeconfig file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:

```terminal
$ export KUBECONFIG=<installation_directory>/auth/kubeconfig
```

Issue the following command to display your authenticated user name:

```terminal
$ oc whoami
```

----
25. Restart all the worker nodes in a rolling fashion.

* Recovering a degraded etcd Operator

# Restoring a cluster manually from an etcd backup

The restore procedure described in the section "Restoring to a previous cluster state":

* Requires the complete recreation of 2 control plane nodes, which might be a complex procedure for clusters installed with the UPI installation method, since an UPI installation does not create any Machine or ControlPlaneMachineset for the control plane nodes.
* Uses the script /usr/local/bin/cluster-restore.sh, which starts a new single-member etcd cluster and then scales it to three members.

In contrast, this procedure:

* Does not require recreating any control plane nodes.
* Directly starts a three-member etcd cluster.

If the cluster uses a MachineSet for the control plane, it is suggested to use the "Restoring to a previous cluster state" for a simpler etcd recovery procedure.

When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an Red Hat OpenShift Container Platform 4.7.2 cluster must use an etcd backup that was taken from 4.7.2.

* Access to the cluster as a user with the cluster-admin role; for example, the kubeadmin user.
* SSH access to all control plane hosts, with a host user allowed to become root; for example, the default core host user.
* A backup directory containing both a previous etcd snapshot and the resources for the static pods from the same backup. The file names in the directory must be in the following formats: snapshot_<datetimestamp>.db and static_kuberesources_<datetimestamp>.tar.gz.

1. Use SSH to connect to each of the control plane nodes.

The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to use a SSH connection for each control plane host you are accessing in a separate terminal.

[IMPORTANT]
----
If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.
----
2. Copy the etcd backup directory to each control plane host.

This procedure assumes that you copied the backup directory containing the etcd snapshot and the resources for the static pods to the /home/core/assets directory of each control plane host. You might need to create such assets folder if it does not exist yet.
3. Stop the static pods on all the control plane nodes; one host at a time.
1. Move the existing Kubernetes API Server static pod manifest out of the kubelet manifest directory.

```terminal
$ mkdir -p /root/manifests-backup
$ mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /root/manifests-backup/
```

2. Verify that the Kubernetes API Server containers have stopped with the command:

```terminal
$ crictl ps | grep kube-apiserver | grep -E -v "operator|guard"
```


The output of this command should be empty. If it is not empty, wait a few minutes and check again.
3. If the Kubernetes API Server containers are still running, terminate them manually with the following command:

```terminal
$ crictl stop <container_id>
```

4. Repeat the same steps for kube-controller-manager-pod.yaml, kube-scheduler-pod.yaml and finally etcd-pod.yaml.
1. Stop the kube-controller-manager pod with the following command:

```terminal
$ mv /etc/kubernetes/manifests/kube-controller-manager-pod.yaml /root/manifests-backup/
```

2. Check if the containers are stopped using the following command:

```terminal
$ crictl ps | grep kube-controller-manager | grep -E -v "operator|guard"
```

3. Stop the kube-scheduler pod using the following command:

```terminal
$ mv /etc/kubernetes/manifests/kube-scheduler-pod.yaml /root/manifests-backup/
```

4. Check if the containers are stopped using the following command:

```terminal
$ crictl ps | grep kube-scheduler | grep -E -v "operator|guard"
```

5. Stop the etcd pod using the following command:

```terminal
$ mv /etc/kubernetes/manifests/etcd-pod.yaml /root/manifests-backup/
```

6. Check if the containers are stopped using the following command:

```terminal
$ crictl ps | grep etcd | grep -E -v "operator|guard"
```

4. On each control plane host, save the current etcd data, by moving it into the backup folder:

```terminal
$ mkdir /home/core/assets/old-member-data
$ mv /var/lib/etcd/member /home/core/assets/old-member-data
```


This data will be useful in case the etcd backup restore does not work and the etcd cluster must be restored to the current state.
5. Find the correct etcd parameters for each control plane host.
1. The value for <ETCD_NAME> is unique for the each control plane host, and it is equal to the value of the ETCD_NAME variable in the manifest /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/restore-etcd-pod/pod.yaml file in the specific control plane host. It can be found with the command:

```terminal
RESTORE_ETCD_POD_YAML="/etc/kubernetes/static-pod-resources/etcd-certs/configmaps/restore-etcd-pod/pod.yaml"
cat $RESTORE_ETCD_POD_YAML | \
  grep -A 1 $(cat $RESTORE_ETCD_POD_YAML | grep 'export ETCD_NAME' | grep -Eo 'NODE_.+_ETCD_NAME') | \
  grep -Po '(?<=value: ").+(?=")'
```

2. The value for <UUID> can be generated in a control plane host with the command:

```terminal
$ uuidgen
```


[NOTE]
----
The value for <UUID> must be generated only once. After generating UUID on one control plane host, do not generate it again on the others. The same UUID will be used in the next steps on all control plane hosts.
----
3. The value for ETCD_NODE_PEER_URL should be set like the following example:

```yaml
https://<IP_CURRENT_HOST>:2380
```


The correct IP can be found from the <ETCD_NAME> of the specific control plane host, with the command:

```terminal
$ echo <ETCD_NAME> | \
  sed -E 's/[.-]/_/g' | \
  xargs -I {} grep {} /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd.env | \
  grep "IP" | grep -Po '(?<=").+(?=")'
```

4. The value for <ETCD_INITIAL_CLUSTER> should be set like the following, where <ETCD_NAME_n> is the <ETCD_NAME> of each control plane host.

[NOTE]
----
The port used must be 2380 and not 2379. The port 2379 is used for etcd database management and is configured directly in etcd start command in container.
----
Example output

```terminal
<ETCD_NAME_0>=<ETCD_NODE_PEER_URL_0>,<ETCD_NAME_1>=<ETCD_NODE_PEER_URL_1>,<ETCD_NAME_2>=<ETCD_NODE_PEER_URL_2> 1
```

Specifies the ETCD_NODE_PEER_URL values from each control plane host.

The <ETCD_INITIAL_CLUSTER> value remains same across all control plane hosts. The same value is required in the next steps on every control plane host.
6. Regenerate the etcd database from the backup.

Such operation must be executed on each control plane host.
1. Copy the etcd backup to /var/lib/etcd directory with the command:

```terminal
$ cp /home/core/assets/backup/<snapshot_yyyy-mm-dd_hhmmss>.db /var/lib/etcd
```

2. Identify the correct etcdctl image before proceeding. Use the following command to retrieve the image from the backup of the pod manifest:

```terminal
$ jq -r '.spec.containers[]|select(.name=="etcdctl")|.image' /root/manifests-backup/etcd-pod.yaml
```


```terminal
$ podman run --rm -it --entrypoint="/bin/bash" -v /var/lib/etcd:/var/lib/etcd:z <image-hash>
```

3. Check that the version of the etcdctl tool is the version of the etcd server where the backup was created:

```terminal
$ etcdctl version
```

4. Run the following command to regenerate the etcd database, using the correct values for the current host:

```terminal
$ ETCDCTL_API=3 /usr/bin/etcdctl snapshot restore /var/lib/etcd/<snapshot_yyyy-mm-dd_hhmmss>.db \
  --name "<ETCD_NAME>" \
  --initial-cluster="<ETCD_INITIAL_CLUSTER>" \
  --initial-cluster-token "openshift-etcd-<UUID>" \
  --initial-advertise-peer-urls "<ETCD_NODE_PEER_URL>" \
  --data-dir="/var/lib/etcd/restore-<UUID>" \
  --skip-hash-check=true
```


[NOTE]
----
The quotes are mandatory when regenerating the etcd database.
----
7. Record the values printed in the added member logs; for example:
Example output

```
2022-06-28T19:52:43Z    info    membership/cluster.go:421   added member    {"cluster-id": "c5996b7c11c30d6b", "local-member-id": "0", "added-peer-id": "56cd73b614699e7", "added-peer-peer-urls": ["https://10.0.91.5:2380"], "added-peer-is-learner": false}
2022-06-28T19:52:43Z    info    membership/cluster.go:421   added member    {"cluster-id": "c5996b7c11c30d6b", "local-member-id": "0", "added-peer-id": "1f63d01b31bb9a9e", "added-peer-peer-urls": ["https://10.0.90.221:2380"], "added-peer-is-learner": false}
2022-06-28T19:52:43Z    info    membership/cluster.go:421   added member    {"cluster-id": "c5996b7c11c30d6b", "local-member-id": "0", "added-peer-id": "fdc2725b3b70127c", "added-peer-peer-urls": ["https://10.0.94.214:2380"], "added-peer-is-learner": false}
```

1. Exit from the container.
2. Repeat these steps on the other control plane hosts, checking that the values printed in the added member logs are the same for all control plane hosts.
8. Move the regenerated etcd database to the default location.

Such operation must be executed on each control plane host.
1. Move the regenerated database (the member folder created by the previous etcdctl snapshot restore command) to the default etcd location /var/lib/etcd:

```terminal
$ mv /var/lib/etcd/restore-<UUID>/member /var/lib/etcd
```

2. Restore the SELinux context for /var/lib/etcd/member folder on /var/lib/etcd directory:

```terminal
$ restorecon -vR /var/lib/etcd/
```

3. Remove the leftover files and directories:

```terminal
$ rm -rf /var/lib/etcd/restore-<UUID>
```


```terminal
$ rm /var/lib/etcd/<snapshot_yyyy-mm-dd_hhmmss>.db
```


[IMPORTANT]
----
When you are finished the /var/lib/etcd directory must contain only the folder member.
----
4. Repeat these steps on the other control plane hosts.
9. Restart the etcd cluster.
1. The following steps must be executed on all control plane hosts, but one host at a time.
2. Move the etcd static pod manifest back to the kubelet manifest directory, in order to make kubelet start the related containers :

```terminal
$ mv /tmp/etcd-pod.yaml /etc/kubernetes/manifests
```

3. Verify that all the etcd containers have started:

```terminal
$ crictl ps | grep etcd | grep -v operator
```

Example output

```terminal
38c814767ad983       f79db5a8799fd2c08960ad9ee22f784b9fbe23babe008e8a3bf68323f004c840                                                         28 seconds ago       Running             etcd-health-monitor                   2                   fe4b9c3d6483c
e1646b15207c6       9d28c15860870e85c91d0e36b45f7a6edd3da757b113ec4abb4507df88b17f06                                                         About a minute ago   Running             etcd-metrics                          0                   fe4b9c3d6483c
08ba29b1f58a7       9d28c15860870e85c91d0e36b45f7a6edd3da757b113ec4abb4507df88b17f06                                                         About a minute ago   Running             etcd                                  0                   fe4b9c3d6483c
2ddc9eda16f53       9d28c15860870e85c91d0e36b45f7a6edd3da757b113ec4abb4507df88b17f06                                                         About a minute ago   Running             etcdctl
```


If the output of this command is empty, wait a few minutes and check again.
10. Check the status of the etcd cluster.
1. On any of the control plane hosts, check the status of the etcd cluster with the following command:

```terminal
$ crictl exec -it $(crictl ps | grep etcdctl | awk '{print $1}') etcdctl endpoint status -w table
```

Example output

```terminal
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT         |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://10.0.89.133:2379 | 682e4a83a0cec6c0 |   3.5.0 |   67 MB |      true |      false |         2 |        218 |                218 |        |
|  https://10.0.92.74:2379 | 450bcf6999538512 |   3.5.0 |   67 MB |     false |      false |         2 |        218 |                218 |        |
| https://10.0.93.129:2379 | 358efa9c1d91c3d6 |   3.5.0 |   67 MB |     false |      false |         2 |        218 |                218 |        |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```

11. Restart the other static pods.

The following steps must be executed on all control plane hosts, but one host at a time.
1. Move the Kubernetes API Server static pod manifest back to the kubelet manifest directory to make kubelet start the related containers with the command:

```terminal
$ mv /root/manifests-backup/kube-apiserver-pod.yaml /etc/kubernetes/manifests
```

2. Verify that all the Kubernetes API Server containers have started:

```terminal
$ crictl ps | grep kube-apiserver | grep -v operator
```


[NOTE]
----
if the output of the following command is empty, wait a few minutes and check again.
----
3. Repeat the same steps for kube-controller-manager-pod.yaml and kube-scheduler-pod.yaml files.
1. Restart the kubelets in all nodes using the following command:

```terminal
$ systemctl restart kubelet
```

2. Start the remaining control plane pods using the following command:

```terminal
$ mv /root/manifests-backup/kube-* /etc/kubernetes/manifests/
```

3. Check if the kube-apiserver, kube-scheduler and kube-controller-manager pods start correctly:

```terminal
$ crictl ps | grep -E 'kube-(apiserver|scheduler|controller-manager)' | grep -v -E 'operator|guard'
```

4. Wipe the OVN databases using the following commands:

```terminal
for NODE in  $(oc get node -o name | sed 's:node/::g')
do
  oc debug node/${NODE} -- chroot /host /bin/bash -c  'rm -f /var/lib/ovn-ic/etc/ovn*.db && systemctl restart ovs-vswitchd ovsdb-server'
  oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-node --field-selector=spec.nodeName=${NODE} --wait
  oc -n openshift-ovn-kubernetes wait pod -l app=ovnkube-node --field-selector=spec.nodeName=${NODE} --for condition=ContainersReady --timeout=600s
done
```


# Additional resources

* Backing up etcd data
* Installing a user-provisioned cluster on bare metal
* Creating a bastion host to access Red Hat OpenShift Container Platform instances and the control plane nodes with SSH
* Replacing a bare-metal control plane node

# Issues and workarounds for restoring a persistent storage state

If your Red Hat OpenShift Container Platform cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a StatefulSet object. When you restore from an etcd backup, the status of the workloads in Red Hat OpenShift Container Platform is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.


[IMPORTANT]
----
The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an Red Hat OpenShift Container Platform cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.
----

The following are some example scenarios that produce an out-of-date status:

* MySQL database is running in a pod backed up by a PV object. Restoring Red Hat OpenShift Container Platform from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.
* Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. Red Hat OpenShift Container Platform is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.
* Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.
* A device is removed or renamed from Red Hat OpenShift Container Platform nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from /dev/disk/by-id or /dev directories. This situation might cause the local PVs to refer to devices that no longer exist.

To fix this problem, an administrator must:
1. Manually remove the PVs with invalid devices.
2. Remove symlinks from respective nodes.
3. Delete LocalVolume or LocalVolumeSet objects (see Storage -> Configuring persistent storage -> Persistent storage using local volumes -> Deleting the Local Storage Operator Resources).