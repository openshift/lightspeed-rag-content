# Migrating from the OpenShift SDN network plugin


As a cluster administrator, you can migrate to the OVN-Kubernetes network plugin from the OpenShift software-defined networking (SDN) plugin.

[WARNING]
----
If your cluster uses the OpenShift SDN network plugin, you cannot upgrade your cluster to Red Hat OpenShift Container Platform 4.17. You must initially migrate your cluster to the OVN-Kubernetes plugin and then upgrade your cluster to Red Hat OpenShift Container Platform 4.17.
Additionally, if your cluster uses a third-party container network interface (CNI) plugin, you cannot migrate your cluster to the OVN-Kubernetes network plugin. For a complete list of these CNI plugins, see Certified OpenShift CNI Plug-ins (Red Hat Knowledgebase).
----
The following methods exist for migrating from the OpenShift SDN network plugin to the OVN-Kubernetes plugin:
Ansible playbook:: The Ansible playbook method automates the offline migration method steps. This method has the same usage scenarios as the manual offline migration method.
Offline migration:: This is a manual process that includes some downtime. This method is primarily used for self-managed Red Hat OpenShift Container Platform deployments, and consider using this method when you cannot perform a limited live migration to the OVN-Kubernetes network plugin.
Limited live migration (Preferred method):: This is an automated process that migrates your cluster from OpenShift SDN to OVN-Kubernetes.

[WARNING]
----
For the limited live migration method only, do not automate the migration from OpenShift SDN to OVN-Kubernetes with a script or another tool such as Red Hat Ansible Automation Platform. This might cause outages or crash your Red Hat OpenShift Container Platform cluster.
----
* About the OVN-Kubernetes network plugin

# Limited live migration to the OVN-Kubernetes network plugin overview

The limited live migration method is the process in which the OpenShift SDN network plugin and its network configurations, connections, and associated resources, are migrated to the OVN-Kubernetes network plugin without service interruption. It is available for Red Hat OpenShift Container Platform, and is the preferred method for migrating from OpenShift SDN to OVN-Kubernetes. In the event that you cannot perform a limited live migration, you can use the offline migration method.


[IMPORTANT]
----
Before you migrate your Red Hat OpenShift Container Platform cluster to use the OVN-Kubernetes network plugin, update your cluster to the latest z-stream release so that all the latest bug fixes apply to your cluster.
----

It is not available for hosted control plane deployment types. This migration method is valuable for deployment types that require constant service availability and offers the following benefits:

* Continuous service availability
* Minimized downtime
* Automatic node rebooting
* Seamless transition from the OpenShift SDN network plugin to the OVN-Kubernetes network plugin

Although a rollback procedure is provided, the limited live migration is intended to be a one-way process.


[NOTE]
----
OpenShift SDN CNI is deprecated as of Red Hat OpenShift Container Platform 4.14. As of Red Hat OpenShift Container Platform 4.15, the network plugin is not an option for new installations. In a subsequent future release, the OpenShift SDN network plugin is planned to be removed and no longer supported. Red Hat will provide bug fixes and support for this feature until it is removed, but this feature will no longer receive enhancements. As an alternative to OpenShift SDN CNI, you can use OVN Kubernetes CNI instead. For more information, see OpenShift SDN CNI removal.
----

The following sections provide more information about the limited live migration method.

## Supported platforms when using the limited live migration method

The following table provides information about the supported platforms for the limited live migration type.




[NOTE]
----
Each listed platform supports installing an Red Hat OpenShift Container Platform cluster on installer-provisioned infrastructure and user-provisioned infrastructure.
----

## Best practices for limited live migration to the OVN-Kubernetes network plugin

For a list of best practices when migrating to the OVN-Kubernetes network plugin with the limited live migration method, see Limited Live Migration from OpenShift SDN to OVN-Kubernetes.

## Considerations for limited live migration to the OVN-Kubernetes network plugin

Before using the limited live migration method to the OVN-Kubernetes network plugin, cluster administrators should consider the following information:

* The limited live migration procedure is unsupported for clusters with OpenShift SDN multitenant mode enabled.
* Egress router pods block the limited live migration process. They must be removed before beginning the limited live migration process.
* During the migration, when the cluster is running with both OVN-Kubernetes and OpenShift SDN, multicast and egress IP addresses are temporarily disabled for both CNIs. Egress firewalls remains functional.
* The migration is intended to be a one-way process. However, for users that want to rollback to OpenShift-SDN, migration from OpenShift-SDN to OVN-Kubernetes must have succeeded. Users can follow the same procedure below to migrate to the OpenShift SDN network plugin from the OVN-Kubernetes network plugin.
* The limited live migration is not supported on HyperShift clusters.
* OpenShift SDN does not support IPsec. After the migration, cluster administrators can enable IPsec.
* OpenShift SDN does not support IPv6. After the migration, cluster administrators can enable dual-stack.
* The OpenShift SDN plugin allows application of the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) to the primary interface on a node. The OVN-Kubernetes network plugin does not have this capability.
* The cluster MTU is the MTU value for pod interfaces. It is always less than your hardware MTU to account for the cluster network overlay overhead. The overhead is 100 bytes for OVN-Kubernetes and 50 bytes for OpenShift SDN.

During the limited live migration, both OVN-Kubernetes and OpenShift SDN run in parallel. OVN-Kubernetes manages the cluster network of some nodes, while OpenShift SDN manages the cluster network of others. To ensure that cross-CNI traffic remains functional, the Cluster Network Operator updates the routable MTU to ensure that both CNIs share the same overlay MTU. As a result, after the migration has completed, the cluster MTU is 50 bytes less.
* OVN-Kubernetes reserves the 100.64.0.0/16 and 100.88.0.0/16 IP address ranges. These subnets cannot be overlapped with any other internal or external network. If these IP addresses have been used by OpenShift SDN or any external networks that might communicate with this cluster, you must patch them to use a different IP address range before starting the limited live migration. See "Patching OVN-Kubernetes address ranges" for more information.
* If your openshift-sdn cluster with Precision Time Protocol (PTP) uses the User Datagram Protocol (UDP) for hardware time stamping and you migrate to the OVN-Kubernetes plugin, the hardware time stamping cannot be applied to primary interface devices, such as an Open vSwitch (OVS) bridge. As a result, UDP version 4 configurations cannot work with a br-ex interface.
* In most cases, the limited live migration is independent of the secondary interfaces of pods created by the Multus CNI plugin. However, if these secondary interfaces were set up on the default network interface controller (NIC) of the host, for example, using MACVLAN, IPVLAN, SR-IOV, or bridge interfaces with the default NIC as the control node, OVN-Kubernetes might encounter malfunctions. Users should remove such configurations before proceeding with the limited live migration.
* When there are multiple NICs inside of the host, and the default route is not on the interface that has the Kubernetes NodeIP, you must use the offline migration instead.
* All DaemonSet objects in the openshift-sdn namespace, which are not managed by the Cluster Network Operator (CNO), must be removed before initiating the limited live migration. These unmanaged daemon sets can cause the migration status to remain incomplete if not properly handled.
* If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the update process. If minAvailable is set to 1 in PodDisruptionBudget, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the PodDisruptionBudget field can prevent the node drain.
* Like OpenShift SDN, OVN-Kubernetes resources such as EgressFirewall resources require ClusterAdmin privileges. Migrating from OpenShift SDN to OVN-Kubernetes does not automatically update role-base access control (RBAC) resources. OpenShift SDN resources granted to a project administrator through the aggregate-to-admin ClusterRole must be manually reviewed and adjusted, as these changes are not included in the migration process.

After migration, manual verification of RBAC resources is required. For information about setting the aggregate-to-admin ClusterRole after migration, see the example in How to allow project admins to manage Egressfirewall resources in RHOCP4.
* When a cluster depends on static routes or routing policies in the host network so that pods can reach some destinations, users should set routingViaHost spec to true and ipForwarding to Global in the gatewayConfig object before the migration. This will offload routing decision to host kernel. For more information, see Recommended practice to follow before Openshift SDN network plugin migration to OVNKubernetes plugin (Red Hat Knowledgebase) and, see step five in "Checking cluster resources before initiating the limited live migration".

* Understanding update channels and releases
* Asynchronous errata updates

## How the limited live migration process works

The following table summarizes the limited live migration process by segmenting between the user-initiated steps in the process and the actions that the migration script performs in response.



## Migrating to the OVN-Kubernetes network plugin by using the limited live migration method

Migrating to the OVN-Kubernetes network plugin by using the limited live migration method is a multiple step process that requires users to check the behavior of egress IP resources, egress firewall resources, and multicast enabled namespaces. Administrators must also review any network policies in their deployment and remove egress router resources before initiating the limited live migration process. The following procedures should be used in succession.

### Checking cluster resources before initiating the limited live migration

Before migrating to OVN-Kubernetes by using the limited live migration, you should check for egress IP resources, egress firewall resources, and multicast-enabled namespaces on your OpenShift SDN deployment. You should also review any network policies in your deployment. If you find that your cluster has these resources before migration, you should check their behavior after migration to ensure that they are working as intended.

The following procedure shows you how to check for egress IP resources, egress firewall resources, multicast-enabled namespaces, network policies, and an NNCP. No action is necessary after checking for these resources.

* You have access to the cluster as a user with the cluster-admin role.

1. As an Red Hat OpenShift Container Platform cluster administrator, check for egress firewall resources. You can do this by using the oc CLI, or by using the Red Hat OpenShift Container Platform web console.
1. To check for egress firewall resource by using the oc CLI tool:
1. To check for egress firewall resources, enter the following command:

```terminal
$ oc get egressnetworkpolicies.network.openshift.io -A
```

Example output

```terminal
NAMESPACE    NAME                      AGE
<namespace>  <example_egressfirewall>  5d
```

2. You can check the intended behavior of an egress firewall resource by using the -o yaml flag. For example:

```terminal
$ oc get egressnetworkpolicy <example_egressfirewall> -n <namespace> -o yaml
```

Example output

```terminal
apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: <example_egress_policy>
  namespace: <namespace>
spec:
  egress:
  - type: Allow
    to:
      cidrSelector: 0.0.0.0/0
  - type: Deny
    to:
      cidrSelector: 10.0.0.0/8
```

2. To check for egress firewall resources by using the Red Hat OpenShift Container Platform web console:
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, type sdn_controller_num_egress_firewalls and click Run queries. If you have egress firewall resources, they are returned in the Expression box.
2. Check your cluster for egress IP resources. You can do this by using the oc CLI, or by using the Red Hat OpenShift Container Platform web console.
1. To check for egress IPs by using the oc CLI tool:
1. To list namespaces with egress IP resources, enter the following command

```terminal
$ oc get netnamespace -A | awk '$3 != ""'
```

Example output

```terminal
NAME        NETID      EGRESS IPS
namespace1  14173093   ["10.0.158.173"]
namespace2  14173020   ["10.0.158.173"]
```

2. To check for egress IPs by using the Red Hat OpenShift Container Platform web console:
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, type sdn_controller_num_egress_ips and click Run queries. If you have egress firewall resources, they are returned in the Expression box.
3. Check your cluster for multicast enabled namespaces. You can do this by using the oc CLI, or by using the Red Hat OpenShift Container Platform web console.
1. To check for multicast enabled namespaces by using the oc CLI tool:
1. To locate namespaces with multicast enabled, enter the following command:

```terminal
$ oc get netnamespace -o json | jq -r '.items[] | select(.metadata.annotations."netnamespace.network.openshift.io/multicast-enabled" == "true") | .metadata.name'
```

Example output

```terminal
namespace1
namespace3
```

2. To check for multicast enabled namespaces by using the Red Hat OpenShift Container Platform web console:
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, type sdn_controller_num_multicast_enabled_namespaces and click Run queries. If you have multicast enabled namespaces, they are returned in the Expression box.
4. Check your cluster for any network policies. You can do this by using the oc CLI.
1. To check for network policies by using the oc CLI tool, enter the following command:

```terminal
$ oc get networkpolicy -n <namespace>
```

Example output

```terminal
NAME              POD-SELECTOR   AGE
allow-multicast   app=my-app     11m
```

5. Optional: If your cluster uses static routes or routing policies in the host network, set routingViaHost spec to true and the ipForwarding spec to Global in the gatewayConfig object during migration.

```terminal
$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "gatewayConfig": {
            "ipForwarding": "Global",
            "routingViaHost": true
    }}}}}'
```

1. Verify that the ipForwarding spec has been set to Global and the routingViaHost spec to true by running the following command:

```terminal
$  oc get networks.operator.openshift.io cluster -o yaml | grep -A 5 "gatewayConfig"
```

Example output

```terminal
apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
# ...
gatewayConfig:
        ipForwarding: Global
        ipv4: {}
        ipv6: {}
        routingViaHost: true
      genevePort: 6081
# ...
```


### Removing egress router pods before initiating the limited live migration

Before initiating the limited live migration, you must check for, and remove, any egress router pods. If there is an egress router pod on the cluster when performing a limited live migration, the Network Operator blocks the migration and returns the following error:


```text
The cluster configuration is invalid (network type limited live migration is not supported for pods with `pod.network.openshift.io/assign-macvlan` annotation.

Please remove all egress router pods). Use `oc edit network.config.openshift.io cluster` to fix.
```


* You have access to the cluster as a user with the cluster-admin role.

1. To locate egress router pods on your cluster, enter the following command:

```terminal
$ oc get pods --all-namespaces -o json | jq '.items[] | select(.metadata.annotations."pod.network.openshift.io/assign-macvlan" == "true") | {name: .metadata.name, namespace: .metadata.namespace}'
```

Example output

```terminal
{
  "name": "egress-multi",
  "namespace": "egress-router-project"
}
```

2. Alternatively, you can query metrics on the Red Hat OpenShift Container Platform web console.
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, enter network_attachment_definition_instances{networks="egress-router"}. Then, click Add.
3. To remove an egress router pod, enter the following command:

```terminal
$ oc delete pod <egress_pod_name> -n <egress_router_project>
```


### Removing the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR)

The OpenShift SDN plugin allows application of the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) to the primary interface on a node. The OVN-Kubernetes network plugin does not have this capability.

If you have an NNCP applied to the primary interface, you must delete the NNCP before migrating to the OVN-Kubernetes network plugin. Deleting the NNCP does not remove the configuration from the primary interface, but the Kubernetes-NMState cannot manage this configuration. Instead, the configure-ovs.sh shell script manages the primary interface and the configuration attached to it.

* You created an NNCP CR and applied it to the primary interface of your network.

1. Remove the configuration from the Cluster Network Operator (CNO) configuration object by running the following command:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{"spec":{"migration":null}}'
```

2. Delete the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) that defines the primary network interface for the OpenShift SDN network plugin by completing the following steps:
1. Check that the existing NNCP CR bonded the primary interface to your cluster by entering the following command:

```terminal
$ oc get nncp
```

Example output

```terminal
NAME          STATUS      REASON
bondmaster0   Available   SuccessfullyConfigured
```


Network Manager stores the connection profile for the bonded primary interface in the /etc/NetworkManager/system-connections system path.
2. Remove the NNCP from your cluster:

```terminal
$ oc delete nncp <nncp_manifest_filename>
```


### Initiating the limited live migration process

After you have checked the behavior of egress IP resources, egress firewall resources, and multicast enabled namespaces, and removed any egress router resources, you can initiate the limited live migration process.

* A cluster has been configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.
* You have installed the OpenShift CLI (oc).
* You have access to the cluster as a user with the cluster-admin role.
* You have created a recent backup of the etcd database.
* The cluster is in a known good state without any errors.
* Before migration to OVN-Kubernetes, a security group rule must be in place to allow UDP packets on port 6081 for all nodes on all cloud platforms.
* If the 100.64.0.0/16 and 100.88.0.0/16 address ranges were previously in use by OpenShift-SDN, you have patched them. A step in the procedure checks whether these address ranges are in use. If they are in use, see "Patching OVN-Kubernetes address ranges".
* You have checked for egress IP resources, egress firewall resources, and multicast enabled namespaces.
* You have removed any egress router pods before beginning the limited live migration. For more information about egress router pods, see "Deploying an egress router pod in redirect mode".
* You have reviewed the "Considerations for limited live migration to the OVN-Kubernetes network plugin" section of this document.

1. To patch the cluster-level networking configuration and initiate the migration from OpenShift SDN to OVN-Kubernetes, enter the following command:

```terminal
$ oc patch Network.config.openshift.io cluster --type='merge' --patch '{"metadata":{"annotations":{"network.openshift.io/network-type-migration":""}},"spec":{"networkType":"OVNKubernetes"}}'
```


After running this command, the migration process begins. During this process, the Machine Config Operator reboots the nodes in your cluster twice. The migration takes approximately twice as long as a cluster upgrade.

[IMPORTANT]
----
This oc patch command checks for overlapping CIDRs in use by OpenShift SDN. If overlapping CIDRs are detected, you must patch them before the limited live migration process can start. For more information, see "Patching OVN-Kubernetes address ranges".
----
2. Optional: To ensure that the migration process has completed, and to check the status of the network.config, you can enter the following commands:

```terminal
$ oc get network.config.openshift.io cluster -o jsonpath='{.status.networkType}'
```


```terminal
$ oc get network.config cluster -o=jsonpath='{.status.conditions}' | jq .
```


You can check limited live migration metrics for troubleshooting issues. For more information, see "Checking limited live migration metrics".
3. After a successful migration operation, remove the network.openshift.io/network-type-migration- annotation from the network.config custom resource by entering the following command:

```terminal
$ oc annotate network.config cluster network.openshift.io/network-type-migration-
```


### Patching OVN-Kubernetes address ranges

OVN-Kubernetes reserves the following IP address ranges:

* 100.64.0.0/16. This IP address range is used for the internalJoinSubnet parameter of OVN-Kubernetes by default.
* 100.88.0.0/16. This IP address range is used for the internalTransSwitchSubnet parameter of OVN-Kubernetes by default.

If these IP addresses have been used by OpenShift SDN or any external networks that might communicate with this cluster, you must patch them to use a different IP address range before initiating the limited live migration.

The following procedure can be used to patch CIDR ranges that are in use by OpenShift SDN if the migration was initially blocked.


[NOTE]
----
This is an optional procedure and must only be used if the migration was blocked after using the oc patch Network.config.openshift.io cluster --type='merge' --patch '{"metadata":{"annotations":{"network.openshift.io/network-type-migration":""}},"spec":{"networkType":"OVNKubernetes"}}' command "Initiating the limited live migration process".
----

* You have access to the cluster as a user with the cluster-admin role.

1. If the 100.64.0.0/16 IP address range is already in use, enter the following command to patch it to a different range. The following example uses 100.70.0.0/16.

```terminal
$ oc patch network.operator.openshift.io cluster --type='merge' -p='{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipv4":{"internalJoinSubnet": "100.70.0.0/16"}}}}}'
```

2. If the 100.88.0.0/16 IP address range is already in use, enter the following command to patch it to a different range. The following example uses  100.99.0.0/16.

```terminal
$ oc patch network.operator.openshift.io cluster --type='merge' -p='{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipv4":{"internalTransitSwitchSubnet": "100.99.0.0/16"}}}}}'
```


After patching the 100.64.0.0/16 and 100.88.0.0/16 IP address ranges, you can initiate the limited live migration.

### Checking cluster resources after initiating the limited live migration

The following procedure shows you how to check for egress IP resources, egress firewall resources, multicast enabled namespaces, and network policies when your deploying is using OVN-Kubernetes. If you had these resources on OpenShift SDN, you should check them after migration to ensure that they are working properly.

* You have access to the cluster as a user with the cluster-admin role.
* You have successfully migrated from OpenShift SDN to OVN-Kubernetes by using the limited live migration.

1. As an Red Hat OpenShift Container Platform cluster administrator, check for egress firewall resources. You can do this by using the oc CLI, or by using the Red Hat OpenShift Container Platform web console.
1. To check for egress firewall resource by using the oc CLI tool:
1. To check for egress firewall resources, enter the following command:

```terminal
$ oc get egressfirewalls.k8s.ovn.org -A
```

Example output

```terminal
NAMESPACE    NAME                      AGE
<namespace>  <example_egressfirewall>   5d
```

2. You can check the intended behavior of an egress firewall resource by using the -o yaml flag. For example:

```terminal
$ oc get egressfirewall <example_egressfirewall> -n <namespace> -o yaml
```

Example output

```terminal
apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: <example_egress_policy>
  namespace: <namespace>
spec:
  egress:
  - type: Allow
    to:
      cidrSelector: 192.168.0.0/16
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0
```


Ensure that the behavior of this resource is intended because it could have changed after migration. For more information about egress firewalls, see "Configuring an egress firewall for a project".
2. To check for egress firewall resources by using the Red Hat OpenShift Container Platform web console:
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, type ovnkube_controller_num_egress_firewall_rules and click Run queries. If you have egress firewall resources, they are returned in the Expression box.
2. Check your cluster for egress IP resources. You can do this by using the oc CLI, or by using the Red Hat OpenShift Container Platform web console.
1. To check for egress IPs by using the oc CLI tool:
1. To list the namespace with egress IP resources, enter the following command:

```terminal
$ oc get egressip
```

Example output

```terminal
NAME                EGRESSIPS    ASSIGNED NODE                              ASSIGNED EGRESSIPS
egress-sample       192.0.2.10   ip-10-0-42-79.us-east-2.compute.internal   192.0.2.10
egressip-sample-2   192.0.2.14   ip-10-0-42-79.us-east-2.compute.internal   192.0.2.14
```

2. To provide detailed information about an egress IP, enter the following command:

```terminal
$ oc get egressip <egressip_name> -o yaml
```

Example output

```terminal
apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"k8s.ovn.org/v1","kind":"EgressIP","metadata":{"annotations":{},"name":"egressip-sample"},"spec":{"egressIPs":["192.0.2.12","192.0.2.13"],"namespaceSelector":{"matchLabels":{"name":"my-namespace"}}}}
  creationTimestamp: "2024-06-27T15:48:36Z"
  generation: 7
  name: egressip-sample
  resourceVersion: "125511578"
  uid: b65833c8-781f-4cc9-bc96-d970259a7631
spec:
  egressIPs:
  - 192.0.2.12
  - 192.0.2.13
  namespaceSelector:
    matchLabels:
      name: my-namespace
```


Repeat this for all egress IPs. Ensure that the behavior of each resource is intended because it could have changed after migration. For more information about EgressIPs, see "Configuring an EgressIP address".
2. To check for egress IPs by using the Red Hat OpenShift Container Platform web console:
1. On the Red Hat OpenShift Container Platform web console, click Observe -> Metrics.
2. In the Expression box, type ovnkube_clustermanager_num_egress_ips and click Run queries. If you have egress firewall resources, they are returned in the Expression box.
3. Check your cluster for multicast enabled namespaces. You can only do this by using the oc CLI.
1. To locate namespaces with multicast enabled, enter the following command:

```terminal
$ oc get namespace -o json | jq -r '.items[] | select(.metadata.annotations."k8s.ovn.org/multicast-enabled" == "true") | .metadata.name'
```

Example output

```terminal
namespace1
namespace3
```

2. To describe each multicast enabled namespace, enter the following command:

```terminal
$ oc describe namespace <namespace>
```

Example output

```terminal
Name:         my-namespace
Labels:       kubernetes.io/metadata.name=my-namespace
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
Annotations:  k8s.ovn.org/multicast-enabled: true
              openshift.io/sa.scc.mcs: s0:c25,c0
              openshift.io/sa.scc.supplemental-groups: 1000600000/10000
              openshift.io/sa.scc.uid-range: 1000600000/10000
Status:       Active
```


Ensure that multicast functionality is correctly configured and working as expected in each namespace. For more information, see "Enabling multicast for a project".
4. Check your cluster's network policies. You can only do this by using the oc CLI.
1. To obtain information about network policies within a namespace, enter the following command:

```terminal
$ oc get networkpolicy -n <namespace>
```

Example output

```terminal
NAME              POD-SELECTOR   AGE
allow-multicast   app=my-app     11m
```

2. To provide detailed information about the network policy, enter the following command:

```terminal
$ oc describe networkpolicy allow-multicast -n <namespace>
```

Example output

```terminal
Name:         allow-multicast
Namespace:    my-namespace
Created on:   2024-07-24 14:55:03 -0400 EDT
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     app=my-app
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      IPBlock:
        CIDR: 224.0.0.0/4
        Except:
  Allowing egress traffic:
    To Port: <any> (traffic allowed to all ports)
    To:
      IPBlock:
        CIDR: 224.0.0.0/4
        Except:
  Policy Types: Ingress, Egress
```


Ensure that the behavior of the network policy is as intended. Optimization for network policies differ between SDN and OVN-K, so users might need to adjust their policies to achieve optimal performance for different CNIs. For more information, see "About network policy".

## Checking limited live migration metrics

Metrics are available to monitor the progress of the limited live migration. Metrics can be viewed on the Red Hat OpenShift Container Platform web console, or by using the oc CLI.

* You have initiated a limited live migration to OVN-Kubernetes.

1. To view limited live migration metrics on the Red Hat OpenShift Container Platform web console:
1. Click Observe -> Metrics.
2. In the Expression box, type openshift_network and click the openshift_network_operator_live_migration_condition option.
2. To view metrics by using the oc CLI:
1. Enter the following command to generate a token for the prometheus-k8s service account in the openshift-monitoring namespace:

```terminal
$ token=`oc create token prometheus-k8s -n openshift-monitoring`
```

2. Enter the following command to request information about the openshift_network_operator_live_migration_condition metric:

```terminal
$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -k -H "Authorization: Bearer $token" 'https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?' --data-urlencode 'query=openshift_network_operator_live_migration_condition' | jq
```

Example output

```terminal
 "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "openshift_network_operator_live_migration_condition",
          "container": "network-operator",
          "endpoint": "metrics",
          "instance": "10.0.83.62:9104",
          "job": "metrics",
          "namespace": "openshift-network-operator",
          "pod": "network-operator-6c87754bc6-c8qld",
          "prometheus": "openshift-monitoring/k8s",
          "service": "metrics",
          "type": "NetworkTypeMigrationInProgress"
        },
        "value": [
          1717653579.587,
          "1"
        ]
      },
...
```


The table in "Information about limited live migration metrics" shows you the available metrics and the label values populated from the openshift_network_operator_live_migration_condition expression. Use this information to monitor progress or to troubleshoot the migration.

### Information about limited live migration metrics

The following table shows you the available metrics and the label values populated from the openshift_network_operator_live_migration_condition expression. Use this information to monitor progress or to troubleshoot the migration.



# Additional resources

* Red Hat OpenShift Network Calculator
* Configuration for the OVN-Kubernetes network plugin
* Backing up etcd
* About network policy
* Changing the cluster MTU
* MTU value selection
* About network policy
* Converting to IPv4/IPv6 dual-stack networking
* OVN-Kubernetes capabilities
* Configuring an egress IP address
* Configuring an egress firewall for a project
* OVN-Kubernetes egress firewall blocks process to deploy application as DeploymentConfig
* Enabling multicast for a project
* OpenShift SDN capabilities
* Configuring egress IPs for a project
* Configuring an egress firewall for a project
* Enabling multicast for a project
* Deploying an egress router pod in redirect mode
* Network [operator.openshift.io/v1

# Offline migration to the OVN-Kubernetes network plugin overview

The offline migration method is a manual process that includes some downtime, during which your cluster is unreachable. You can use an Ansible playbook that automates the offline migration steps so that you can save time. These methods are primarily used for self-managed Red Hat OpenShift Container Platform deployments, and are an alternative to the limited live migration procedure. Use these methods only when you cannot perform a limited live migration to the OVN-Kubernetes network plugin.


[IMPORTANT]
----
Before you migrate your Red Hat OpenShift Container Platform cluster to use the OVN-Kubernetes network plugin, update your cluster to the latest z-stream release so that all the latest bug fixes apply to your cluster.
----

Although a rollback procedure is provided, the offline migration methods are intended as one-way processes.


[NOTE]
----
OpenShift SDN CNI is deprecated as of Red Hat OpenShift Container Platform 4.14. As of Red Hat OpenShift Container Platform 4.15, the network plugin is not an option for new installations. In a subsequent future release, the OpenShift SDN network plugin is planned to be removed and no longer supported. Red Hat will provide bug fixes and support for this feature until it is removed, but this feature will no longer receive enhancements. As an alternative to OpenShift SDN CNI, you can use OVN Kubernetes CNI instead. For more information, see OpenShift SDN CNI removal.
----

The following sections provide more information about the offline migration method.

## Supported platforms when using the offline migration methods

The following table provides information about the supported platforms for the manual offline migration type.



The following table provides information about the supported platforms for the manual offline migration type.




[NOTE]
----
For each listed platform in these tables, the platform supports installing an Red Hat OpenShift Container Platform cluster on installer-provisioned infrastructure and user-provisioned infrastructure.
----

## Best practices for manual offline migration and the Ansible playbook offline migration methods

For a list of best practices when migrating to the OVN-Kubernetes network plugin with the offline migration method, see Best practices for OpenShift SDN to OVN Kubernetes CNI Offline migration.

## Considerations for the offline migration methods to the OVN-Kubernetes network plugin

If you have more than 150 nodes in your Red Hat OpenShift Container Platform cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.

The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.

While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.

* If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:
* Namespace isolation
* Egress router pods
* Before migrating to OVN-Kubernetes, ensure that the following IP address ranges are not in use: 100.64.0.0/16, 169.254.169.0/29, 100.88.0.0/16, fd98::/64, fd69::/125, and fd97::/64. OVN-Kubernetes uses these ranges internally. Do not include any of these ranges in any other CIDR definitions in your cluster or infrastructure.
* If your openshift-sdn cluster with Precision Time Protocol (PTP) uses the User Datagram Protocol (UDP) for hardware time stamping and you migrate to the OVN-Kubernetes plugin, the hardware time stamping cannot be applied to primary interface devices, such as an Open vSwitch (OVS) bridge. As a result, UDP version 4 configurations cannot work with a br-ex interface.
* Like OpenShift SDN, OVN-Kubernetes resources require ClusterAdmin privileges. Migrating from OpenShift SDN to OVN-Kubernetes does not automatically update role-base access control (RBAC) resources. OpenShift SDN resources granted to a project administrator through the aggregate-to-admin ClusterRole must be manually reviewed and adjusted, as these changes are not included in the migration process.

After migration, manual verification of RBAC resources is required.

The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.

### Primary network interface

The OpenShift SDN plugin allows application of the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) to the primary interface on a node. The OVN-Kubernetes network plugin does not have this capability.

If you have an NNCP applied to the primary interface, you must delete the NNCP before migrating to the OVN-Kubernetes network plugin. Deleting the NNCP does not remove the configuration from the primary interface, but with OVN-Kubernetes, the Kubernetes NMState cannot manage this configuration. Instead, the configure-ovs.sh shell script manages the primary interface and the configuration attached to this interface.

### Namespace isolation

OVN-Kubernetes supports only the network policy isolation mode.


[IMPORTANT]
----
For a cluster using OpenShift SDN that is configured in either the multitenant or subnet isolation mode, you can still migrate to the OVN-Kubernetes network plugin. Note that after the migration operation, multitenant isolation mode is dropped, so you must manually configure network policies to achieve the same level of project-level isolation for pods and services.
----

### Egress IP addresses

OpenShift SDN supports two different Egress IP modes:

* In the automatically assigned approach, an egress IP address range is assigned to a node.
* In the manually assigned approach, a list of one or more egress IP addresses is assigned to a node.

The migration process supports migrating Egress IP configurations that use the automatically assigned mode.

The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:



For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".

### Egress network policies

The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:




[NOTE]
----
Because the name of an EgressFirewall object can only be set to default, after the migration all migrated EgressNetworkPolicy objects are named default, regardless of what the name was under OpenShift SDN.
If you subsequently rollback to OpenShift SDN, all EgressNetworkPolicy objects are named default as the prior name is lost.
For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".
----

### Egress router pods

OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.

When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.

### Multicast

The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:



For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".

### Network policies

OVN-Kubernetes fully supports the Kubernetes NetworkPolicy API in the networking.k8s.io/v1 API group. No changes are necessary in your network policies when migrating from OpenShift SDN.

* Understanding update channels and releases
* Asynchronous errata updates

## How the offline migration process works

The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.



## Using an Ansible playbook to migrate to the OVN-Kubernetes network plugin

As a cluster administrator, you can use an Ansible collection, network.offline_migration_sdn_to_ovnk, to migrate from the OpenShift SDN Container Network Interface (CNI) network plugin to the OVN-Kubernetes plugin for your cluster. The Ansible collection includes the following playbooks:

* playbooks/playbook-migration.yml: Includes playbooks that execute in a sequence where each playbook represents a step in the migration process.
* playbooks/playbook-rollback.yml: Includes playbooks that execute in a sequence where each playbook represents a step in the rollback process.

* You installed the python3 package, minimum version 3.10.
* You installed the jmespath and jq packages.
* You logged in to the Red Hat Hybrid Cloud Console and opened the Ansible Automation Platform web console.
* You created a security group rule that allows User Datagram Protocol (UDP) packets on port 6081 for all nodes on all cloud platforms. If you do not do this task, your cluster might fail to schedule pods.
* Check if your cluster uses static routes or routing policies in the host network.
* If true, a later procedure step requires that you set the routingViaHost parameter to true and the ipForwarding parameter to Global in the gatewayConfig section of the playbooks/playbook-migration.yml file.
* If the OpenShift-SDN plugin uses the 100.64.0.0/16 and 100.88.0.0/16 address ranges, you patched the address ranges. For more information, see "Patching OVN-Kubernetes address ranges" in the Additional resources section.

1. Install the ansible-core package, minimum version 2.15. The following example command shows how to install the ansible-core package on Red Hat Enterprise Linux (RHEL):

```terminal
$ sudo dnf install -y ansible-core
```

2. Create an ansible.cfg file and add information similar to the following example to the file. Ensure that file exists in the same directory as where the ansible-galaxy commands and the playbooks run.

```ini
$ cat << EOF >> ansible.cfg
[galaxy]
server_list = automation_hub, validated

[galaxy_server.automation_hub]
url=https://console.redhat.com/api/automation-hub/content/published/
auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
token=

#[galaxy_server.release_galaxy]
#url=https://galaxy.ansible.com/

[galaxy_server.validated]
url=https://console.redhat.com/api/automation-hub/content/validated/
auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
token=
EOF
```

3. From the Ansible Automation Platform web console, go to the Connect to Hub page and complete the following steps:
1. In the Offline token section of the page, click the Load token button.
2. After the token loads, click the Copy to clipboard icon.
3. Open the ansible.cfg file and paste the API token in the token= parameter. The API token is required for authenticating against the server URL specified in the ansible.cfg file.
4. Install the network.offline_migration_sdn_to_ovnk Ansible collection by entering the following ansible-galaxy command:

```terminal
$ ansible-galaxy collection install network.offline_migration_sdn_to_ovnk
```

5. Verify that the network.offline_migration_sdn_to_ovnk Ansible collection is installed on your system:

```terminal
$ ansible-galaxy collection list | grep network.offline_migration_sdn_to_ovnk
```

Example output

```terminal
network.offline_migration_sdn_to_ovnk   1.0.2
```


The network.offline_migration_sdn_to_ovnk Ansible collection is saved in the default path of ~/.ansible/collections/ansible_collections/network/offline_migration_sdn_to_ovnk/.
6. Configure migration features in the playbooks/playbook-migration.yml file:

```yaml
# ...
    migration_interface_name: eth0
    migration_disable_auto_migration: true
    migration_egress_ip: false
    migration_egress_firewall: false
    migration_multicast: false
    migration_routing_via_host: true
    migration_ip_forwarding: Global
    migration_cidr: "10.240.0.0/14"
    migration_prefix: 23
    migration_mtu: 1400
    migration_geneve_port: 6081
    migration_ipv4_subnet: "100.64.0.0/16"
# ...
```

migration_interface_name:: If you use an NodeNetworkConfigurationPolicy (NNCP) resource on a primary interface, specify the interface name in the migration-playbook.yml file so that the NNCP resource gets deleted on the primary interface during the migration process.
migration_disable_auto_migration:: Disables the auto-migration of OpenShift SDN CNI plug-in features to the OVN-Kubernetes plugin. If you disable auto-migration of features, you must also set the migration_egress_ip, migration_egress_firewall, and migration_multicast parameters to false. If you need to enable auto-migration of features, set the parameter to false.
migration_routing_via_host:: Set to true to configure local gateway mode or false to configure shared gateway mode for nodes in your cluster. The default value is false. In local gateway mode, traffic is routed through the host network stack. In shared gateway mode, traffic is not routed through the host network stack.
migration_ip_forwarding:: If you configured local gateway mode, set IP forwarding to Global if you need the host network of the node to act as a router for traffic not related to OVN-Kubernetes.
migration_cidr:: Specifies a Classless Inter-Domain Routing (CIDR) IP address block for your cluster. You cannot use any CIDR block that overlaps the 100.64.0.0/16 CIDR block, because the OVN-Kubernetes network provider uses this block internally.
migration_prefix:: Ensure that you specify a prefix value, which is the slice of the CIDR block apportioned to each node in your cluster.
migration_mtu:: Optional parameter that sets a specific maximum transmission unit (MTU) to your cluster network after the migration process.
migration_geneve_port:: Optional parameter that sets a Geneve port for OVN-Kubernetes. The default port is 6081.
migration_ipv4_subnet:: Optional parameter that sets an IPv4 address range for internal use by OVN-Kubernetes. The default value for the parameter is 100.64.0.0/16.
7. To run the playbooks/playbook-migration.yml file, enter the following command:

```terminal
$ ansible-playbook -v playbooks/playbook-migration.yml
```


* Patching OVN-Kubernetes address ranges
* Getting started with playbooks (Red Hat Ansible Automation Platform documentation)

## Migrating to the OVN-Kubernetes network plugin by using the offline migration method

As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes. During the migration, you must reboot every node in your cluster.


[IMPORTANT]
----
While performing the migration, your cluster is unavailable and workloads might be interrupted. Perform the migration only when an interruption in service is acceptable.
----

* You have a cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.
* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the cluster-admin role.
* You have a recent backup of the etcd database.
* You can manually reboot each node.
* You checked that your cluster is in a known good state without any errors.
* You created a security group rule that allows User Datagram Protocol (UDP) packets on port 6081 for all nodes on all cloud platforms.
* You removed webhooks. Alternatively, you can set a timeout value for each webhook, which is detailed in the procedure. If you did not complete one of these tasks, your cluster might fail to schedule pods.

1. If you did not remove webhooks, set the timeout value for each webhook to 3 seconds by creating a ValidatingWebhookConfiguration custom resource and then specify the timeout value for the timeoutSeconds parameter:

```terminal
oc patch ValidatingWebhookConfiguration <webhook_name> --type='json' \ 1
-p '[{"op": "replace", "path": "/webhooks/0/timeoutSeconds", "value": 3}]'
```

Where <webhook_name> is the name of your webhook.
2. To backup the configuration for the cluster network, enter the following command:

```terminal
$ oc get Network.config.openshift.io cluster -o yaml > cluster-openshift-sdn.yaml
```

3. Verify that the  OVN_SDN_MIGRATION_TIMEOUT environment variable is set and is equal to 0s by running the following command:

```bash
#!/bin/bash

if [ -n "$OVN_SDN_MIGRATION_TIMEOUT" ] && [ "$OVN_SDN_MIGRATION_TIMEOUT" = "0s" ]; then
    unset OVN_SDN_MIGRATION_TIMEOUT
fi

#loops the timeout command of the script to repeatedly check the cluster Operators until all are available.

co_timeout=${OVN_SDN_MIGRATION_TIMEOUT:-1200s}
timeout "$co_timeout" bash <<EOT
until
  oc wait co --all --for='condition=AVAILABLE=True' --timeout=10s && \
  oc wait co --all --for='condition=PROGRESSING=False' --timeout=10s && \
  oc wait co --all --for='condition=DEGRADED=False' --timeout=10s;
do
  sleep 10
  echo "Some ClusterOperators Degraded=False,Progressing=True,or Available=False";
done
EOT
```

4. Remove the configuration from the Cluster Network Operator (CNO) configuration object by running the following command:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
--patch '{"spec":{"migration":null}}'
```

5. Delete the NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) that defines the primary network interface for the OpenShift SDN network plugin by completing the following steps:
1. Check that the existing NNCP CR bonded the primary interface to your cluster by entering the following command:

```terminal
$ oc get nncp
```

Example output

```terminal
NAME          STATUS      REASON
bondmaster0   Available   SuccessfullyConfigured
```


Network Manager stores the connection profile for the bonded primary interface in the /etc/NetworkManager/system-connections system path.
2. Remove the NNCP from your cluster:

```terminal
$ oc delete nncp <nncp_manifest_filename>
```

6. To prepare all the nodes for the migration, set the migration field on the CNO configuration object by running the following command:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'
```


[NOTE]
----
This step does not deploy OVN-Kubernetes immediately. Instead, specifying the migration field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.
----
1. Check that the reboot is finished by running the following command:

```terminal
$ oc get mcp
```

2. Check that all cluster Operators are available by running the following command:

```terminal
$ oc get co
```

3. Alternatively: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:
* Egress IPs
* Egress firewall
* Multicast

To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": <bool>,
          "egressFirewall": <bool>,
          "multicast": <bool>
        }
      }
    }
  }'
```


where:
bool: Specifies whether to enable migration of the feature. The default is true.
7. Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:
* Maximum transmission unit (MTU). Consider the following before customizing the MTU for this optional step:
* If you use the default MTU, and you want to keep the default MTU during migration, this step can be ignored.
* If you used a custom MTU, and you want to keep the custom MTU during migration, you must declare the custom MTU value in this step.
* This step does not work if you want to change the MTU value during migration. Instead, you must first follow the instructions for "Changing the cluster MTU". You can then keep the custom MTU value by performing this procedure and declaring the custom MTU value in this step.

[NOTE]
----
OpenShift-SDN and OVN-Kubernetes have different overlay overhead. MTU values should be selected by following the guidelines found on the "MTU value selection" page.
----
* Geneve (Generic Network Virtualization Encapsulation) overlay network port
* OVN-Kubernetes IPv4 internal subnet
* The gateway mode and IP forwarding
* If your cluster uses static routes or routing policies in the host network, set the routingViaHost spec to true and the ipForwarding spec to Global in the gatewayConfig object.

To customize any of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.

```terminal
$ oc patch Network.operator.openshift.io cluster --type=merge \
--patch '{
  "spec": {
    "defaultNetwork": {
      "ovnKubernetesConfig": {
        "mtu": <mtu>,
        "genevePort": <port>,
        "ipv4":
          "InternalJoinSubnet": "<ipv4_subnet>",
        "gatewayConfig": {
          "ipForwarding": "<ip_forwarding>",
          "routingViaHost": <gateway_mode>
        }}}}}'
```


where:
mtu:: The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to 100 less than the smallest node MTU value.
port:: The UDP port for the Geneve overlay network. If a value is not specified, the default is 6081. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is 4789.
ipv4_subnet:: An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your Red Hat OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is 100.64.0.0/16.
routingViaHost:: A value of true sets local gateway mode and a value of false sets shared gateway mode. The default value is false. In local gateway mode, traffic is routed through the host network stack. In shared gateway mode, traffic is not routed through the host network stack.
ipForwarding::: Set IP forwarding to Global alongside the local gateway mode if you need the host network of the node to act as a router for traffic not related to OVN-Kubernetes.
Example oc patch command to update mtu field

```terminal
$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'
```

Example oc patch command to update the gatewayConfig object

```terminal
$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "gatewayConfig": {
            "ipForwarding": "Global",
            "routingViaHost": true
    }}}}}'
```

8. As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:

```terminal
$ oc get mcp
```


A successfully updated node has the following status: UPDATED=true, UPDATING=false, DEGRADED=false.

[NOTE]
----
By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
----
9. Confirm the status of the new machine configuration on the hosts:
1. To list the machine configuration state and the name of the applied machine configuration, enter the following command:

```terminal
$ oc describe node | egrep "hostname|machineconfig"
```

Example output

```terminal
kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
```


Verify that the following statements are true:
* The value of machineconfiguration.openshift.io/state field is Done.
* The value of the machineconfiguration.openshift.io/currentConfig field is equal to the value of the machineconfiguration.openshift.io/desiredConfig field.
2. To confirm that the machine config is correct, enter the following command:

```terminal
$ oc get machineconfig <config_name> -o yaml | grep ExecStart
```


where <config_name> is the name of the machine config from the machineconfiguration.openshift.io/currentConfig field.

The machine config can include the following update to the systemd configuration:

```plain
ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes
```

3. If a node is stuck in the NotReady state, investigate the machine config daemon pod logs and resolve any errors.
1. To list the pods, enter the following command:

```terminal
$ oc get pod -n openshift-machine-config-operator
```

Example output

```terminal
NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h
```


The names for the config daemon pods are in the following format: machine-config-daemon-<seq>. The <seq> value is a random five character alphanumeric sequence.
2. Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:

```terminal
$ oc logs <pod> -n openshift-machine-config-operator
```


where pod is the name of a machine config daemon pod.
3. Resolve any errors in the logs shown by the output from the previous command.
10. To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:
* To specify the network provider without changing the cluster network IP address block, enter the following command:

```terminal
$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'
```

* To specify a different cluster network IP address block, enter the following command:

```terminal
$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "<cidr>",
          "hostPrefix": <prefix>
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'
```


where cidr is a CIDR block and prefix is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the 100.64.0.0/16 CIDR block because the OVN-Kubernetes network provider uses this block internally.

[IMPORTANT]
----
You cannot change the service network address block during the migration.
----
11. Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:

```terminal
$ oc -n openshift-multus rollout status daemonset/multus
```


The name of the Multus pods is in the form of multus-<xxxxx> where <xxxxx> is a random sequence of letters. It might take several moments for the pods to restart.
Example output

```text
Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out
```

12. To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:

[IMPORTANT]
----
The following scripts reboot all of the nodes in the cluster at the same time. This can cause your cluster to be unstable. Another option is to reboot your nodes manually one at a time. Rebooting nodes one-by-one causes considerable downtime in a cluster with many nodes.
Cluster Operators will not work correctly before you reboot the nodes.
----
* With the oc rsh command, you can use a bash script similar to the following:

```bash
#!/bin/bash
readarray -t POD_NODES <<< "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE <<< "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" && sleep 3
    done
done
```

* With the ssh command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.

```bash
#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done
```

13. Confirm that the migration succeeded:
1. To confirm that the network plugin is OVN-Kubernetes, enter the following command.  The value of status.networkType must be OVNKubernetes.

```terminal
$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'
```

2. To confirm that the cluster nodes are in the Ready state, enter the following command:

```terminal
$ oc get nodes
```

3. To confirm that your pods are not in an error state, enter the following command:

```terminal
$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'
```


If pods on a node are in an error state, reboot that node.
4. To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:

```terminal
$ oc get co
```


The status of every cluster Operator must be the following: AVAILABLE="True", PROGRESSING="False", DEGRADED="False". If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.
14. Complete the following steps only if the migration succeeds and your cluster is in a good state:
1. To remove the migration configuration from the CNO configuration object, enter the following command:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'
```

2. To remove custom configuration for the OpenShift SDN network provider, enter the following command:

```terminal
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'
```

3. To remove the OpenShift SDN network provider namespace, enter the following command:

```terminal
$ oc delete namespace openshift-sdn
```

4. After a successful migration operation, remove the network.openshift.io/network-type-migration- annotation from the network.config custom resource by entering the following command:

```terminal
$ oc annotate network.config cluster network.openshift.io/network-type-migration-
```


* Optional: After cluster migration, you can convert your IPv4 single-stack cluster to a dual-network cluster network that supports IPv4 and IPv6 address families. For more information, see "Converting to IPv4/IPv6 dual-stack networking".

# Understanding changes to external IP behavior in OVN-Kubernetes

When migrating from OpenShift SDN to OVN-Kubernetes (OVN-K), services that use external IPs might become inaccessible across namespaces due to network policy enforcement.

In OpenShift SDN, external IPs were accessible across namespaces by default. However, in OVN-K, network policies strictly enforce multitenant isolation, preventing access to services exposed via external IPs from other namespaces.

To ensure access, consider the following alternatives:

* Use an ingress or route: Instead of exposing services by using external IPs, configure an ingress or route to allow external access while maintaining security controls.
* Adjust the NetworkPolicy custom resource (CR): Modify a NetworkPolicy CR to explicitly allow access from required namespaces and ensure that traffic is allowed to the designated service ports. Without explicitly allowing traffic to the required ports, access might still be blocked, even if the namespace is allowed.
* Use a LoadBalancer service: If applicable, deploy a LoadBalancer service instead of relying on external IPs. For more information about configuring see "NetworkPolicy and external IPs in OVN-Kubernetes".

* NetworkPolicy and external IPs in OVN-Kubernetes.