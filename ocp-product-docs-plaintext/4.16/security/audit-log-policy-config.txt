# Configuring the audit log policy


You can control the amount of information that is logged to the API server audit logs by choosing the audit log policy profile to use.

# About audit log policy profiles

Audit log profiles define how to log requests that come to the OpenShift API server, Kubernetes API server, OpenShift OAuth API server, and OpenShift OAuth server.

Red Hat OpenShift Container Platform provides the following predefined audit policy profiles:



1. Sensitive resources, such as Secret, Route, and OAuthClient objects, are only logged at the metadata level.
OpenShift OAuth server events are only logged at the metadata level.

By default, Red Hat OpenShift Container Platform uses the Default audit log profile. You can use another audit policy profile that also logs request bodies, but be aware of the increased resource usage such as CPU, memory, and I/O.

# Configuring the audit log policy

You can configure the audit log policy to use when logging requests that come to the API servers.

* You have access to the cluster as a user with the cluster-admin role.

1. Edit the APIServer resource:

```terminal
$ oc edit apiserver cluster
```

2. Update the spec.audit.profile field:

```yaml
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
...
spec:
  audit:
    profile: WriteRequestBodies    1
```

Set to Default, WriteRequestBodies, AllRequestBodies, or None. The default profile is Default.

[WARNING]
----
It is not recommended to disable audit logging by using the None profile unless you are fully aware of the risks of not logging data that can be beneficial when troubleshooting issues. If you disable audit logging and a support situation arises, you might need to enable audit logging and reproduce the issue in order to troubleshoot properly.
----
3. Save the file to apply the changes.

* Verify that a new revision of the Kubernetes API server pods is rolled out. It can take several minutes for all nodes to update to the new revision.

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition for the Kubernetes API server to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 12 1
```

In this example, the latest revision number is 12.

If the output shows a message similar to one of the following messages, the update is still in progress. Wait a few minutes and try again.
* 3 nodes are at revision 11; 0 nodes have achieved new revision 12
* 2 nodes are at revision 11; 1 nodes are at revision 12

# Configuring the audit log policy with custom rules

You can configure an audit log policy that defines custom rules. You can specify multiple groups and define which profile to use for that group.

These custom rules take precedence over the top-level profile field. The custom rules are evaluated from top to bottom, and the first that matches is applied.


[IMPORTANT]
----
If you set the top-level profile field to None, an API server, such as the Kubernetes API server, ignores custom rules and disables audit logging.
----

* You have access to the cluster as a user with the cluster-admin role.

1. Edit the APIServer resource:

```terminal
$ oc edit apiserver cluster
```

2. Add the spec.audit.customRules field:

```yaml
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
...
spec:
  audit:
    customRules:                        1
    - group: system:authenticated:oauth
      profile: WriteRequestBodies
    - group: system:authenticated
      profile: AllRequestBodies
    profile: Default                    2
```

Add one or more groups and specify the profile to use for that group. These custom rules take precedence over the top-level profile field. The custom rules are evaluated from top to bottom, and the first that matches is applied.
Set to Default, WriteRequestBodies, or AllRequestBodies. If you do not set this top-level profile field, it defaults to the Default profile.
3. Save the file to apply the changes.

* Verify that a new revision of the Kubernetes API server pods is rolled out. It can take several minutes for all nodes to update to the new revision.

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition for the Kubernetes API server to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 12 1
```

In this example, the latest revision number is 12.

If the output shows a message similar to one of the following messages, the update is still in progress. Wait a few minutes and try again.
* 3 nodes are at revision 11; 0 nodes have achieved new revision 12
* 2 nodes are at revision 11; 1 nodes are at revision 12

# Disabling audit logging

You can disable audit logging for Red Hat OpenShift Container Platform. When you disable audit logging, even OAuth access token requests and OAuth authorize token requests are not logged.


[WARNING]
----
It is not recommended to disable audit logging by using the None profile unless you are fully aware of the risks of not logging data that can be beneficial when troubleshooting issues. If you disable audit logging and a support situation arises, you might need to enable audit logging and reproduce the issue in order to troubleshoot properly.
----

* You have access to the cluster as a user with the cluster-admin role.

1. Edit the APIServer resource:

```terminal
$ oc edit apiserver cluster
```

2. Set the spec.audit.profile field to None:

```yaml
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
...
spec:
  audit:
    profile: None
```


[NOTE]
----
You can also disable audit logging only for specific groups by specifying custom rules in the spec.audit.customRules field.
----
3. Save the file to apply the changes.

* Verify that a new revision of the Kubernetes API server pods is rolled out. It can take several minutes for all nodes to update to the new revision.

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
```


Review the NodeInstallerProgressing status condition for the Kubernetes API server to verify that all nodes are at the latest revision. The output shows AllNodesAtLatestRevision upon successful update:

```terminal
AllNodesAtLatestRevision
3 nodes are at revision 12 1
```

In this example, the latest revision number is 12.

If the output shows a message similar to one of the following messages, the update is still in progress. Wait a few minutes and try again.
* 3 nodes are at revision 11; 0 nodes have achieved new revision 12
* 2 nodes are at revision 11; 1 nodes are at revision 12