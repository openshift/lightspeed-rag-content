# Exporters


Exporters send data to one or more back ends or destinations. An exporter can be push or pull based. By default, no exporters are configured. One or more exporters must be configured. Exporters can support one or more data sources. Exporters might be used with their default settings, but many exporters require configuration to specify at least the destination and security settings.
Currently, the following General Availability and Technology Preview exporters are available for the Red Hat build of OpenTelemetry:
* OTLP Exporter
* OTLP HTTP Exporter
* Debug Exporter
* Load Balancing Exporter
* Prometheus Exporter
* Prometheus Remote Write Exporter
* Kafka Exporter
* AWS CloudWatch Exporter
* AWS EMF Exporter
* AWS X-Ray Exporter
* File Exporter

# OTLP Exporter

The OTLP gRPC Exporter exports traces and metrics by using the OpenTelemetry protocol (OTLP).


```yaml
# ...
  config:
    exporters:
      otlp:
        endpoint: tempo-ingester:4317 1
        tls: 2
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
          insecure: false 3
          insecure_skip_verify: false # 4
          reload_interval: 1h 5
          server_name_override: <name> 6
        headers: 7
          X-Scope-OrgID: "dev"
    service:
      pipelines:
        traces:
          exporters: [otlp]
        metrics:
          exporters: [otlp]
# ...
```


The OTLP gRPC endpoint. If the https:// scheme is used, then client transport security is enabled and overrides the insecure setting in the tls.
The client-side TLS configuration. Defines paths to TLS certificates.
Disables client transport security when set to true. The default value is false by default.
Skips verifying the certificate when set to true. The default value is false.
Specifies the time interval at which the certificate is reloaded. If the value is not set, the certificate is never reloaded. The reload_interval accepts a string containing valid units of time such as ns, us (or Âµs), ms, s, m, h.
Overrides the virtual host name of authority such as the authority header field in requests. You can use this for testing.
Headers are sent for every request performed during an established connection.

# OTLP HTTP Exporter

The OTLP HTTP Exporter exports traces and metrics by using the OpenTelemetry protocol (OTLP).


```yaml
# ...
  config:
    exporters:
      otlphttp:
        endpoint: http://tempo-ingester:4318 1
        tls: 2
        headers: 3
          X-Scope-OrgID: "dev"
        disable_keep_alives: false 4

    service:
      pipelines:
        traces:
          exporters: [otlphttp]
        metrics:
          exporters: [otlphttp]
# ...
```


The OTLP HTTP endpoint. If the https:// scheme is used, then client transport security is enabled and overrides the insecure setting in the tls.
The client side TLS configuration. Defines paths to TLS certificates.
Headers are sent in every HTTP request.
If true, disables HTTP keep-alives. It will only use the connection to the server for a single HTTP request.

# Debug Exporter

The Debug Exporter prints traces and metrics to the standard output.


```yaml
# ...
  config:
    exporters:
      debug:
        verbosity: detailed 1
        sampling_initial: 5 2
        sampling_thereafter: 200 3
        use_internal_logger: true 4
    service:
      pipelines:
        traces:
          exporters: [debug]
        metrics:
          exporters: [debug]
# ...
```


Verbosity of the debug export: detailed, normal, or basic. When set to detailed, pipeline data are verbosely logged. Defaults to normal.
Initial number of messages logged per second. The default value is 2 messages per second.
Sampling rate after the initial number of messages, the value in sampling_initial, has been logged. Disabled by default with the default 1 value. Sampling is enabled with values greater than 1. For more information, see the page for the sampler function in the zapcore package on the Go Project's website.
When set to true, enables output from the Collector's internal logger for the exporter.

# Load Balancing Exporter

The Load Balancing Exporter consistently exports spans, metrics, and logs according to the routing_key configuration.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      loadbalancing:
        routing_key: "service" 1
        protocol:
          otlp: 2
            timeout: 1s
        resolver: 3
          static: 4
            hostnames:
            - backend-1:4317
            - backend-2:4317
          dns: 5
            hostname: otelcol-headless.observability.svc.cluster.local
          k8s: 6
            service: lb-svc.kube-public
            ports:
              - 15317
              - 16317
# ...
```


The routing_key: service exports spans for the same service name to the same Collector instance to provide accurate aggregation. The routing_key: traceID exports spans based on their traceID. The implicit default is traceID based routing.
The OTLP is the only supported load-balancing protocol. All options of the OTLP exporter are supported.
You can configure only one resolver.
The static resolver distributes the load across the listed endpoints.
You can use the DNS resolver only with a Kubernetes headless service.
The Kubernetes resolver is recommended.

# Prometheus Exporter

The Prometheus Exporter exports metrics in the Prometheus or OpenMetrics formats.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      prometheus:
        endpoint: 0.0.0.0:8889 1
        tls: 2
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
        namespace: prefix 3
        const_labels: 4
          label1: value1
        enable_open_metrics: true 5
        resource_to_telemetry_conversion: 6
          enabled: true
        metric_expiration: 180m 7
        add_metric_suffixes: false 8
    service:
      pipelines:
        metrics:
          exporters: [prometheus]
# ...
```


The network endpoint where the metrics are exposed. The Red Hat build of OpenTelemetry Operator automatically exposes the port specified in the endpoint field to the <instance_name>-collector service.
The server-side TLS configuration. Defines paths to TLS certificates.
If set, exports metrics under the provided value.
Key-value pair labels that are applied for every exported metric.
If true, metrics are exported by using the OpenMetrics format. Exemplars are only exported in the OpenMetrics format and only for histogram and monotonic sum metrics such as counter. Disabled by default.
If enabled is true, all the resource attributes are converted to metric labels. Disabled by default.
Defines how long metrics are exposed without updates. The default is 5m.
Adds the metrics types and units suffixes. Must be disabled if the monitor tab in the Jaeger console is enabled. The default is true.


[NOTE]
----
When the spec.observability.metrics.enableMetrics field in the OpenTelemetryCollector custom resource (CR) is set to true, the OpenTelemetryCollector CR automatically creates a Prometheus ServiceMonitor or PodMonitor CR to enable Prometheus to scrape your metrics.
----

# Prometheus Remote Write Exporter

The Prometheus Remote Write Exporter exports metrics to compatible back ends.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      prometheusremotewrite:
        endpoint: "https://my-prometheus:7900/api/v1/push" 1
        tls: 2
          ca_file: ca.pem
          cert_file: cert.pem
          key_file: key.pem
        target_info: true 3
        export_created_metric: true 4
        max_batch_size_bytes: 3000000 5
    service:
      pipelines:
        metrics:
          exporters: [prometheusremotewrite]
# ...
```


Endpoint for sending the metrics.
Server-side TLS configuration. Defines paths to TLS certificates.
When set to true, creates a target_info metric for each resource metric.
When set to true, exports a _created metric for the Summary, Histogram, and Monotonic Sum metric points.
Maximum size of the batch of samples that is sent to the remote write endpoint. Exceeding this value results in batch splitting. The default value is 3000000, which is approximately 2.861 megabytes.


[WARNING]
----
* This exporter drops non-cumulative monotonic, histogram, and summary OTLP metrics.
* You must enable the --web.enable-remote-write-receiver feature flag on the remote Prometheus instance. Without it, pushing the metrics to the instance using this exporter fails.
----

# Kafka Exporter

The Kafka Exporter exports logs, metrics, and traces to Kafka. This exporter uses a synchronous producer that blocks and does not batch messages. You must use it with batch and queued retry processors for higher throughput and resiliency.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      kafka:
        brokers: ["localhost:9092"] 1
        protocol_version: 2.0.0 2
        topic: otlp_spans 3
        auth:
          plain_text: 4
            username: example
            password: example
          tls: 5
            ca_file: ca.pem
            cert_file: cert.pem
            key_file: key.pem
            insecure: false 6
            server_name_override: kafka.example.corp 7
    service:
      pipelines:
        traces:
          exporters: [kafka]
# ...
```


The list of Kafka brokers. The default is localhost:9092.
The Kafka protocol version. For example, 2.0.0. This is a required field.
The name of the Kafka topic to read from. The following are the defaults: otlp_spans for traces, otlp_metrics for metrics, otlp_logs for logs.
The plain text authentication configuration. If omitted, plain text authentication is disabled.
The client-side TLS configuration. Defines paths to the TLS certificates. If omitted, TLS authentication is disabled.
Disables verifying the server's certificate chain and host name. The default is false.
ServerName indicates the name of the server requested by the client to support virtual hosting.

# AWS CloudWatch Logs Exporter

The AWS CloudWatch Logs Exporter sends logs data to the Amazon CloudWatch Logs service and signs requests by using the AWS SDK for Go and the default credential provider chain.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      awscloudwatchlogs:
        log_group_name: "<group_name_of_amazon_cloudwatch_logs>" 1
        log_stream_name: "<log_stream_of_amazon_cloudwatch_logs>" 2
        region: <aws_region_of_log_stream> 3
        endpoint: <service_endpoint_of_amazon_cloudwatch_logs> 4
        log_retention: <supported_value_in_days> 5
# ...
```


Required. If the log group does not exist yet, it is automatically created.
Required. If the log stream does not exist yet, it is automatically created.
Optional. If the AWS region is not already set in the default credential chain, you must specify it.
Optional. You can override the default Amazon CloudWatch Logs service endpoint to which the requests are forwarded. For the list of service endpoints by region, see Amazon CloudWatch Logs endpoints and quotas (AWS General Reference).
Optional. With this parameter, you can set the log retention policy for new Amazon CloudWatch log groups. If this parameter is omitted or set to 0, the logs never expire by default. Supported values for retention in days are 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 2192, 2557, 2922, 3288, or 3653.

* What is Amazon CloudWatch Logs? (Amazon CloudWatch Logs User Guide)
* Specifying Credentials (AWS SDK for Go Developer Guide)
* Amazon CloudWatch Logs endpoints and quotas (AWS General Reference)

# AWS EMF Exporter

The AWS EMF Exporter converts the following OpenTelemetry metrics datapoints to the AWS CloudWatch Embedded Metric Format (EMF):

* Int64DataPoints
* DoubleDataPoints
* SummaryDataPoints

The EMF metrics are then sent directly to the Amazon CloudWatch Logs service by using the PutLogEvents API.

One of the benefits of using this exporter is the possibility to view logs and metrics in the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      awsemf:
        log_group_name: "<group_name_of_amazon_cloudwatch_logs>" 1
        log_stream_name: "<log_stream_of_amazon_cloudwatch_logs>" 2
        resource_to_telemetry_conversion: 3
          enabled: true
        region: <region> 4
        endpoint: <endpoint> 5
        log_retention: <supported_value_in_days> 6
        namespace: <custom_namespace> 7
# ...
```


Customized log group name.
Customized log stream name.
Optional. Converts resource attributes to telemetry attributes such as metric labels. Disabled by default.
The AWS region of the log stream. If a region is not already set in the default credential provider chain, you must specify the region.
Optional. You can override the default Amazon CloudWatch Logs service endpoint to which the requests are forwarded. For the list of service endpoints by region, see Amazon CloudWatch Logs endpoints and quotas (AWS General Reference).
Optional. With this parameter, you can set the log retention policy for new Amazon CloudWatch log groups. If this parameter is omitted or set to 0, the logs never expire by default. Supported values for retention in days are 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 2192, 2557, 2922, 3288, or 3653.
Optional. A custom namespace for the Amazon CloudWatch metrics.

## Log group name

The log_group_name parameter allows you to customize the log group name and supports the default /metrics/default value or the following placeholders:

/aws/metrics/{ClusterName}:: This placeholder is used to search for the ClusterName or aws.ecs.cluster.name resource attribute in the metrics data and replace it with the actual cluster name.
{NodeName}:: This placeholder is used to search for the NodeName or k8s.node.name resource attribute.
{TaskId}:: This placeholder is used to search for the TaskId or aws.ecs.task.id resource attribute.

If no resource attribute is found in the resource attribute map, the placeholder is replaced by the undefined value.

## Log stream name

The log_stream_name parameter allows you to customize the log stream name and supports the default otel-stream value or the following placeholders:

{ClusterName}:: This placeholder is used to search for the ClusterName or aws.ecs.cluster.name resource attribute.
{ContainerInstanceId}:: This placeholder is used to search for the ContainerInstanceId or aws.ecs.container.instance.id resource attribute. This resource attribute is valid only for the AWS ECS EC2 launch type.
{NodeName}:: This placeholder is used to search for the NodeName or k8s.node.name resource attribute.
{TaskDefinitionFamily}:: This placeholder is used to search for the TaskDefinitionFamily or aws.ecs.task.family resource attribute.
{TaskId}:: This placeholder is used to search for the TaskId or aws.ecs.task.id resource attribute in the metrics data and replace it with the actual task ID.

If no resource attribute is found in the resource attribute map, the placeholder is replaced by the undefined value.

* Specification: Embedded metric format (Amazon CloudWatch User Guide)
* PutLogEvents (Amazon CloudWatch Logs API Reference)
* Amazon CloudWatch Logs endpoints and quotas (AWS General Reference)

# AWS X-Ray Exporter

The AWS X-Ray Exporter converts OpenTelemetry spans to AWS X-Ray Segment Documents and then sends them directly to the AWS X-Ray service. The AWS X-Ray Exporter uses the PutTraceSegments API and signs requests by using the AWS SDK for Go and the default credential provider chain.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config:
    exporters:
      awsxray:
        region: "<region>" 1
        endpoint: <endpoint> 2
        resource_arn: "<aws_resource_arn>" 3
        role_arn: "<iam_role>" 4
        indexed_attributes: [ "<indexed_attr_0>", "<indexed_attr_1>" ] 5
        aws_log_groups: ["<group1>", "<group2>"] 6
        request_timeout_seconds: 120 7
# ...
```


The destination region for the X-Ray segments sent to the AWS X-Ray service. For example, eu-west-1.
Optional. You can override the default Amazon CloudWatch Logs service endpoint to which the requests are forwarded. For the list of service endpoints by region, see Amazon CloudWatch Logs endpoints and quotas (AWS General Reference).
The Amazon Resource Name (ARN) of the AWS resource that is running the Collector.
The AWS Identity and Access Management (IAM) role for uploading the X-Ray segments to a different account.
The list of attribute names to be converted to X-Ray annotations.
The list of log group names for Amazon CloudWatch Logs.
Time duration in seconds before timing out a request. If omitted, the default value is 30.

* What is AWS X-Ray? (AWS X-Ray Developer Guide)
* AWS SDK for Go API Reference (AWS Documentation)
* Specifying Credentials (AWS SDK for Go Developer Guide)
* IAM roles (AWS Identity and Access Management User Guide)

# File Exporter

The File Exporter writes telemetry data to files in persistent storage and supports file operations such as rotation, compression, and writing to multiple files. With this exporter, you can also use a resource attribute to control file naming. The only required setting is path, which specifies the destination path for telemetry files in the persistent-volume file system.


[IMPORTANT]
----
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


```yaml
# ...
  config: |
    exporters:
      file:
        path: /data/metrics.json 1
        rotation: 2
          max_megabytes: 10 3
          max_days: 3 4
          max_backups: 3 5
          localtime: true 6
        format: proto 7
        compression: zstd 8
        flush_interval: 5 9
# ...
```


The file-system path where the data is to be written. There is no default.
File rotation is an optional feature of this exporter. By default, telemetry data is exported to a single file. Add the rotation setting to enable file rotation.
The max_megabytes setting is the maximum size a file is allowed to reach until it is rotated. The default is 100.
The max_days setting is for how many days a file is to be retained, counting from the timestamp in the file name. There is no default.
The max_backups setting is for retaining several older files. The defalt is 100.
The localtime setting specifies the local-time format for the timestamp, which is appended to the file name in front of any extension, when the file is rotated. The default is the Coordinated Universal Time (UTC).
The format for encoding the telemetry data before writing it to a file. The default format is json. The proto format is also supported.
File compression is optional and not set by default. This setting defines the compression algorithm for the data that is exported to a file. Currently, only the zstd compression algorithm is supported. There is no default.
The time interval between flushes. A value without a unit is set in nanoseconds. This setting is ignored when file rotation is enabled through the rotation settings.

# Additional resources

* OpenTelemetry Protocol (OTLP) documentation