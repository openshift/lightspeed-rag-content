# Managing automatic boot source updates


You can manage automatic updates for the following boot sources:
* All Red Hat boot sources
* All custom boot sources
* Individual Red Hat or custom boot sources
Boot sources can make virtual machine (VM) creation more accessible and efficient for users. If automatic boot source updates are enabled, the Containerized Data Importer (CDI) imports, polls, and updates the images so that they are ready to be cloned for new VMs. By default, CDI automatically updates Red Hat boot sources.

# Managing Red Hat boot source updates

You can opt out of automatic updates for all system-defined boot sources by disabling the enableCommonBootImageImport feature gate. If you disable this feature gate, all DataImportCron objects are deleted. This does not remove previously imported boot source objects that store operating system images, though administrators can delete them manually.

When the enableCommonBootImageImport feature gate is disabled, DataSource objects are reset so that they no longer point to the original boot source. An administrator can manually provide a boot source by creating a new persistent volume claim (PVC) or volume snapshot for the DataSource object, then populating it with an operating system image.

## Managing automatic updates for all system-defined boot sources

Disabling automatic boot source imports and updates can lower resource usage. In disconnected environments, disabling automatic boot source updates prevents CDIDataImportCronOutdated alerts from filling up logs.

To disable automatic updates for all system-defined boot sources, turn off the enableCommonBootImageImport feature gate by setting the value to false. Setting this value to true re-enables the feature gate and turns automatic updates back on.


[NOTE]
----
Custom boot sources are not affected by this setting.
----

* Toggle the feature gate for automatic boot source updates by editing the HyperConverged custom resource (CR).
* To disable automatic boot source updates, set the spec.featureGates.enableCommonBootImageImport field in the HyperConverged CR to false. For example:

```terminal
$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", \
  "value": false}]'
```

* To re-enable automatic boot source updates, set the spec.featureGates.enableCommonBootImageImport field in the HyperConverged CR to true. For example:

```terminal
$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", \
  "value": true}]'
```


# Managing custom boot source updates

Custom boot sources that are not provided by OpenShift Virtualization are not controlled by the feature gate. You must manage them individually by editing the HyperConverged custom resource (CR).


[IMPORTANT]
----
You must configure a storage class. Otherwise, the cluster cannot receive automated updates for custom boot sources. See Defining a storage class for details.
----

## Configuring the default and virt-default storage classes

A storage class determines how persistent storage is provisioned for workloads. In OpenShift Virtualization, the virt-default storage class takes precedence over the cluster default storage class and is used specifically for virtualization workloads. Only one storage class should be set as virt-default or cluster default at a time. If multiple storage classes are marked as default, the virt-default storage class overrides the cluster default. To ensure consistent behavior, configure only one storage class as the default for virtualization workloads.


[IMPORTANT]
----
Boot sources are created using the default storage class. If your cluster does not have a default storage class, you must define one.
----

1. Patch the current virt-default or a cluster default storage class to false:
1. Identify all storage classes currently marked as virt-default by running the following command:

```terminal
$ oc get sc -o json| jq '.items[].metadata|select(.annotations."storageclass.kubevirt.io/is-default-virt-class"=="true")|.name'
```

2. For each storage class returned, remove the virt-default annotation by running the following command:

```terminal
$ oc patch storageclass <storage_class_name> -p '{"metadata": {"annotations": {"storageclass.kubevirt.io/is-default-virt-class": "false"}}}'
```

3. Identify all storage classes currently marked as cluster default by running the following command:

```terminal
$ oc get sc -o json| jq '.items[].metadata|select(.annotations."storageclass.kubernetes.io/is-default-class"=="true")|.name'
```

4. For each storage class returned, remove the cluster default annotation by running the following command:

```terminal
$ oc patch storageclass <storage_class_name> -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}'
```

2. Set a new default storage class:
1. Assign the virt-default role to a storage class by running the following command:

```terminal
$ oc patch storageclass <storage_class_name> -p '{"metadata": {"annotations": {"storageclass.kubevirt.io/is-default-virt-class": "true"}}}'
```

2. Alternatively, assign the cluster default role to a storage class by running the following command:

```terminal
$ oc patch storageclass <storage_class_name> -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
```

3. Delete any outdated DataVolume and VolumeSnapshot objects from the openshift-virtualization-os-images namespace by running the following command.

```terminal
$ oc delete DataVolume,VolumeSnapshot -n openshift-virtualization-os-images --selector=cdi.kubevirt.io/dataImportCron
```

4. Wait for all DataSource objects to reach a "Ready - True" status. Data sources can reference either a PersistentVolumeClaim (PVC) or a VolumeSnapshot. To check the expected source format, run the following command:

```terminal
$ oc get storageprofile <storage_class_name> -o json | jq .status.dataImportCronSourceFormat
```


## Configuring a storage class for boot source images

You can configure a specific storage class in the HyperConverged resource.


[IMPORTANT]
----
To ensure stable behavior and avoid unnecessary re-importing, you can specify the storageClassName in the dataImportCronTemplates section of the HyperConverged resource.
----

1. Open the HyperConverged CR in your default editor by running the following command:

```terminal
$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
```

2. Add the dataImportCronTemplate to the spec section of the HyperConverged resource and set the storageClassName:

```yaml
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      name: rhel9-image-cron
    spec:
      template:
        spec:
          storage:
            storageClassName: <storage_class> 1
      schedule: "0 */12 * * *" 2
      managedDataSource: <data_source> 3
# ...
```

Define the storage class.
Required: Schedule for the job specified in cron format.
Required: The data source to use.

[NOTE]
----
For the custom image to be detected as an available boot source, the value of the spec.dataVolumeTemplates.spec.sourceRef.name parameter in the VM template must match this value.
----
3. Wait for the HyperConverged Operator (HCO) and Scheduling, Scale, and Performance (SSP) resources to complete reconciliation.
4. Delete any outdated DataVolume and VolumeSnapshot objects from the openshift-virtualization-os-images namespace.

```terminal
$ oc delete DataVolume,VolumeSnapshot -n openshift-virtualization-os-images --selector=cdi.kubevirt.io/dataImportCron
```

5. Wait for all DataSource objects to reach a "Ready - True" status. Data sources can reference either a PersistentVolumeClaim (PVC) or a VolumeSnapshot. To check the expected source format, run the following command:

```terminal
$ oc get storageprofile <storage_class_name> -o json | jq .status.dataImportCronSourceFormat
```


## Enabling automatic updates for custom boot sources

OpenShift Virtualization automatically updates system-defined boot sources by default, but does not automatically update custom boot sources. You must manually enable automatic updates by editing the HyperConverged custom resource (CR).

* The cluster has a default storage class.

1. Open the HyperConverged CR in your default editor by running the following command:

```terminal
$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
```

2. Edit the HyperConverged CR, adding the appropriate template and boot source in the dataImportCronTemplates section. For example:
Example custom resource

```yaml
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      name: centos-stream9-image-cron
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true" 1
    spec:
      schedule: "0 */12 * * *" 2
      template:
        spec:
          source:
            registry: 3
              url: docker://quay.io/containerdisks/centos-stream:9
          storage:
            resources:
              requests:
                storage: 30Gi
      garbageCollect: Outdated
      managedDataSource: centos-stream9 4
```

This annotation is required for storage classes with volumeBindingMode set to WaitForFirstConsumer.
Schedule for the job specified in cron format.
Use to create a data volume from a registry source. Use the default pod pullMethod and not node pullMethod, which is based on the node docker cache. The node docker cache is useful when a registry image is available via Container.Image, but the CDI importer is not authorized to access it.
For the custom image to be detected as an available boot source, the name of the image's managedDataSource must match the name of the template's DataSource, which is found under spec.dataVolumeTemplates.spec.sourceRef.name in the VM template YAML file.
3. Save the file.

## Enabling volume snapshot boot sources

Enable volume snapshot boot sources by setting the parameter in the StorageProfile associated with the storage class that stores operating system base images. Although DataImportCron was originally designed to maintain only PVC sources, VolumeSnapshot sources scale better than PVC sources for certain storage types.


[NOTE]
----
Use volume snapshots on a storage profile that is proven to scale better when cloning from a single snapshot.
----

* You must have access to a volume snapshot with the operating system image.
* The storage must support snapshotting.

1. Open the storage profile object that corresponds to the storage class used to provision boot sources by running the following command:

```terminal
$ oc edit storageprofile <storage_class>
```

2. Review the dataImportCronSourceFormat specification of the StorageProfile to confirm whether or not the VM is using PVC or volume snapshot by default.
3. Edit the storage profile, if needed, by updating the dataImportCronSourceFormat specification to snapshot.
Example storage profile

```yaml
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
# ...
spec:
  dataImportCronSourceFormat: snapshot
```


1. Open the storage profile object that corresponds to the storage class used to provision boot sources.

```terminal
$ oc get storageprofile <storage_class>  -oyaml
```

2. Confirm that the dataImportCronSourceFormat specification of the StorageProfile is set to 'snapshot', and that any DataSource objects that the DataImportCron points to now reference volume snapshots.

You can now use these boot sources to create virtual machines.

# Disabling automatic updates for a single boot source

You can disable automatic updates for an individual boot source, whether it is custom or system-defined, by editing the HyperConverged custom resource (CR).

1. Open the HyperConverged CR in your default editor by running the following command:

```terminal
$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
```

2. Disable automatic updates for an individual boot source by editing the spec.dataImportCronTemplates field.
Custom boot source:: 
* Remove the boot source from the spec.dataImportCronTemplates field. Automatic updates are disabled for custom boot sources by default.
System-defined boot source:: 
1. Add the boot source to spec.dataImportCronTemplates.

[NOTE]
----
Automatic updates are enabled by default for system-defined boot sources, but these boot sources are not listed in the CR unless you add them.
----
2. Set the value of the dataimportcrontemplate.kubevirt.io/enable annotation to 'false'.
For example:

```yaml
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      annotations:
        dataimportcrontemplate.kubevirt.io/enable: 'false'
      name: rhel8-image-cron
# ...
```
3. Save the file.

# Verifying the status of a boot source

You can determine if a boot source is system-defined or custom by viewing the HyperConverged custom resource (CR).

1. View the contents of the HyperConverged CR by running the following command:

```terminal
$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o yaml
```

Example output

```yaml
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
# ...
status:
# ...
  dataImportCronTemplates:
  - metadata:
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true"
      name: centos-9-image-cron
    spec:
      garbageCollect: Outdated
      managedDataSource: centos-stream9
      schedule: 55 8/12 * * *
      template:
        metadata: {}
        spec:
          source:
            registry:
              url: docker://quay.io/containerdisks/centos-stream:9
          storage:
            resources:
              requests:
                storage: 30Gi
        status: {}
    status:
      commonTemplate: true 1
# ...
  - metadata:
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true"
      name: user-defined-dic
    spec:
      garbageCollect: Outdated
      managedDataSource: user-defined-centos-stream9
      schedule: 55 8/12 * * *
      template:
        metadata: {}
        spec:
          source:
            registry:
              pullMethod: node
              url: docker://quay.io/containerdisks/centos-stream:9
          storage:
            resources:
              requests:
                storage: 30Gi
        status: {}
    status: {} 2
# ...
```

Indicates a system-defined boot source.
Indicates a custom boot source.
2. Verify the status of the boot source by reviewing the status.dataImportCronTemplates.status field.
* If the field contains commonTemplate: true, it is a system-defined boot source.
* If the status.dataImportCronTemplates.status field has the value {}, it is a custom boot source.