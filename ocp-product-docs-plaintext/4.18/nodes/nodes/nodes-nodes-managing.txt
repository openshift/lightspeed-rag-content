# Managing nodes


Red Hat OpenShift Container Platform uses a KubeletConfig custom resource (CR) to manage the configuration of nodes. By creating an instance of a KubeletConfig object, a managed machine config is created to override setting on the node.

[NOTE]
----
Logging in to remote machines for the purpose of changing their configuration is not supported.
----

# Modifying nodes

To make configuration changes to a cluster, or machine pool, you must create a custom resource definition (CRD), or kubeletConfig object. Red Hat OpenShift Container Platform uses the Machine Config Controller to watch for changes introduced through the CRD to apply the changes to the cluster.


[NOTE]
----
Because the fields in a kubeletConfig object are passed directly to the kubelet from upstream Kubernetes, the validation of those fields is handled directly by the kubelet itself. Please refer to the relevant Kubernetes documentation for the valid values for these fields. Invalid values in the kubeletConfig object can render cluster nodes unusable.
----

1. Obtain the label associated with the static CRD, Machine Config Pool, for the type of node you want to configure.
Perform one of the following steps:
1. Check current labels of the desired machine config pool.

For example:

```terminal
$  oc get machineconfigpool  --show-labels
```

Example output

```terminal
NAME      CONFIG                                             UPDATED   UPDATING   DEGRADED   LABELS
master    rendered-master-e05b81f5ca4db1d249a1bf32f9ec24fd   True      False      False      operator.machineconfiguration.openshift.io/required-for-upgrade=
worker    rendered-worker-f50e78e1bc06d8e82327763145bfcf62   True      False      False
```

2. Add a custom label to the desired machine config pool.

For example:

```terminal
$ oc label machineconfigpool worker custom-kubelet=enabled
```

2. Create a kubeletconfig custom resource (CR) for your configuration change, as demonstrated in the following sample configuration for a custom-config CR:

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: custom-config
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: enabled
  kubeletConfig:
    podsPerCore: 10
    maxPods: 250
    systemReserved:
      cpu: 2000m
      memory: 1Gi
#...
```


where:
name:: Assign a name to CR.
custom-kubelet:: Specify the label to apply the configuration change, this is the label you added to the machine config pool.
kubeletConfig:: Specify the new value(s) you want to change.
3. Create the CR object:

```terminal
$ oc create -f <file-name>
```


For example:

```terminal
$ oc create -f master-kube-config.yaml
```


Most Kubelet Configuration options can be set by the user. The following options are not allowed to be overwritten:

* CgroupDriver
* ClusterDNS
* ClusterDomain
* StaticPodPath


[NOTE]
----
If a single node contains more than 50 images, pod scheduling might be imbalanced across nodes. This is because the list of images on a node is shortened to 50 by default. You can disable the image limit by editing the KubeletConfig object and setting the value of nodeStatusMaxImages to -1.
----

# Updating boot images

The Machine Config Operator (MCO) uses a boot image to bring up a Red Hat Enterprise Linux CoreOS (RHCOS) node. By default, Red Hat OpenShift Container Platform does not manage the boot image.

This means that the boot image in your cluster is not updated along with your cluster. For example, if your cluster was originally created with Red Hat OpenShift Container Platform 4.12, the boot image that the cluster uses to create nodes is the same 4.12 version, even if your cluster is at a later version. If the cluster is later upgraded to 4.13 or later, new nodes continue to scale with the same 4.12 image.

This process could cause the following issues:

* Extra time to start up nodes
* Certificate expiration issues
* Version skew issues

To avoid these issues, you can configure your cluster to update the boot image whenever you update your cluster. By modifying the MachineConfiguration object, you can enable this feature. Currently, the ability to update the boot image is available for only Google Cloud Platform (GCP) and Amazon Web Services (AWS) clusters and is not supported for Cluster CAPI Operator managed clusters.

To view the current boot image used in your cluster, examine a machine set:


```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: ci-ln-hmy310k-72292-5f87z-worker-a
  namespace: openshift-machine-api
spec:
# ...
  template:
# ...
    spec:
# ...
      providerSpec:
# ...
        value:
          disks:
          - autoDelete: true
            boot: true
            image: projects/rhcos-cloud/global/images/rhcos-412-85-202203181601-0-gcp-x86-64 1
# ...
```


This boot image is the same as the originally-installed Red Hat OpenShift Container Platform version, in this example Red Hat OpenShift Container Platform 4.12, regardless of the current version of the cluster. The way that the boot image is represented in the machine set depends on the platform, as the structure of the providerSpec field differs from platform to platform.

If you configure your cluster to update your boot images, the boot image referenced in your machine sets matches the current version of the cluster.

* You have enabled the TechPreviewNoUpgrade feature set by using the feature gates. For more information, see "Enabling features using feature gates" in the "Additional resources"  section.

1. Edit the MachineConfiguration object, named cluster, to enable the updating of boot images by running the following command:

```terminal
$ oc edit MachineConfiguration cluster
```

1. Optional: Configure the boot image update feature for all the machine sets:

```yaml
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: cluster
  namespace: openshift-machine-config-operator
spec:
# ...
  managedBootImages: 1
    machineManagers:
    - resource: machinesets
      apiGroup: machine.openshift.io
      selection:
        mode: All 2
```

Activates the boot image update feature.
Specifies that all the machine sets in the cluster are to be updated.
2. Optional: Configure the boot image update feature for specific machine sets:

```yaml
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: cluster
  namespace: openshift-machine-config-operator
spec:
# ...
  managedBootImages: 1
    machineManagers:
    - resource: machinesets
      apiGroup: machine.openshift.io
      selection:
        mode: Partial
          partial:
            machineResourceSelector:
              matchLabels:
                update-boot-image: "true" 2
```

Activates the boot image update feature.
Specifies that any machine set with this label is to be updated.

[TIP]
----
If an appropriate label is not present on the machine set, add a key/value pair by running a command similar to following:

```
$ oc label machineset.machine ci-ln-hmy310k-72292-5f87z-worker-a update-boot-image=true -n openshift-machine-api
```

----

1. Get the boot image version by running the following command:

```terminal
$ oc get machinesets <machineset_name> -n openshift-machine-api -o yaml
```

Example machine set with the boot image reference

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: ci-ln-77hmkpt-72292-d4pxp
    update-boot-image: "true"
  name: ci-ln-77hmkpt-72292-d4pxp-worker-a
  namespace: openshift-machine-api
spec:
# ...
  template:
# ...
    spec:
# ...
      providerSpec:
# ...
        value:
          disks:
          - autoDelete: true
            boot: true
            image: projects/rhcos-cloud/global/images/rhcos-416-92-202402201450-0-gcp-x86-64 1
# ...
```

This boot image is the same as the current Red Hat OpenShift Container Platform version.

## Disabling updated boot images

To disable the updated boot image feature, edit the MachineConfiguration object so that the machineManagers field is an empty array.


[NOTE]
----
If you are updating a Google Cloud Platform (GCP) or Amazon Web Services (AWS) cluster from Red Hat OpenShift Container Platform 4.18 to 4.19, and you have not configured the managedBootImages parameter, the update is blocked with a This cluster is GCP or AWS but lacks a boot image configuration. message. The update is blocked intentionally on GCP or AWS clusters in order to alert you that the default updated boot image behavior is changing between version 4.18 and 4.19 to enable updated boot images by default on those platforms .
To allow the update, perform one of the following tasks:
* If you want to allow the feature to be enabled, acknowledge that you are aware of the change in default behavior by patching the admin-acks config map by using the following command:

```terminal
$ oc -n openshift-config patch cm admin-acks --patch '{"data":{"ack-4.18-boot-image-opt-out-in-4.19":"true"}}' --type=merge
```

* If you do not want the updated boot image feature enabled, explicitly disable the feature by using the following procedure.
It is important to note that if you use boot images from the AWS Marketplace or the GCP Marketplace, enabling the updated boot image feature overwrites those images with a standard Red Hat Enterprise Linux CoreOS (RHCOS) image. You should explicitly disable this feature and not patch the admin-acks config map. If you accidentally enable the updated boot image feature, you can disable it by using the following procedure. Then, replace the marketplace boot images by modifying the compute machine sets, as described  in Modifying a compute machine set.
----

If you disable this feature after some nodes have been created with the new boot image version, any existing nodes retain their current boot image. Turning off this feature does not rollback the nodes or machine sets to the originally-installed boot image. The machine sets retain the boot image version that was present when the feature was enabled and is not updated again when the cluster is upgraded to a new Red Hat OpenShift Container Platform version in the future.

1. Disable updated boot images by editing the MachineConfiguration object:

```terminal
$ oc edit MachineConfiguration cluster
```

2. Make the machineManagers parameter an empty array:

```yaml
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: cluster
  namespace: openshift-machine-config-operator
spec:
# ...
  managedBootImages: 1
    machineManagers: []
```

Remove the parameters listed under machineManagers and add the [] characters to disable boot image updates.

* Modifying a compute machine set

# Configuring control plane nodes as schedulable

You can configure control plane nodes to be schedulable, meaning that new pods are allowed for placement on the control plane nodes. By default, control plane nodes are not schedulable.

You can set the control plane nodes to be schedulable, but must retain the compute nodes.


[NOTE]
----
You can deploy Red Hat OpenShift Container Platform with no compute nodes on a bare-metal cluster. In this case, the control plane nodes are marked schedulable by default.
----

You can allow or disallow control plane nodes to be schedulable by configuring the mastersSchedulable field.


[IMPORTANT]
----
When you configure control plane nodes from the default unschedulable to schedulable, additional subscriptions are required. This is because control plane nodes then become compute nodes.
----

1. Edit the schedulers.config.openshift.io resource.

```terminal
$ oc edit schedulers.config.openshift.io cluster
```

2. Configure the mastersSchedulable field.

```yaml
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  creationTimestamp: "2019-09-10T03:04:05Z"
  generation: 1
  name: cluster
  resourceVersion: "433"
  selfLink: /apis/config.openshift.io/v1/schedulers/cluster
  uid: a636d30a-d377-11e9-88d4-0a60097bee62
spec:
  mastersSchedulable: false
status: {}
#...
```


where:
mastersSchedulable:: Set to true to allow control plane nodes to be schedulable, or false to disallow control plane nodes to be schedulable.
3. Save the file to apply the changes.

# Setting SELinux booleans

Red Hat OpenShift Container Platform allows you to enable and disable an SELinux boolean on a Red Hat Enterprise Linux CoreOS (RHCOS) node. The following procedure explains how to modify SELinux booleans on nodes using the Machine Config Operator (MCO). This procedure uses container_manage_cgroup as the example boolean. You can modify this value to whichever boolean you need.

* You have installed the OpenShift CLI (`oc`).

1. Create a new YAML file with a MachineConfig object, displayed in the following example:

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-setsebool
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Set SELinux booleans
          Before=kubelet.service

          [Service]
          Type=oneshot
          ExecStart=/sbin/setsebool container_manage_cgroup=on
          RemainAfterExit=true

          [Install]
          WantedBy=multi-user.target graphical.target
        enabled: true
        name: setsebool.service
#...
```

2. Create the new MachineConfig object by running the following command:

```terminal
$ oc create -f 99-worker-setsebool.yaml
```


[NOTE]
----
Applying any changes to the MachineConfig object causes all affected nodes to gracefully reboot after the change is applied.
----

# Adding kernel arguments to nodes

In some special cases, you might want to add kernel arguments to a set of nodes in your cluster. This should only be done with caution and clear understanding of the implications of the arguments you set.


[WARNING]
----
Improper use of kernel arguments can result in your systems becoming unbootable.
----

Examples of kernel arguments you could set include:

* nosmt: Disables symmetric multithreading (SMT) in the kernel. Multithreading allows multiple logical threads for each CPU. You could consider nosmt in multi-tenant environments to reduce risks from potential cross-thread attacks. By disabling SMT, you essentially choose security over performance.
* systemd.unified_cgroup_hierarchy: Enables Linux control group version 2 (cgroup v2). cgroup v2 is the next version of the kernel control group and offers multiple improvements.

[IMPORTANT]
----
cgroup v1 is a deprecated feature. Deprecated functionality is still included in Red Hat OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
For the most recent list of major functionality that has been deprecated or removed within Red Hat OpenShift Container Platform, refer to the Deprecated and removed features section of the Red Hat OpenShift Container Platform release notes.
----
* enforcing=0: Configures Security Enhanced Linux (SELinux) to run in permissive mode. In permissive mode, the system acts as if SELinux is enforcing the loaded security policy, including labeling objects and emitting access denial entries in the logs, but it does not actually deny any operations. While not supported for production systems, permissive mode can be helpful for debugging.

[WARNING]
----
Disabling SELinux on RHCOS in production is not supported. After SELinux has been disabled on a node, it must be re-provisioned before re-inclusion in a production cluster.
----

See Kernel.org kernel parameters for a list and descriptions of kernel arguments.

In the following procedure, you create a MachineConfig object that identifies:

* A set of machines to which you want to add the kernel argument. In this case, machines with a worker role.
* Kernel arguments that are appended to the end of the existing kernel arguments.
* A label that indicates where in the list of machine configs the change is applied.

* You have cluster-admin privileges.
* Your cluster is running.

1. List existing MachineConfig objects for your Red Hat OpenShift Container Platform cluster to determine how to
label your machine config:

```terminal
$ oc get MachineConfig
```

Example output

```terminal
NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
```

2. Create a MachineConfig object file that identifies the kernel argument (for example, 05-worker-kernelarg-selinuxpermissive.yaml)

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
    - enforcing=03
```


where:
machineconfiguration.openshift.io/role:: Applies the new kernel argument only to worker nodes.
name:: Named to identify where it fits among the machine configs (05) and what it does (adds a kernel argument to configure SELinux permissive mode).
kernelArguments:: Identifies the exact kernel argument as enforcing=0.
3. Create the new machine config:

```terminal
$ oc create -f 05-worker-kernelarg-selinuxpermissive.yaml
```

4. Check the machine configs to see that the new one was added:

```terminal
$ oc get MachineConfig
```

Example output

```terminal
NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
05-worker-kernelarg-selinuxpermissive                                                         3.4.0             105s
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.4.0             33m
```

5. Check the nodes:

```terminal
$ oc get nodes
```

Example output

```terminal
NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.31.3
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.31.3
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.31.3
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.31.3
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.31.3
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.31.3
```


You can see that scheduling on each worker node is disabled as the change is being applied.
6. Check that the kernel argument worked by going to one of the worker nodes and listing
the kernel command-line arguments (in /proc/cmdline on the host):

```terminal
$ oc debug node/ip-10-0-141-105.ec2.internal
```

Example output

```terminal
Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
BOOT_IMAGE=/ostree/rhcos-... console=tty0 console=ttyS0,115200n8
rootflags=defaults,prjquota rw root=UUID=fd0... ostree=/ostree/boot.0/rhcos/16...
coreos.oem.id=qemu coreos.oem.id=ec2 ignition.platform.id=ec2 enforcing=0

sh-4.2# exit
```


You should see the enforcing=0 argument added to the other kernel arguments.

# Allowing swap memory use on nodes

You can allow workloads on the cluster nodes to use swap memory.


[IMPORTANT]
----
cgroup v1 is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----


[NOTE]
----
Swap memory support is available only for container-native virtualization (CNV) users or use cases.
----

To allow swap memory usage on your nodes, create a kubeletconfig custom resource (CR) to set the failSwapOn parameter to false.

Optionally, you can control swap memory usage by Red Hat OpenShift Container Platform workloads on those nodes by setting the swapBehavior parameter to one of the following values:

* NoSwap prevents Red Hat OpenShift Container Platform worloads from using swap memory.
* LimitedSwap allows Red Hat OpenShift Container Platform workloads that fall under the Burstable QoS class to use swap memory.

Regardless of the swapBehavior setting, any workloads that are not managed by Red Hat OpenShift Container Platform on that node can still use swap memory if the failSwapOn parameter is false.

Because the kubelet will not start in the presence of swap memory without this configuration, you must allow swap memory in Red Hat OpenShift Container Platform before enabling swap memory on the nodes. If there is no swap memory present on a node, enabling swap memory in Red Hat OpenShift Container Platform has no effect.


[WARNING]
----
Using swap memory can negatively impact workload performance and out-of-resource handling. Do not enable swap memory on control plane nodes.
----

* You have a running Red Hat OpenShift Container Platform cluster that uses version 4.10 or later.
* Your cluster is configured to use cgroup v2. Swap memory is not supported on nodes in clusters that use cgroup v1.
* You are logged in to the cluster as a user with administrative privileges.
* You have enabled the TechPreviewNoUpgrade feature set on the cluster (see Nodes -> Working with clusters -> Enabling features using feature gates).

[NOTE]
----
Enabling the TechPreviewNoUpgrade feature set cannot be undone and prevents minor version updates. These feature sets are not recommended on production clusters.
----

1. Apply a custom label to the machine config pool where you want to allow swap memory.

```terminal
$ oc label machineconfigpool worker kubelet-swap=enabled
```

2. Create a custom resource (CR) to enable and configure swap settings.

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: swap-config
spec:
  machineConfigPoolSelector:
    matchLabels:
      kubelet-swap: enabled
  kubeletConfig:
    failSwapOn: false
    memorySwap:
      swapBehavior: LimitedSwap
#...
```


where:
failSwapOn:: Set to false to enable swap memory use on the associated nodes. Set to true to disable swap memory use.
swapBehavior:: Optional: Specify the swap memory behavior for Red Hat OpenShift Container Platform pods.
* NoSwap: Red Hat OpenShift Container Platform pods cannot use swap. This is the default.
* LimitedSwap: Red Hat OpenShift Container Platform pods of Burstable QoS class only are permitted to employ swap.
3. Enable swap memory on the nodes by setting the swapaccount=1 kernel argument and configure swap memory as needed.

# About configuring parallel container image pulls

To help control bandwidth issues, you can configure the number of workload images that can be pulled at the same time.

By default, the cluster pulls images in parallel, which allows multiple workloads to pull images at the same time. Pulling multiple images in parallel can improve workload start-up time because workloads can pull needed images without waiting for each other. However, pulling too many images at the same time can use excessive network bandwidth and cause latency issues throughout your cluster.

The default setting allows unlimited simultaneous image pulls. But, you can configure the maximum number of images that can be pulled in parallel. You can also force serial image pulling, which means that only one image can be pulled at a time.

To control the number of images that can be pulled simultaneously, use a kubelet configuration to set the maxParallelImagePulls to specify a limit. Additional image pulls above this limit are held until one of the current pulls is complete.

To force serial image pulls, use a kubelet configuration to set serializeImagePulls field to true.

## Configuring parallel container image pulls

You can control the number of images that can be pulled by your workload simultaneously by using a kubelet configuration. You can set a maximum number of images that can be pulled or force workloads to pull images one at a time.

* You have a running Red Hat OpenShift Container Platform cluster.
* You are logged in to the cluster as a user with administrative privileges.

1. Apply a custom label to the machine config pool where you want to configure parallel pulls by running a command similar to the following.

```terminal
$ oc label machineconfigpool <mcp_name> parallel-pulls=set
```

2. Create a custom resource (CR) to configure parallel image pulling.

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: parallel-image-pulls
# ...
spec:
  machineConfigPoolSelector:
    matchLabels:
      parallel-pulls: set
  kubeletConfig:
    serializeImagePulls: false
    maxParallelImagePulls: 3
# ...
```


where:
serializeImagePulls:: Set to false to enable parallel image pulls. Set to true to force serial image pulling. The default is false.
maxParallelImagePulls:: Specify the maximum number of images that can be pulled in parallel. Enter a number or set to nil to specify no limit. This field cannot be set if SerializeImagePulls is true. The default is nil.
3. Create the new machine config by running a command similar to the following:

```terminal
$ oc create -f <file_name>.yaml
```


1. Check the machine configs to see that a new one was added by running the following command:

```terminal
$ oc get MachineConfig
```


```terminal
NAME                                                GENERATEDBYCONTROLLER                        IGNITIONVERSION   AGE
00-master                                           70025364a114fc3067b2e82ce47fdb0149630e4b     3.5.0             133m
00-worker                                           70025364a114fc3067b2e82ce47fdb0149630e4b     3.5.0             133m
# ...
99-parallel-generated-kubelet                       70025364a114fc3067b2e82ce47fdb0149630e4b     3.5.0             15s 1
# ...
rendered-parallel-c634a80f644740974ceb40c054c79e50  70025364a114fc3067b2e82ce47fdb0149630e4b     3.5.0             10s 2
```


where:
99-parallel-generated-kubelet:: The new machine config. In this example, the machine config is for the parallel custom machine config pool.
rendered-parallel-<sha_numnber>:: The new rendered machine config. In this example, the machine config is for the parallel custom machine config pool.
2. Check to see that the nodes in the parallel machine config pool are being updated by running the following command:

```terminal
$ oc get machineconfigpool
```


```terminal
NAME       CONFIG                                               UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
parallel   rendered-parallel-3904f0e69130d125b3b5ef0e981b1ce1   False     True       False      1              0                   0                     0                      65m
master     rendered-master-7536834c197384f3734c348c1d957c18     True      False      False      3              3                   3                     0                      140m
worker     rendered-worker-c634a80f644740974ceb40c054c79e50     True      False      False      2              2                   2                     0                      140m
```

3. When the nodes are updated, verify that the parallel pull maximum was configured:
1. Open an oc debug session to a node by running a command similar to the following:

```terminal
$ oc debug node/<node_name>
```

2. Set /host as the root directory within the debug shell by running the following command:

```terminal
sh-5.1# chroot /host
```

3. Examine the kubelet.conf file by running the following command:

```terminal
sh-5.1# cat /etc/kubernetes/kubelet.conf | grep -i maxParallelImagePulls
```


```terminal
maxParallelImagePulls: 3
```


# Migrating control plane nodes from one RHOSP host to another manually

If control plane machine sets are not enabled on your cluster, you can run a script that moves a control plane node from one Red Hat OpenStack Platform (RHOSP) node to another.


[NOTE]
----
Control plane machine sets are not enabled on clusters that run on user-provisioned infrastructure.
For information about control plane machine sets, see "Managing control plane machines with control plane machine sets".
----

* The environment variable OS_CLOUD refers to a clouds entry that has administrative credentials in a clouds.yaml file.
* The environment variable KUBECONFIG refers to a configuration that contains administrative Red Hat OpenShift Container Platform credentials.

* From a command line, run the following script:

```bash
#!/usr/bin/env bash

set -Eeuo pipefail

if [ $# -lt 1 ]; then
	echo "Usage: '$0 node_name'"
	exit 64
fi

# Check for admin OpenStack credentials
openstack server list --all-projects >/dev/null || { >&2 echo "The script needs OpenStack admin credentials. Exiting"; exit 77; }

# Check for admin OpenShift credentials
oc adm top node >/dev/null || { >&2 echo "The script needs OpenShift admin credentials. Exiting"; exit 77; }

set -x

declare -r node_name="$1"
declare server_id
server_id="$(openstack server list --all-projects -f value -c ID -c Name | grep "$node_name" | cut -d' ' -f1)"
readonly server_id

# Drain the node
oc adm cordon "$node_name"
oc adm drain "$node_name" --delete-emptydir-data --ignore-daemonsets --force

# Power off the server
oc debug "node/${node_name}" -- chroot /host shutdown -h 1

# Verify the server is shut off
until openstack server show "$server_id" -f value -c status | grep -q 'SHUTOFF'; do sleep 5; done

# Migrate the node
openstack server migrate --wait "$server_id"

# Resize the VM
openstack server resize confirm "$server_id"

# Wait for the resize confirm to finish
until openstack server show "$server_id" -f value -c status | grep -q 'SHUTOFF'; do sleep 5; done

# Restart the VM
openstack server start "$server_id"

# Wait for the node to show up as Ready:
until oc get node "$node_name" | grep -q "^${node_name}[[:space:]]\+Ready"; do sleep 5; done

# Uncordon the node
oc adm uncordon "$node_name"

# Wait for cluster operators to stabilize
until oc get co -o go-template='statuses: {{ range .items }}{{ range .status.conditions }}{{ if eq .type "Degraded" }}{{ if ne .status "False" }}DEGRADED{{ end }}{{ else if eq .type "Progressing"}}{{ if ne .status "False" }}PROGRESSING{{ end }}{{ else if eq .type "Available"}}{{ if ne .status "True" }}NOTAVAILABLE{{ end }}{{ end }}{{ end }}{{ end }}' | grep -qv '\(DEGRADED\|PROGRESSING\|NOTAVAILABLE\)'; do sleep 5; done
```


If the script completes, the control plane machine is migrated to a new RHOSP node.

# Additional resources

* Managing control plane machines with control plane machine sets