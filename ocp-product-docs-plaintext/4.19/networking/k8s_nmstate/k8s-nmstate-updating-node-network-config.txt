# Observing and updating the node network state and configuration


After you install the Kubernetes NMState Operator, you can use the Operator to observe and update your cluster's node network state and network configuration.
For more information about how to install the NMState Operator, see Kubernetes NMState Operator.

[IMPORTANT]
----
You cannot provide any configuration that modifies the br-ex bridge, an OVN-Kubernetes-managed Open vSwitch bridge. However, you can configure a customized br-ex bridge.
For more information, see "Creating a manifest object that includes a customized br-ex bridge" in the Deploying installer-provisioned clusters on bare metal document or the Installing a user-provisioned cluster on bare metal document.
----

# Viewing the network state of a node by using the CLI

Node network state is the network configuration for all nodes in the cluster. A NodeNetworkState object exists on every node in the cluster. This object is periodically updated and captures the state of the network for that node.

* You have installed the OpenShift CLI (`oc`).

1. List all the NodeNetworkState objects in the cluster:

```terminal
$ oc get nns
```

2. Inspect a NodeNetworkState object to view the network on that node. The output in this example has been redacted for clarity:

```terminal
$ oc get nns node01 -o yaml
```

Example output

```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkState
metadata:
  name: node01 1
status:
  currentState: 2
    dns-resolver:
# ...
    interfaces:
# ...
    route-rules:
# ...
    routes:
# ...
  lastSuccessfulUpdateTime: "2020-01-31T12:14:00Z" 3
```

The name of the NodeNetworkState object is taken from the node.
The currentState contains the complete network configuration for the node, including DNS, interfaces, and routes.
Timestamp of the last successful update. This is updated periodically as long as the node is reachable and can be used to evalute the freshness of the report.

# Viewing the network state of a node (NNS) from the web console

As an administrator, you can use the Red Hat OpenShift Container Platform web console to observe NodeNetworkState resources and network interfaces, and access network details.

1. Navigate to Networking → NodeNetworkState.

In the NodeNetworkState page, you can view the list of NodeNetworkState resources and the corresponding interfaces that are created on the nodes. You can use Filter based on Interface state, Interface type, and IP, or the search bar based on criteria Name or Label, to narrow down the displayed NodeNetworkState resources.
2. To access the detailed information about NodeNetworkState resource, click the NodeNetworkState resource name listed in the Name column .
3. To expand and view the Network Details section for the NodeNetworkState resource, click the greater than (>) symbol . Alternatively, you can click on each interface type under the Network interface column to view the network details.

## Viewing a graphical representation of the NNS topology

To make the configuration of the node network in the cluster easier to understand, you can view it in the form of a diagram. The NNS topology diagram displays all node components (network interface controllers, bridges, bonds, and VLANs), their properties and configurations, and connections between the nodes.

To open the topology view of the cluster, do the following:

1. In the Administrator view of the web console, navigate to Networking -> NodeNetworkState.
2. In the upper-right corner of the page, click the Topology icon.

The NNS topology diagram opens. Each group of components represents a single node.
* To display the configuration and propertires of a node, click inside the border of the node.
* To display the features or the YAML file of a specific component (for example, an interface or a bridge), click the icon of the component.
* The icons of active components have green borders; the icons of disconnected components have red borders.

# The NodeNetworkConfigurationPolicy manifest file

A NodeNetworkConfigurationPolicy (NNCP) manifest file defines policies that the Kubernetes NMState Operator uses to configure networking for nodes that exist in an Red Hat OpenShift Container Platform cluster.

After you apply a node network policy to a node, the Kubernetes NMState Operator configures the networking configuration for nodes according to the node network policy details.

You can create an NNCP by using either the OpenShift CLI (`oc`) or the Red Hat OpenShift Container Platform web console. As a postinstallation task you can create an NNCP or edit an existing NNCP.


[NOTE]
----
Before you create an NNCP, ensure that you read the "Example policy configurations for different interfaces" document.
----

If you want to delete an NNCP, you can use the oc delete nncp command to complete this action. However, this command does not delete any objects, such as a bridge interface.

Deleting the node network policy that added an interface to a node does not change the configuration of the policy on the node. Similarly, removing an interface does not delete the policy, because the Kubernetes NMState Operator re-adds the removed interface whenever a pod or a node is restarted.

To effectively delete the NNCP, the node network policy, and any interfaces would typically require the following actions:

1. Edit the NNCP and remove interface details from the file. Ensure that you do not remove name, state, and type parameters from the file.
2. Add state: absent under the interfaces.state section of the NNCP.
3. Run oc apply -f <nncp_file_name>. After the Kubernetes NMState Operator applies the node network policy to each node in your cluster, any interface that exists on each node is now marked as absent.
4. Run oc delete nncp to delete the NNCP.

## Additional resources

* Example policy configurations for different interfaces
* Removing an interface from nodes

# Managing policy from the web console

You can update the node network configuration, such as adding or removing interfaces from nodes, by applying NodeNetworkConfigurationPolicy manifests to the cluster.
Manage the policy from the web console by accessing the list of created policies in the NodeNetworkConfigurationPolicy page under the Networking menu. This page enables you to create, update, monitor, and delete the policies.

## Monitoring the policy status

You can monitor the policy status from the NodeNetworkConfigurationPolicy page. This page displays all the policies created in the cluster in a tabular format, with the following columns:

Name:: The name of the policy created.
Matched nodes:: The count of nodes where the policies are applied. This could be either a subset of nodes based on the node selector or all the nodes on the cluster.
Node network state:: The enactment state of the matched nodes. You can click on the enactment state and view detailed information on the status.

To find the desired policy, you can filter the list either based on enactment state by using the Filter option, or by using the search option.

## Creating a policy

You can create a policy by using either a form or YAML in the web console.

1. Navigate to Networking → NodeNetworkConfigurationPolicy.
2. In the NodeNetworkConfigurationPolicy page, click Create, and select From Form option.

In case there are no existing policies, you can alternatively click Create NodeNetworkConfigurationPolicy to createa policy using form.

[NOTE]
----
To create policy using YAML, click Create, and select With YAML option. The following steps are applicable to create a policy only by using form.
----
3. Optional: Check the Apply this NodeNetworkConfigurationPolicy only to specific subsets of nodes using the node selector checkbox to specify the nodes where the policy must be applied.
4. Enter the policy name in the Policy name field.
5. Optional: Enter the description of the policy in the Description field.
6. Optional: In the Policy Interface(s) section, a bridge interface is added by default with preset values in editable fields. Edit the values by executing the following steps:
1. Enter the name of the interface in Interface name field.
2. Select the network state from Network state dropdown. The default selected value is Up.
3. Select the type of interface from Type dropdown. The available values are Bridge, Bonding, and Ethernet. The default selected value is Bridge.

[NOTE]
----
Addition of a VLAN interface by using the form is not supported. To add a VLAN interface, you must use YAML to create the policy. Once added, you cannot edit the policy by using form.
----
4. Optional: In the IP configuration section, check IPv4 checkbox to assign an IPv4 address to the interface, and configure the IP address assignment details:
1. Click IP address to configure the interface with a static IP address, or DHCP to auto-assign an IP address.
2. If you have selected IP address option, enter the IPv4 address in IPV4 address field, and enter the prefix length in Prefix length field.

If you have selected DHCP option, uncheck the options that you want to disable. The available options are Auto-DNS, Auto-routes, and Auto-gateway. All the options are selected by default.
5. Optional: Enter the port number in Port field.
6. Optional: Check the checkbox Enable STP to enable STP.
7. Optional: To add an interface to the policy, click Add another interface to the policy.
8. Optional: To remove an interface from the policy, click  icon next to the interface.

[NOTE]
----
Alternatively, you can click Edit YAML on the top of the page to continue editing the form using YAML.
----
7. Click Create to complete policy creation.

# Updating the policy

## Updating the policy by using form

1. Navigate to Networking → NodeNetworkConfigurationPolicy.
2. In the NodeNetworkConfigurationPolicy page, click the . icon placed next to the policy you want to edit, and click Edit.
3. Edit the fields that you want to update.
4. Click Save.


[NOTE]
----
Addition of a VLAN interface using the form is not supported. To add a VLAN interface, you must use YAML to create the policy. Once added, you cannot edit the policy using form.
----

## Updating the policy by using YAML

1. Navigate to Networking → NodeNetworkConfigurationPolicy.
2. In the NodeNetworkConfigurationPolicy page, click the policy name under the Name column for the policy you want to edit.
3. Click the YAML tab, and edit the YAML.
4. Click Save.

## Deleting the policy

1. Navigate to Networking → NodeNetworkConfigurationPolicy.
2. In the NodeNetworkConfigurationPolicy page, click the . icon placed next to the policy you want to delete, and click Delete.
3. In the pop-up window, enter the policy name to confirm deletion, and click Delete.

# Managing policy by using the CLI

## Creating an interface on nodes

Create an interface on nodes in the cluster by applying a NodeNetworkConfigurationPolicy (NNCP) manifest to the cluster. The manifest details the requested configuration for the interface.

By default, the manifest applies to all nodes in the cluster. To add the interface to specific nodes, add the spec: nodeSelector parameter and the appropriate <key>:<value> for your node selector.

You can configure multiple nmstate-enabled nodes concurrently. The configuration applies to 50% of the nodes in parallel. This strategy prevents the entire cluster from being unavailable if the network connection fails. To apply the policy configuration in parallel to a specific portion of the cluster, use the maxUnavailable parameter in the NodeNetworkConfigurationPolicy manifest configuration file.


[NOTE]
----
If you have two nodes and you apply an NNCP manifest with the maxUnavailable parameter set to 50% to these nodes, one node at a time receives the NNCP configuration. If you then introduce an additional NNCP manifest file with the maxUnavailable parameter set to 50%, this NCCP is independent of the initial NNCP; this means that if both NNCP manifests apply a bad configuration to nodes, you can no longer guarantee that half of your cluster is functional.
----

* You have installed the OpenShift CLI (`oc`).

1. Create the NodeNetworkConfigurationPolicy manifest. The following example configures a Linux bridge on all worker nodes and configures the DNS resolver:

```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy 1
spec:
  nodeSelector: 2
    node-role.kubernetes.io/worker: "" 3
  maxUnavailable: 3 4
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port 5
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
          auto-dns: false
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1
    dns-resolver: 6
      config:
        search:
        - example.com
        - example.org
        server:
        - 8.8.8.8
```

Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses the node-role.kubernetes.io/worker: "" node selector to select all worker nodes in the cluster.
Optional: Specifies the maximum number of nmstate-enabled nodes that the policy configuration can be applied to concurrently. This parameter can be set to either a percentage value (string), for example, "10%", or an absolute value (number), such as 3.
Optional: Human-readable description for the interface.
Optional: Specifies the search and server settings for the DNS server.
2. Create the node network policy:

```terminal
$ oc apply -f br1-eth1-policy.yaml 1
```

File name of the node network configuration policy manifest.

## Additional resources

* Example for creating multiple interfaces in the same policy
* Examples of different IP management methods in policies

## Confirming node network policy updates on nodes

When you apply a node network policy, a NodeNetworkConfigurationEnactment object is created for every node in the cluster. The node network configuration enactment is a read-only object that represents the status of execution of the policy on that node.
If the policy fails to be applied on the node, the enactment for that node includes a traceback for troubleshooting.

* You have installed the OpenShift CLI (`oc`).

1. To confirm that a policy has been applied to the cluster, list the policies and their status:

```terminal
$ oc get nncp
```

2. Optional: If a policy is taking longer than expected to successfully configure, you can inspect the requested state and status conditions of a particular policy:

```terminal
$ oc get nncp <policy> -o yaml
```

3. Optional: If a policy is taking longer than expected to successfully configure on all nodes, you can list the status of the enactments on the cluster:

```terminal
$ oc get nnce
```

4. Optional: To view the configuration of a particular enactment, including any error reporting for a failed configuration:

```terminal
$ oc get nnce <node>.<policy> -o yaml
```


## Removing an interface from nodes

You can remove an interface from one or more nodes in the cluster by editing the NodeNetworkConfigurationPolicy object and setting the state of the interface to absent.

Removing an interface from a node does not automatically restore the node network configuration to a previous state. If you want to restore the previous state, you will need to define that node network configuration in the policy.

If you remove a bridge or bonding interface, any node NICs in the cluster that were previously attached or subordinate to that bridge or bonding interface are placed in a down state and become unreachable. To avoid losing connectivity, configure the node NIC in the same policy so that it has a status of up and either DHCP or a static IP address.


[NOTE]
----
Deleting the node network policy that added an interface does not change the configuration of the policy on the node. Although a NodeNetworkConfigurationPolicy is an object in the cluster, the object only represents the requested configuration. Similarly, removing an interface does not delete the policy.
----

* You have installed the OpenShift CLI (`oc`).

1. Update the NodeNetworkConfigurationPolicy manifest used to create the interface. The following example removes a Linux bridge and configures the eth1 NIC with DHCP to avoid losing connectivity:

```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: <br1-eth1-policy> 1
spec:
  nodeSelector: 2
    node-role.kubernetes.io/worker: "" 3
  desiredState:
    interfaces:
    - name: br1
      type: linux-bridge
      state: absent 4
    - name: eth1 5
      type: ethernet 6
      state: up 7
      ipv4:
        dhcp: true 8
        enabled: true 9
```

Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses the node-role.kubernetes.io/worker: "" node selector to select all worker nodes in the cluster.
Changing the state to absent removes the interface.
The name of the interface that is to be unattached from the bridge interface.
The type of interface. This example creates an Ethernet networking interface.
The requested state for the interface.
Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
Enables ipv4 in this example.
2. Update the policy on the node and remove the interface:

```terminal
$ oc apply -f <br1-eth1-policy.yaml> 1
```

File name of the policy manifest.

# Example policy configurations for different interfaces

Before you read the different example NodeNetworkConfigurationPolicy (NNCP) manifest configurations, consider the following factors when you apply a policy to nodes so that your cluster runs under its best performance conditions:

* When you need to apply a policy to more than one node, create a NodeNetworkConfigurationPolicy manifest for each target node. The Kubernetes NMState Operator applies the policy to each node with a defined NNCP in an unspecified order. Scoping a policy with this approach reduces the length of time for policy application but risks a cluster-wide outage if an error exists in the configuration of the cluster. To avoid this type of error, initially apply an NNCP to some nodes, confirm the NNCP is configured correctly for these nodes, and then proceed with applying the policy to the remaining nodes.
* When you need to apply a policy to many nodes but you only want to create a single NNCP for all the nodes, the Kubernetes NMState Operator applies the policy to each node in sequence. You can set the speed and coverage of policy application for target nodes with the maxUnavailable parameter in the cluster's configuration file. By setting a lower percentage value for the parameter, you can reduce the risk of a cluster-wide outage if the outage impacts the small percentage of nodes that are receiving the policy application.
* If you set the maxUnavailable parameter to 50% in two NNCP manifests, the policy configuration coverage applies to 100% of the nodes in your cluster.
* When a node restarts, the Kubernetes NMState Operator cannot control the order to which it applies policies to nodes. The Kubernetes NMState Operator might apply interdependent policies in a sequence that results in a degraded network object.
* Consider specifying all related network configurations in a single policy.

## Example: Ethernet interface node network configuration policy

Configure an Ethernet interface on nodes in the cluster by applying a NodeNetworkConfigurationPolicy manifest to the cluster.

The following YAML file is an example of a manifest for an Ethernet interface.
It includes sample values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: eth1-policy 1
spec:
  nodeSelector: 2
    kubernetes.io/hostname: <node01> 3
  desiredState:
    interfaces:
    - name: eth1 4
      description: Configuring eth1 on node01 5
      type: ethernet 6
      state: up 7
      ipv4:
        dhcp: true 8
        enabled: true 9
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses a hostname node selector.
Name of the interface.
Optional: Human-readable description of the interface.
The type of interface. This example creates an Ethernet networking interface.
The requested state for the interface after creation.
Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
Enables ipv4 in this example.

## Example: Linux bridge interface node network configuration policy

Create a Linux bridge interface on nodes in the cluster by applying a NodeNetworkConfigurationPolicy manifest
to the cluster.

The following YAML file is an example of a manifest for a Linux bridge interface.
It includes samples values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy 1
spec:
  nodeSelector: 2
    kubernetes.io/hostname: <node01> 3
  desiredState:
    interfaces:
      - name: br1 4
        description: Linux bridge with eth1 as a port 5
        type: linux-bridge 6
        state: up 7
        ipv4:
          dhcp: true 8
          enabled: true 9
        bridge:
          options:
            stp:
              enabled: false 10
          port:
            - name: eth1 11
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses a hostname node selector.
Name of the interface.
Optional: Human-readable description of the interface.
The type of interface. This example creates a bridge.
The requested state for the interface after creation.
Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
Enables ipv4 in this example.
Disables stp in this example.
The node NIC to which the bridge attaches.

## Example: VLAN interface node network configuration policy

Create a VLAN interface on nodes in the cluster by applying a NodeNetworkConfigurationPolicy manifest to the cluster.


[NOTE]
----
Define all related configurations for the VLAN interface of a node in a single NodeNetworkConfigurationPolicy manifest. For example, define the VLAN interface for a node and the related routes for the VLAN interface in the same NodeNetworkConfigurationPolicy manifest.
When a node restarts, the Kubernetes NMState Operator cannot control the order in which policies are applied. Therefore, if you use separate policies for related network configurations, the Kubernetes NMState Operator might apply these policies in a sequence that results in a degraded network object.
----

The following YAML file is an example of a manifest for a VLAN interface.
It includes samples values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vlan-eth1-policy 1
spec:
  nodeSelector: 2
    kubernetes.io/hostname: <node01> 3
  desiredState:
    interfaces:
    - name: eth1.102 4
      description: VLAN using eth1 5
      type: vlan 6
      state: up 7
      vlan:
        base-iface: eth1 8
        id: 102 9
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses a hostname node selector.
Name of the interface. When deploying on bare metal, only the <interface_name>.<vlan_number> VLAN format is supported.
Optional: Human-readable description of the interface.
The type of interface. This example creates a VLAN.
The requested state for the interface after creation.
The node NIC to which the VLAN is attached.
The VLAN tag.

* Configuring an SR-IOV network device
* Configuring hardware offloading

## Example: Bond interface node network configuration policy

Create a bond interface on nodes in the cluster by applying a NodeNetworkConfigurationPolicy manifest
to the cluster.


[NOTE]
----
OpenShift Virtualization only supports the following bond modes:
* mode=1 active-backup
* mode=2 balance-xor
* mode=4 802.3ad
Other bond modes are not supported.
----

The following YAML file is an example of a manifest for a bond interface.
It includes samples values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond0-eth1-eth2-policy 1
spec:
  nodeSelector: 2
    kubernetes.io/hostname: <node01> 3
  desiredState:
    interfaces:
    - name: bond0 4
      description: Bond with ports eth1 and eth2 5
      type: bond 6
      state: up 7
      ipv4:
        dhcp: true 8
        enabled: true 9
      link-aggregation:
        mode: active-backup 10
        options:
          miimon: '140' 11
        port: 12
        - eth1
        - eth2
      mtu: 1450 13
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses a hostname node selector.
Name of the interface.
Optional: Human-readable description of the interface.
The type of interface. This example creates a bond.
The requested state for the interface after creation.
Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
Enables ipv4 in this example.
The driver mode for the bond. This example uses an active backup mode.
Optional: This example uses miimon to inspect the bond link every 140ms.
The subordinate node NICs in the bond.
Optional: The maximum transmission unit (MTU) for the bond. If not specified, this value is set to 1500 by default.

## Example: Multiple interfaces in the same node network configuration policy

You can create multiple interfaces in the same node network configuration policy. These interfaces can reference each other, allowing you to build and deploy a network configuration by using a single policy manifest.

The following example YAML file creates a bond that is named bond10 across two NICs and VLAN that is named bond10.103 that connects to the bond.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond-vlan 1
spec:
  nodeSelector: 2
    kubernetes.io/hostname: <node01> 3
  desiredState:
    interfaces:
    - name: bond10 4
      description: Bonding eth2 and eth3 5
      type: bond 6
      state: up 7
      link-aggregation:
        mode: balance-xor 8
        options:
          miimon: '140' 9
        port: 10
        - eth2
        - eth3
    - name: bond10.103 4
      description: vlan using bond10 5
      type: vlan 6
      state: up 7
      vlan:
         base-iface: bond10 11
         id: 103 12
      ipv4:
        dhcp: true 13
        enabled: true 14
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example uses hostname node selector.
Name of the interface.
Optional: Human-readable description of the interface.
The type of interface.
The requested state for the interface after creation.
The driver mode for the bond.
Optional: This example uses miimon to inspect the bond link every 140ms.
The subordinate node NICs in the bond.
The node NIC to which the VLAN is attached.
The VLAN tag.
Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
Enables ipv4 in this example.

## Example: Node network configuration policy for virtual functions

Update host network settings for Single Root I/O Virtualization (SR-IOV) network virtual functions (VF) in an existing cluster by applying a NodeNetworkConfigurationPolicy manifest.

You can apply a NodeNetworkConfigurationPolicy manifest to an existing cluster to complete the following tasks:

* Configure QoS host network settings for VFs to optimize performance.
* Add, remove, or update VFs for a network interface.
* Manage VF bonding configurations.


[NOTE]
----
To update host network settings for SR-IOV VFs by using NMState on physical functions that are also managed through the SR-IOV Network Operator, you must set the externallyManaged parameter in the relevant SriovNetworkNodePolicy resource to true. For more information, see the Additional resources section.
----

The following YAML file is an example of a manifest that defines QoS policies for a VF.
This YAML includes samples values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: qos 1
spec:
  nodeSelector: 2
    node-role.kubernetes.io/worker: "" 3
  desiredState:
    interfaces:
      - name: ens1f0 4
        description: Change QOS on VF0 5
        type: ethernet 6
        state: up 7
        ethernet:
         sr-iov:
           total-vfs: 3 8
           vfs:
           - id: 0 9
             max-tx-rate: 200 10
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
This example applies to all nodes with the worker role.
Name of the physical function (PF) network interface.
Optional: Human-readable description of the interface.
The type of interface.
The requested state for the interface after configuration.
The total number of VFs.
Identifies the VF with an ID of 0.
Sets a maximum transmission rate, in Mbps, for the VF. This sample value sets a rate of 200 Mbps.

The following YAML file is an example of a manifest that adds a VF for a network interface.

In this sample configuration, the ens1f1v0 VF is created on the ens1f1 physical interface, and this VF is added to a bonded network interface bond0. The bond uses active-backup mode for redundancy. In this example, the VF is configured to use hardware offloading to manage the VLAN directly on the physical interface.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: addvf 1
spec:
  nodeSelector: 2
    node-role.kubernetes.io/worker: "" 3
  maxUnavailable: 3
  desiredState:
    interfaces:
      - name: ens1f1 4
        type: ethernet
        state: up
        ethernet:
            sr-iov:
              total-vfs: 1 5
              vfs:
                - id: 0
                  trust: true 6
                  vlan-id: 477 7
      - name: bond0 8
        description: Attach VFs to bond 9
        type: bond 10
        state: up 11
        link-aggregation:
          mode: active-backup 12
          options:
            primary: ens1f0v0 13
          port: 14
            - ens1f0v0
            - ens1f1v0 15
```


Name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster.
The example applies to all nodes with the worker role.
Name of the VF network interface.
Number of VFs to create.
Setting to allow failover bonding between the active and backup VFs.
ID of the VLAN. The example uses hardward offloading to define a VLAN directly on the VF.
Name of the bonding network interface.
Optional: Human-readable description of the interface.
The type of interface.
The requested state for the interface after configuration.
The bonding policy for the bond.
The primary attached bonding port.
The ports for the bonded network interface.
In this example, the VLAN network interface is added as an additional interface to the bonded network interface.

## Example: Network interface with a VRF instance node network configuration policy

Associate a Virtual Routing and Forwarding (VRF) instance with a network interface by applying a NodeNetworkConfigurationPolicy custom resource (CR).

By associating a VRF instance with a network interface, you can support traffic isolation, independent routing decisions, and the logical separation of network resources.


[WARNING]
----
When configuring Virtual Route Forwarding (VRF), you must change the VRF value to a table ID lower than 1000 because a value higher than 1000 is reserved for Red Hat OpenShift Container Platform.
----

In a bare-metal environment, you can announce load balancer services through interfaces belonging to a VRF instance by using MetalLB. For more information, see the Additional resources section.

The following YAML file is an example of associating a VRF instance to a network interface.
It includes samples values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vrfpolicy 1
spec:
  nodeSelector:
    vrf: "true" 2
  maxUnavailable: 3
  desiredState:
    interfaces:
      - name: ens4vrf 3
        type: vrf 4
        state: up
        vrf:
          port:
            - ens4 5
          route-table-id: 2 6
```


The name of the policy.
This example applies the policy to all nodes with the label vrf:true.
The name of the interface.
The type of interface. This example creates a VRF instance.
The node interface to which the VRF attaches.
The name of the route table ID for the VRF.

* About virtual routing and forwarding
* Exposing a service through a network VRF

# Creating an IP over InfiniBand interface on nodes

On the Red Hat OpenShift Container Platform web console, you can install a Red&#160;Hat certified third-party Operator, such as the NVIDIA Network Operator, that supports InfiniBand (IPoIB) mode. Typically, you would use the third-party Operator with other vendor infrastructure to manage resources in an Red Hat OpenShift Container Platform cluster. To create an IPoIB interface on nodes in your cluster, you must define an InfiniBand (IPoIB) interface in a NodeNetworkConfigurationPolicy (NNCP) manifest file.


[IMPORTANT]
----
The Red Hat OpenShift Container Platform documentation describes defining only the IPoIB interface configuration in a NodeNetworkConfigurationPolicy (NNCP) manifest file. You must refer to the NVIDIA and other third-party vendor documentation for the majority of the configuring steps. Red Hat support does not extend to anything external to the NNCP configuration.
For more information about the NVIDIA Operator, see Getting Started with Red Hat OpenShift (NVIDIA Docs Hub).
----

* You installed a Red Hat certified third-party Operator that supports an IPoIB interface.
* You have installed the OpenShift CLI (`oc`).

1. Create or edit a NodeNetworkConfigurationPolicy (NNCP) manifest file, and then specify an IPoIB interface in the file.

```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: worker-0-ipoib
spec:
# ...
    interfaces:
    - description: ""
      infiniband:
        mode: datagram 1
        pkey: "0xffff" 2
      ipv4:
        address:
        - ip: 100.125.3.4
          prefix-length: 16
        dhcp: false
        enabled: true
      ipv6:
        enabled: false
      name: ibp27s0
      state: up
      identifier: mac-address 3
      mac-address: 20:00:55:04:01:FE:80:00:00:00:00:00:00:00:02:C9:02:00:23:13:92 4
      type: infiniband 5
# ...
```

datagram is the default mode for an IPoIB interface, and this mode improves optimizes performance and latency. connected mode is a supported mode but consider only using this mode when you need to adjust the maximum transmission unit (MTU) value to improve node connectivity with surrounding network devices.
Supports a string or an integer value. The parameter defines the protection key, or P-key, for the interface for the purposes of authentication and encrypted communications with a third-party vendor, such as NVIDIA. Values None and 0xffff indicate the protection key for the base interface in an InfiniBand system.
Supported values include name, the default value, and mac-address. The name value applies a configuration to an interface that holds a specified interface name.
Holds the MAC address of an interface. For an IP-over-InfiniBand (IPoIB) interface, the address is a 20-byte string.
Sets the type of interface to infiniband.
2. Apply the NNCP configuration to each node in your cluster by running the following command. The Kubernetes NMState Operator can then create an IPoIB interface on each node.

```yaml
$ oc apply -f <nncp_file_name> 1
```

Replace <nncp_file_name> with the name of your NNCP file.

# Example policy configurations that use dynamic matching and templating

The following example configuration snippets show node network policies that use dynamic matching and templating.

## Example: Linux bridge interface node network configuration policy to inherit static IP address from the NIC attached to the bridge

Create a Linux bridge interface on nodes in the cluster and transfer the static IP configuration of the NIC to the bridge by applying a single NodeNetworkConfigurationPolicy manifest to the cluster.

The following YAML file is an example of a manifest for a Linux bridge interface. It includes sample values that you must replace with your own information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-copy-ipv4-policy 1
spec:
  nodeSelector: 2
    node-role.kubernetes.io/worker: ""
  capture:
    eth1-nic: interfaces.name=="eth1" 3
    eth1-routes: routes.running.next-hop-interface=="eth1"
    br1-routes: capture.eth1-routes | routes.running.next-hop-interface := "br1"
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port
        type: linux-bridge 4
        state: up
        ipv4: "{{ capture.eth1-nic.interfaces.0.ipv4 }}" 5
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1 6
     routes:
        config: "{{ capture.br1-routes.routes.running }}"
```


The name of the policy.
Optional: If you do not include the nodeSelector parameter, the policy applies to all nodes in the cluster. This example uses the node-role.kubernetes.io/worker: "" node selector to select all worker nodes in the cluster.
The reference to the node NIC to which the bridge attaches.
The type of interface. This example creates a bridge.
The IP address of the bridge interface. This value matches the IP address of the NIC which is referenced by the spec.capture.eth1-nic entry.
The node NIC to which the bridge attaches.

## Example: Node network configuration policy to enable LLDP reporting

The following YAML file is an example of a NodeNetworkConfigurationPolicy manifest that enables the Link Layer Discovery Protocol (LLDP) listener for all ethernet ports in your Red Hat OpenShift Container Platform cluster. Devices on a local area network can use LLDP to advertise their identity, capabilities, and neighbor information.


```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: enable-lldp-ethernets-up 1
spec:
  capture:
    ethernets: interfaces.type=="ethernet"
    ethernets-up: capture.ethernets | interfaces.state=="up"
    ethernets-lldp: capture.ethernets-up | interfaces.lldp.enabled:=true 2
  desiredState:
    interfaces: "{{ capture.ethernets-lldp.interfaces }}"
# ...
```


Specifies the name of the node network configuration policy.
Specifies that LLDP is enabled for all ethernet ports that have the interface state set to up.

* The NMPolicy project - Policy syntax

# Examples: IP management

The following example configuration snippets show different methods of IP management.

These examples use the ethernet interface type to simplify the example while showing the related context in the policy configuration. These IP management examples can be used with the other interface types.

## Static

The following snippet statically configures an IP address on the Ethernet interface:


```yaml
# ...
    interfaces:
    - name: eth1
      description: static IP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: false
        address:
        - ip: 192.168.122.250 1
          prefix-length: 24
        enabled: true
# ...
```


Replace this value with the static IP address for the interface.

## No IP address

The following snippet ensures that the interface has no IP address:


```yaml
# ...
    interfaces:
    - name: eth1
      description: No IP on eth1
      type: ethernet
      state: up
      ipv4:
        enabled: false
# ...
```



[IMPORTANT]
----
Always set the state parameter to up when you set both the ipv4.enabled and the ipv6.enabled parameter to false to disable an interface. If you set state: down with this configuration, the interface receives a DHCP IP address because of automatic DHCP assignment.
----

## Dynamic host configuration

The following snippet configures an Ethernet interface that uses a dynamic IP address, gateway address, and DNS:


```yaml
# ...
    interfaces:
    - name: eth1
      description: DHCP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        enabled: true
# ...
```


The following snippet configures an Ethernet interface that uses a dynamic IP address but does not use a dynamic gateway address or DNS:


```yaml
# ...
    interfaces:
    - name: eth1
      description: DHCP without gateway or DNS on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        auto-gateway: false
        auto-dns: false
        enabled: true
# ...
```


## Media Access Control (MAC) address

You can use a MAC address to identify a network interface instead of using the name of the network interface. A network interface name can change for various reasons, such as an operating system configuration change. However, every network interface has a unique MAC address that does not change. This means that using a MAC address is a more permanent way to identify a specific network interface.

Supported values for the identifier parameter include the default name value and the value mac-address. The name value applies a configuration to an interface that holds a specified interface name.

Using a mac-address value for the identifier parameter indicates that a MAC address is the identifier for the network interface. If you set the identifier value to mac-address, you must enter a specific MAC address in the following mac-address parameter field.


[NOTE]
----
You can still specify a value for the name parameter, but setting the identifier: mac-address value means that a MAC address is used as the primary identifier for a network interface. If you specify an incorrect MAC address, nmstate reports an invalid argument error.
----

The following snippet specifies a MAC address as the primary identifier for an Ethernet device, named eth1, with a MAC address of 8A:8C:92:1A:F6:98:


```yaml
# ...
interfaces:
- name: eth1
  profile-name: wan0
  type: ethernet
  state: up
  identifier: mac-address
  mac-address: 8A:8C:92:1A:F6:98
# ...
```


## DNS

By default, the nmstate API stores DNS values globally as against storing them in a network interface. For certain situations, you must configure a network interface to store DNS values.


[TIP]
----
Setting a DNS configuration is comparable to modifying the /etc/resolv.conf file.
----

To define a DNS configuration for a network interface, you must initially specify the dns-resolver section in the network interface&#8217;s YAML configuration file. To apply an NNCP configuration to your network interface, you need to run the oc apply -f <nncp_file_name> command.

The following example shows a default situation that stores DNS values globally:

* Configure a static DNS without a network interface. Note that when updating the /etc/resolv.conf file on a host node, you do not need to specify an interface, IPv4 or IPv6, in the NodeNetworkConfigurationPolicy (NNCP) manifest.
Example of a DNS configuration for a network interface that globally stores DNS values

```yaml
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
 name: worker-0-dns-testing
spec:
  nodeSelector:
    kubernetes.io/hostname: <target_node>
  desiredState:
    dns-resolver:
      config:
        search:
        - example.com
        - example.org
        server:
        - 2001:db8:f::1
        - 192.0.2.251
# ...
```


[IMPORTANT]
----
You can specify DNS options under the dns-resolver.config section of your NNCP file as demonstrated in the following example:

```terminal
# ...
desiredState:
    dns-resolver:
      config:
        options:
         - timeout:2
         - attempts:3
# ...
```

If you want to remove the DNS options from your network interface, apply the following configuration to your NNCP and then run the oc apply -f <nncp_file_name> command:

```terminal
# ...
    dns-resolver:
      config: {}
    interfaces: []
# ...
```

----

The following examples show situations that require configuring a network interface to store DNS values:

* If you want to rank a static DNS name server over a dynamic DNS name server, define the interface that runs either the Dynamic Host Configuration Protocol (DHCP) or the IPv6 Autoconfiguration (autoconf) mechanism in the network interface YAML configuration file.
Example configuration that adds 192.0.2.1 to DNS name servers retrieved from the DHCPv4 network protocol

```yaml
# ...
dns-resolver:
  config:
    server:
    - 192.0.2.1
interfaces:
  - name: eth1
    type: ethernet
    state: up
    ipv4:
      enabled: true
      dhcp: true
      auto-dns: true
# ...
```

* If you need to configure a network interface to store DNS values instead of adopting the default method, which uses the nmstate API to store DNS values globally, you can set static DNS values and static IP addresses in the network interface YAML file.

[IMPORTANT]
----
Storing DNS values at the network interface level might cause name resolution issues after you attach the interface to network components, such as an Open vSwitch (OVS) bridge, a Linux bridge, or a bond.
----
Example configuration that stores DNS values at the interface level

```yaml
# ...
dns-resolver:
  config:
    search:
    - example.com
    - example.org
    server:
    - 2001:db8:1::d1
    - 2001:db8:1::d2
    - 192.0.2.1
interfaces:
  - name: eth1
    type: ethernet
    state: up
    ipv4:
      address:
      - ip: 192.0.2.251
        prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      address:
      - ip: 2001:db8:1::1
        prefix-length: 64
      dhcp: false
      enabled: true
      autoconf: false
# ...
```

* If you want to set static DNS search domains and dynamic DNS name servers for your network interface, define the dynamic interface that runs either the Dynamic Host Configuration Protocol (DHCP) or the IPv6 Autoconfiguration (autoconf) mechanism in the network interface YAML configuration file.
Example configuration that sets example.com and example.org static DNS search domains along with dynamic DNS name server settings

```yaml
# ...
dns-resolver:
  config:
    search:
    - example.com
    - example.org
    server: []
interfaces:
  - name: eth1
    type: ethernet
    state: up
    ipv4:
      enabled: true
      dhcp: true
      auto-dns: true
    ipv6:
      enabled: true
      dhcp: true
      autoconf: true
      auto-dns: true
# ...
```


## Static routing

The following snippet configures a static route and a static IP on interface eth1.


```yaml
dns-resolver:
  config:
# ...
interfaces:
  - name: eth1
    description: Static routing on eth1
    type: ethernet
    state: up
    ipv4:
      dhcp: false
      enabled: true
      address:
      - ip: 192.0.2.251 1
        prefix-length: 24
routes:
  config:
  - destination: 198.51.100.0/24
    metric: 150
    next-hop-address: 192.0.2.1 2
    next-hop-interface: eth1
    table-id: 254
# ...
```


The static IP address for the Ethernet interface.
The next hop address for the node traffic. This must be in the same subnet as the IP address set for the Ethernet interface.


[IMPORTANT]
----
You cannot use the OVN-Kubernetes br-ex bridge as the next hop interface when configuring a static route unless you manually configured a customized br-ex bridge.
For more information, see "Creating a manifest object that includes a customized br-ex bridge" in the Deploying installer-provisioned clusters on bare metal document or the Installing a user-provisioned cluster on bare metal document.
----

* Creating a manifest object that includes a customized br-ex bridge (Installer-provisioned infrastructure)
* Creating a manifest object that includes a customized br-ex bridge (User-provisioned infrastructure)