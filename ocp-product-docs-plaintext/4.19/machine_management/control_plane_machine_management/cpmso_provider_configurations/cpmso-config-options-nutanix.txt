# Control plane configuration options for Nutanix


You can change the configuration of your Nutanix control plane machines by updating values in the control plane machine set.
When you save an update to the control plane machine set, the Control Plane Machine Set Operator updates the control plane machines according to your configured update strategy.

# Sample YAML for configuring Nutanix clusters

The following example YAML snippet shows a provider specification configuration for a Nutanix cluster.

## Sample Nutanix provider specification

When you create a control plane machine set for an existing cluster, the provider specification must match the providerSpec configuration in the control plane machine custom resource (CR) that the installation program creates.

### Values obtained by using the OpenShift CLI

In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.

Infrastructure ID:: The <cluster_id> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:

```terminal
$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
```



```yaml
apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
metadata:
  name: cluster
  namespace: openshift-machine-api
spec:
# ...
  template:
# ...
      spec:
        providerSpec:
          value:
            apiVersion: machine.openshift.io/v1
            bootType: "" 1
            categories: 2
            - key: <category_name>
              value: <category_value>
            cluster: 3
              type: uuid
              uuid: <cluster_uuid>
            credentialsSecret:
              name: nutanix-credentials 4
            image: 5
              name: <cluster_id>-rhcos
              type: name
            kind: NutanixMachineProviderConfig 6
            memorySize: 16Gi 7
            metadata:
              creationTimestamp: null
            project: 8
              type: name
              name: <project_name>
            subnets: 9
            - type: uuid
              uuid: <subnet_uuid>
            systemDiskSize: 120Gi 10
            userDataSecret:
              name: master-user-data 11
            vcpuSockets: 8 12
            vcpusPerSocket: 1 13
```


Specifies the boot type that the control plane machines use. For more information about boot types, see Understanding UEFI, Secure Boot, and TPM in the Virtualized Environment. Valid values are Legacy, SecureBoot, or UEFI. The default is Legacy.

[NOTE]
----
You must use the Legacy boot type in Red Hat OpenShift Container Platform 4.19.
----
Specifies one or more Nutanix Prism categories to apply to control plane machines. This stanza requires key and value parameters for a category key-value pair that exists in Prism Central. For more information about categories, see Category management.
Specifies a Nutanix Prism Element cluster configuration. In this example, the cluster type is uuid, so there is a uuid stanza.

[NOTE]
----
Clusters that use Red Hat OpenShift Container Platform version 4.15 or later can use failure domain configurations.
If the cluster uses a failure domain, configure this parameter in the failure domain.
If you specify this value in the provider specification when using failure domains, the Control Plane Machine Set Operator ignores it.
----
Specifies the secret name for the cluster. Do not change this value.
Specifies the image that was used to create the disk.
Specifies the cloud provider platform type. Do not change this value.
Specifies the memory allocated for the control plane machines.
Specifies the Nutanix project that you use for your cluster. In this example, the project type is name, so there is a name stanza.
Specify one or more Prism Element subnet objects.
In this example, the subnet type is uuid, so there is a uuid stanza.
A maximum of 32 subnets for each Prism Element failure domain in the cluster is supported.

[IMPORTANT]
----
The following known issues with configuring multiple subnets for an existing Nutanix cluster by using a control plane machine set exist in Red Hat OpenShift Container Platform version 4.18:
* Adding subnets above the existing subnet in the subnets stanza causes a control plane node to become stuck in the Deleting state.
As a workaround, only add subnets below the existing subnet in the subnets stanza.
* Sometimes, after adding a subnet, the updated control plane machines appear in the Nutanix console but the Red Hat OpenShift Container Platform cluster is unreachable.
There is no workaround for this issue.
These issues occur on clusters that use a control plane machine set to configure subnets regardless of whether subnets are specified in a failure domain or the provider specification.
For more information, see OCPBUGS-50904.
----

The CIDR IP address prefix for one of the specified subnets must contain the virtual IP addresses that the Red Hat OpenShift Container Platform cluster uses.
All subnet UUID values must be unique.

[NOTE]
----
Clusters that use Red Hat OpenShift Container Platform version 4.15 or later can use failure domain configurations.
If the cluster uses a failure domain, configure this parameter in the failure domain.
If you specify this value in the provider specification when using failure domains, the Control Plane Machine Set Operator ignores it.
----
Specifies the VM disk size for the control plane machines.
Specifies the control plane user data secret. Do not change this value.
Specifies the number of vCPU sockets allocated for the control plane machines.
Specifies the number of vCPUs for each control plane vCPU socket.

## Failure domains for Nutanix clusters

To add or update the failure domain configuration on a Nutanix cluster, you must make coordinated changes to several resources.
The following actions are required:

1. Modify the cluster infrastructure custom resource (CR).
2. Modify the cluster control plane machine set CR.
3. Modify or replace the compute machine set CRs.

For more information, see "Adding failure domains to an existing Nutanix cluster" in the Post-installation configuration content.

* Adding failure domains to an existing Nutanix cluster