# OpenShift Virtualization release notes



# Providing documentation feedback

To report an error or to improve our documentation, log in to your Red Hat Jira account and submit a Jira issue.

# About Red Hat OpenShift Virtualization

With Red Hat OpenShift Virtualization, you can bring traditional virtual machines (VMs) into Red Hat OpenShift Container Platform and run them alongside containers. In OpenShift Virtualization, VMs are native Kubernetes objects that you can manage by using the Red Hat OpenShift Container Platform web console or the command line.

OpenShift Virtualization is represented by the  icon.

You can use OpenShift Virtualization the OVN-Kubernetes Container Network Interface (CNI) network provider.

Learn more about what you can do with OpenShift Virtualization.

Learn more about OpenShift Virtualization architecture and deployments.

Prepare your cluster for OpenShift Virtualization.

## Supported cluster versions for OpenShift Virtualization

The latest stable release of OpenShift Virtualization 4.18 is 4.18.0.

OpenShift Virtualization 4.18 is supported for use on Red Hat OpenShift Container Platform 4.19 clusters. To use the latest z-stream release of OpenShift Virtualization, you must first upgrade to the latest version of Red Hat OpenShift Container Platform.

## Supported guest operating systems

To view the supported guest operating systems for OpenShift Virtualization, see Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization, OpenShift Virtualization and Red Hat Enterprise Linux with KVM.

## Microsoft Windows SVVP certification

OpenShift Virtualization is certified in Microsoft&#8217;s Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.

The SVVP certification applies to:

* Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named Red Hat OpenShift Container Platform 4 on RHEL CoreOS 9.
* Intel and AMD CPUs.

# Quick starts

Quick start tours are available for several OpenShift Virtualization features. To view the tours, click the Help icon ? in the menu bar on the header of the Red Hat OpenShift Container Platform web console and then select Quick Starts. You can filter the available tours by entering the keyword virtualization in the Filter field.

# New and changed features

This release adds new features and enhancements related to the following components and concepts:

## Infrastructure

* You can now prevent the inadvertent deletion of a virtual machine (VM) by enabling delete protection for the VM. You can also disable delete protection that has been set for a VM.

As a cluster administrator, you can prevent users from enabling VM delete protection by removing the option at the cluster level.

## Virtualization

* You can now choose to expand virtual machines (VMs) using instance types and preferences. For more information, see Using InstancetypeReferencePolicy to expand VirtualMachines in the Red Hat Customer Portal.
* Red Hat Enterprise Linux (RHEL) 10 is added as a certified guest operating system. For a complete list of supported guest operating systems, see Certified Guest Operating Systems in OpenShift Virtualization.

* You can now update the machine type of multiple virtual machines (VMs) at the same time from the OpenShift CLI (`oc`).

## Networking

* You can now configure a NodeNetworkConfigurationPolicy manifest to enable the Link Layer Discovery Protocol (LLDP) listener for all ethernet ports in your Red Hat OpenShift Container Platform cluster.
* You can now create a new Node Network Configuration Policy (NNCP) in the topology view of the cluster and see its graphical representation in real time. Clicking Create on the Node network configuration page opens a form for configuring the elements of the new NNCP, and the NNCP is also displayed as a diagram.

* You can now use the OVN-Kubernetes localnet network topology to connect a VM to a secondary user-defined network.

## Storage

* You can now use a PVC as the source of a custom DataImportCron in the dataImportCronTemplates section of the HyperConverged custom resource (CR). See Managing automatic boot source updates for more information.

## Web console

* You can now configure multiple IOThreads for virtual machines that use fast storage, such as SSD (solid-state drive) or NVMe (non-volatile memory express). This improves I/O performance by enabling multiple threads for disk access.

* On the VirtualMachines page, you can now see the summary of CPU, memory, and storage usage by your VMs. To restrict this summary to the VMs in a specific project, select the project name in the tree view.

* On the VirtualMachines page, you can now navigate between your VMs by using the tree view.

* An attempt to stop, restart, or pause a VM or multiple VMs now displays a confirmation dialog.

* You can now access the commands of the Options menu . from the tree view by right-clicking the VM. If you right-click a project and select a command, the action is applied to all VMs in the project.
* You can now perform bulk actions on multiple virtual machines (VMs), including adding or removing labels, viewing the number of VMs selected for deletion, and moving VMs to a folder within the same namespace.

* On the VirtualMachines page, you can now use the tree view to organize VMs in folders and drag and drop VMs to these folders.

* You can now search for virtual machines by fields such as name, project, description, labels, date created, vCPU, and memory. You can also save frequently used search queries.

## Monitoring

* The following alerts for the OpenShift Virtualization Operator are now included in the Red Hat OpenShift Container Platform runbooks:
* HAControlPlaneDown
* HighCPUWorkload
* NodeNetworkInterfaceDown

* New metrics are now available and improve the observability of virtual machines (VMs) and virtual machine instances (VMIs). You can use these metrics to monitor the following VM lifecycle events, resource usage, and migration details:
* Migration metrics
* vNIC networking information metrics
* Allocated storage size metrics for running and stopped VMs

In addition, the following VM and VMI metadata metrics are now available:
* The pod_name label in kubevirt_vmi_info
* UID in VM and VMI metrics
* The VM creation date

For a complete list of virtualization metrics, see KubeVirt components metrics.

## Notable technical changes

* VirtualMachines that use instance types and preferences no longer have their specification mutated at runtime to include derived metadata, such as revisionName. This metadata is now stored in the status field to preserve the declarative VM specification and ensure compatibility.

* In OpenShift Virtualization 4.19, the default permissions for live migration have changed to improve cluster security. Users must now be explicitly granted the kubevirt.io:migrate cluster role to create, delete, or update live migration requests. Previously, namespace administrators had these permissions by default. For more information, see About live migration permissions.

# Deprecated and removed features

## Deprecated features

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.

* The OperatorConditionsUnhealthy alert is deprecated. You can safely silence it.

* The following HyperConverged custom resource (CR) fields have been deprecated and copied from their original location under the spec.featureGates fields to a new location in the spec field, where they can be used if needed:
* DeployVmConsoleProxy
* EnableApplicationAwareQuota
* EnableCommonBootImageImport

If used in the spec.featureGates location, the old fields are ignored.

# Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

Technology Preview Features Support Scope

* With the release of OpenShift Virtualization 4.19.1, you can install OpenShift Virtualization on Oracle Cloud Infrastructure (OCI). For more information, see OpenShift Virtualization and Oracle Cloud Infrastructure known issues and limitations in the Red Hat Knowledgebase, and Installing OpenShift Virtualization on OCI on GitHub.

* You can install OpenShift Virtualization on Google Cloud Platform (GCP). For more information, see OpenShift Virtualization and Google Cloud Platform known storage issues and limitations in the Red Hat Knowledgebase.

* You can now manage the link state of a primary or secondary virtual machine (VM) interface by using the Red Hat OpenShift Container Platform web console or the CLI.

* You can install OpenShift Virtualization on Azure Red Hat OpenShift (ARO). For more information, see OpenShift Virtualization for Azure Red Hat OpenShift (preview) in the Microsoft documentation.

* The DevKubeVirtRelieveAndMigrate descheduler profile is now available. This profile enhances the LongLifecycle profile by supporting load-aware descheduling, dynamic soft taints, and improved workload rebalancing.

* You can now deploy OpenShift Virtualization on IPv6 single-stack clusters. Support for IPv6 single-stack is limited to the OVN-Kubernetes localnet and Linux bridge Container Network Interface (CNI) plugins.

* By using the Red Hat OpenShift Container Platform web console, you can now migrate VMs in bulk from one storage class to another storage class.

# Known issues

## Networking

* When you update from Red Hat OpenShift Container Platform 4.12 to a newer minor version, VMs that use the cnv-bridge Container Network Interface (CNI) fail to live migrate. (https://access.redhat.com/solutions/7069807)
* As a workaround, change the spec.config.type field in your NetworkAttachmentDefinition manifest from cnv-bridge to bridge before performing the update.

## Nodes

* Uninstalling OpenShift Virtualization does not remove the feature.node.kubevirt.io node labels created by OpenShift Virtualization. You must remove the labels manually. (CNV-38543)

## Storage

* Restoring a snapshot of a virtual machine (VM) migrated using Migration Toolkit for Containers (MTC) fails. The restore creates a persistent volume claim (PVC) but not a data volume (DV). The VM spec references a DataVolumeTemplate missing from the volumes list. (CNV-61279)
* As a workaround, restart the VM after storage migration and before taking the snapshot. This creates a new controller revision that avoids the issue.

* If you perform storage class migration for a stopped VM, the VM might not be able to start because of a missing bootable device. To prevent this, do not attempt storage class migration if the VM is not running. (CNV-55104)

[IMPORTANT]
----
Storage class migration is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope.
----

## Virtualization

* When the mode of live migration is PostCopy, hot-plugging CPU or memory resource fails. (CNV-48348)

* OpenShift Virtualization links a service account token in use by a pod to that specific pod. OpenShift Virtualization implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (CNV-33835)
* As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

* When adding a virtual Trusted Platform Module (vTPM) device to a Windows VM, the BitLocker Drive Encryption system check passes even if the vTPM device is not persistent. This is because a vTPM device that is not persistent stores and recovers encryption keys using ephemeral storage for the lifetime of the virt-launcher pod. When the VM migrates or is shut down and restarts, the vTPM data is lost. (CNV-36448)

## IBM Z and IBM LinuxONE

* If you create a VM from a template and select Boot from CD, the VM fails to boot and the error unsupported configuration: SATA is not supported with this QEMU binary is logged. This occurs because the CD-ROM is automatically mounted as a SATA device, which is not supported on s390x architecture. (CNV-61740)
* As a workaround, navigate to the VM's Configuration ->  Storage tab, select the CD-ROM, and change the interface type from SATA to SCSI.

* GPU devices appear in the Hardware Devices list for s390x VMs, but GPU support is not available for s390x architecture. You can disregard these list entries. (CNV-61957)

* When you create a VM by using Red Hat Enterprise Linux (RHEL) container disk images for s390x architecture, call traces referencing virtio_balloon free page reporting print to the VM console. This is due to a kernel bug. (OCPBUGS-51113)
* As a workaround, disable memory ballooning for the VM by adding the following parameter to the VM YAML configuration: spec.domain.devices.autoattachMemBalloon: false.

You can also disable free page reporting of memory ballooning for all new VMs. To do so, edit the HyperConverged CR and add the parameter spec.virtualMachineOptions.disableFreePageReporting: true.

* VMs based on s390x architecture can only use the IPL boot mode. However, in the Red Hat OpenShift Container Platform web console, the Boot mode list for s390x VMs incorrectly includes BIOS, UEFI, and UEFI (secure) boot modes. If you select one of these modes for an s390x-based VM, the operation fails. (CNV-56889)

* In the Red Hat OpenShift Container Platform web console, it is erroneously possible to define multiple CPU threads for a VM based on s390x architecture. If you define multiple CPU threads, the VM enters a CrashLoopBackOff state with the qemu-kvm: S390 does not support more than 1 threads error. (CNV-56890)

# Maintenance releases

Release notes for asynchronous releases of Red Hat OpenShift Virtualization.

## 4.19.1

* With the new IBM Fusion Access for SAN, you can now deploy VMs on a scalable, clustered file system in Red Hat OpenShift Virtualization. Fusion Access for SAN offers access to consolidated, block-level data storage. It presents storage devices such as disk arrays to the operating system as if they were direct-attached storage.

The Fusion Access for SAN Operator is available in the Red Hat OpenShift Container Platform Operator hub.

See About IBM Fusion Access for SAN for more information.

* When a file system in Fusion Access for SAN has two local disks and one local disk fails, both local disks move to the Unknown state, with no indication which of the local disks failed. (OCPNAS-56)

* When creating more than one file system for VM storage in Fusion Access for SAN, deleting the initial primary file system results in all of the remaining file systems becoming unusable. You cannot migrate or restart any of the VMs running on the remaining file systems, and you cannot create new VMs on the remaining file systems.

To determine which file system is the primary file system, run the following command:

```terminal
$ oc get cso -n ibm-spectrum-scale-csi ibm-spectrum-scale-csi -o jsonpath='{.spec.clusters[*].primary.primaryFs}'
```


(OCPNAS-61)

* When a disruption occurs between the worker nodes in a Fusion Access for SAN storage cluster and the shared LUNs they are connected to, the VMs on the storage cluster pause and cannot be unpaused even after the service was restored. The only way to recover the VM is to restart it. (OCPNAS-62)

* Storage live migration from ODF to Fusion Access for SAN using MTC (v1.8.6) only works when the target access mode is specified as RWO. However, Fusion Access for SAN uses filesystem/RWX by default.

When you migrate from ODF to Fusion Access for SAN (RWO) you receive the following error in the VM logs:

```text
message: 'cannot migrate VMI: PVC dv-fedora000-mig-hwtp is not shared, live migration
  requires that all PVCs must be shared (using ReadWriteMany access mode)'
reason: DisksNotLiveMigratable
```


This results in the VM being inaccessible when the worker node is not available.

(OCPNAS-77)

* When you create a new file system in Fusion Access for SAN with the same name as an existing file system, an error appears, and the Create file system button is stuck displaying a loading spinner. If you reload the page, it lists only the original file system. However, if you try to create another new file system, the LUNs you selected for the second file system no longer appear as available. (OCPNAS-81)

* If a Fusion Access for SAN file system is filled to its maximum capacity, the mmhealth state of the file system custom resource (CR) becomes Degraded. This is caused by the no_disk_space_warn event. After freeing disk space, you can once again use the file system, but the file system keeps the Degraded status. (OCPNAS-110)

* When using a multipath LUN in Fusion Access for SAN, removing a local disk does not remove the partition. (OCPNAS-124)
* As a workaround, run the following commands on one of the nodes:

```terminal
$ oc multipath -f <device>
```


```terminal
$ oc multipath -r
```


Running these commands on one of the nodes fixes all of the nodes.

* LUNs used to create a file system in Fusion Access for SAN still appear as available for use until the file system moves from the Creating state to the Healthy state. This can result in users creating an additional file system with LUNs that that are already in use. After the first file system shifts to the Healthy state, the LUNs disappear from the second file system. (OCPNAS-126)

* Fusion Access for SAN formats disks with existing partitions that are not Fusion Access for SAN related. When attempting to add a new iSCSI target with an existing partition and data, Fusion Access for SAN automatically formats the share without warning. (OCPNAS-143)

* Deleting a second file system in Fusion Access for SAN results in the following error:

```text
Your focus-trap must have at least one container with at least one tabbable node in it at all times.
```


(OCPNAS-163)
** As a workaround, reload the page and delete the second file system.

* If your credentials for the image registry used to install Fusion Access for SAN change, you must delete the kmm-registry-push-pull-secret pull secret in the ibm-fusion-access namespace.Then you must restart the fusion-access-operator-controller-manager pod in the ibm-fusion-access namespace. (OCPNAS-170)

* If you change the KMM settings that trigger a rebuild while the Fusion Access for SAN storage cluster is running and using the kernel modules, KMM cannot unload the modules, resulting in an error. (OCPNAS-172)

* When backing up VMs with OADP datamover on a Fusion Access for SAN storage cluster, the process remains in the Pending state for a long time before shifting to the Bound state and beginning the backup. The process might even remain in Pending until it times out completely. (OCPNAS-175)

* When creating a file system, it may take over twenty minutes for the Status of the new file system to change from Creating to Healthy. During that time, the Status appears stuck in Creating, and the following error message appears when you click on the status:

```text
Failed to create filesystem. Check the operator log for more details.
```


This error is not correct.

(OCPNAS-184)