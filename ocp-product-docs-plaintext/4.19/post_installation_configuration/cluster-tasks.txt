# Postinstallation cluster tasks


After installing Red Hat OpenShift Container Platform, you can further expand and customize your cluster to your requirements.

# Available cluster customizations

You complete most of the cluster configuration and customization after you deploy your Red Hat OpenShift Container Platform cluster. A number of configuration resources are available.


[NOTE]
----
If you install your cluster on IBM Z(R), not all features and functions are available.
----

You modify the configuration resources to configure the major features of the
cluster, such as the image registry, networking configuration, image build
behavior, and the identity provider.

For current documentation of the settings that you control by using these resources, use
the oc explain command, for example oc explain builds --api-version=config.openshift.io/v1

## Cluster configuration resources

All cluster configuration resources are globally scoped (not namespaced) and named cluster.



## Operator configuration resources

These configuration resources are cluster-scoped instances, named cluster, which control the behavior of a specific component as
owned by a particular Operator.



## Additional configuration resources

These configuration resources represent a single instance of a particular component. In some cases, you can request multiple
instances by creating multiple instances of the resource. In other cases, the Operator can use only a specific
resource instance name in a specific namespace. Reference the component-specific
documentation for details on how and when you can create additional resource instances.



## Informational Resources

You use these resources to retrieve information about the cluster. Some configurations might require you to edit these resources directly.



# Adding worker nodes

After you deploy your Red Hat OpenShift Container Platform cluster, you can add worker nodes to scale cluster resources. There are different ways you can add worker nodes depending on the installation method and the environment of your cluster.

## Adding worker nodes to an on-premise cluster

For on-premise clusters, you can add worker nodes by using the Red Hat OpenShift Container Platform CLI (oc) to generate an ISO image, which can then be used to boot one or more nodes in your target cluster.
This process can be used regardless of how you installed your cluster.

You can add one or more nodes at a time while customizing each node with more complex configurations, such as static network configuration, or you can specify only the MAC address of each node.
Any configurations that are not specified during ISO generation are retrieved from the target cluster and applied to the new nodes.

Preflight validation checks are also performed when booting the ISO image to inform you of failure-causing issues before you attempt to boot each node.

Adding worker nodes to an on-premise cluster

## Adding worker nodes to installer-provisioned infrastructure clusters

For installer-provisioned infrastructure clusters, you can manually or automatically scale the MachineSet object to match the number of available bare-metal hosts.

To add a bare-metal host, you must configure all network prerequisites, configure an associated baremetalhost object, then provision the worker node to the cluster. You can add a bare-metal host manually or by using the web console.

* Adding worker nodes using the web console
* Adding worker nodes using YAML in the web console
* Manually adding a worker node to an installer-provisioned infrastructure cluster

## Adding worker nodes to user-provisioned infrastructure clusters

For user-provisioned infrastructure clusters, you can add worker nodes by using a RHEL or RHCOS ISO image and connecting it to your cluster using cluster Ignition config files. For RHEL worker nodes, the following example uses Ansible playbooks to add worker nodes to the cluster. For RHCOS worker nodes, the following example uses an ISO image and network booting to add worker nodes to the cluster.

* Adding RHCOS worker nodes to a user-provisioned infrastructure cluster

## Adding worker nodes to clusters managed by the Assisted Installer

For clusters managed by the Assisted Installer, you can add worker nodes by using the Red Hat OpenShift Cluster Manager console, the Assisted Installer REST API or you can manually add worker nodes using an ISO image and cluster Ignition config files.

* Adding worker nodes using the OpenShift Cluster Manager
* Adding worker nodes using the Assisted Installer REST API
* Manually adding worker nodes to a SNO cluster

## Adding worker nodes to clusters managed by the multicluster engine for Kubernetes

For clusters managed by the multicluster engine for Kubernetes, you can add worker nodes by using the dedicated multicluster engine console.

* Creating your cluster with the console

# Adjust worker nodes

If you incorrectly sized the worker nodes during deployment, adjust them by creating one or more new compute machine sets, scale them up, then scale the original compute machine set down before removing them.

## Understanding the difference between compute machine sets and the machine config pool

MachineSet objects describe Red Hat OpenShift Container Platform nodes with respect to the cloud or machine provider.

The MachineConfigPool object allows MachineConfigController components to define and provide the status of machines in the context of upgrades.

The MachineConfigPool object allows users to configure how upgrades are rolled out to the Red Hat OpenShift Container Platform nodes in the machine config pool.

The NodeSelector object can be replaced with a reference to the MachineSet object.

## Scaling a compute machine set manually

To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.

This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.

* Install an Red Hat OpenShift Container Platform cluster and the oc command line.
* Log in to  oc as a user with cluster-admin permission.

1. View the compute machine sets that are in the cluster by running the following command:

```terminal
$ oc get machinesets.machine.openshift.io -n openshift-machine-api
```


The compute machine sets are listed in the form of <clusterid>-worker-<aws-region-az>.
2. View the compute machines that are in the cluster by running the following command:

```terminal
$ oc get machines.machine.openshift.io -n openshift-machine-api
```

3. Set the annotation on the compute machine that you want to delete by running the following command:

```terminal
$ oc annotate machines.machine.openshift.io/<machine_name> -n openshift-machine-api machine.openshift.io/delete-machine="true"
```

4. Scale the compute machine set by running one of the following commands:

```terminal
$ oc scale --replicas=2 machinesets.machine.openshift.io <machineset> -n openshift-machine-api
```


Or:

```terminal
$ oc edit machinesets.machine.openshift.io <machineset> -n openshift-machine-api
```


[TIP]
----
You can alternatively apply the following YAML to scale the compute machine set:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: <machineset>
  namespace: openshift-machine-api
spec:
  replicas: 2
```

----

You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.

[IMPORTANT]
----
By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.
You can skip draining the node by annotating machine.openshift.io/exclude-node-draining in a specific machine.
----

* Verify the deletion of the intended machine by running the following command:

```terminal
$ oc get machines.machine.openshift.io
```


## The compute machine set deletion policy

Random, Newest, and Oldest are the three supported deletion options. The default is Random, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:


```yaml
spec:
  deletePolicy: <delete_policy>
  replicas: <desired_replica_count>
```


Specific machines can also be prioritized for deletion by adding the annotation machine.openshift.io/delete-machine=true to the machine of interest, regardless of the deletion policy.


[IMPORTANT]
----
By default, the Red Hat OpenShift Container Platform router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to 0 unless you first relocate the router pods.
----


[NOTE]
----
Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.
----

## Creating default cluster-wide node selectors

You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.

With cluster-wide node selectors, when you create a pod in that cluster, Red Hat OpenShift Container Platform adds the default node selectors to the pod and schedules
the pod on nodes with matching labels.

You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.


[NOTE]
----
You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.
----

To add a default cluster-wide node selector:

1. Edit the Scheduler Operator CR to add the default cluster-wide node selectors:

```terminal
$ oc edit scheduler cluster
```

Example Scheduler Operator CR with a node selector

```yaml
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...
spec:
  defaultNodeSelector: type=user-node,region=east 1
  mastersSchedulable: false
```

Add a node selector with the appropriate <key>:<value> pairs.

After making this change, wait for the pods in the openshift-kube-apiserver project to redeploy. This can take several minutes. The default cluster-wide node selector does not take effect until the pods redeploy.
2. Add labels to a node by using a compute machine set or editing the node directly:
* Use a compute machine set to add labels to nodes managed by the compute machine set when a node is created:
1. Run the following command to add labels to a MachineSet object:

```terminal
$ oc patch MachineSet <name> --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"<key>"="<value>","<key>"="<value>"}}]'  -n openshift-machine-api 1
```

Add a <key>/<value> pair for each label.

For example:

```terminal
$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api
```


[TIP]
----
You can alternatively apply the following YAML to add labels to a compute machine set:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: <machineset>
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"
```

----
2. Verify that the labels are added to the MachineSet object by using the oc edit command:

For example:

```terminal
$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api
```

Example MachineSet object

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
  ...
spec:
  ...
  template:
    metadata:
  ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
  ...
```

3. Redeploy the nodes associated with that compute machine set by scaling down to 0 and scaling up the nodes:

For example:

```terminal
$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api
```


```terminal
$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api
```

4. When the nodes are ready and available, verify that the label is added to the nodes by using the oc get command:

```terminal
$ oc get nodes -l <key>=<value>
```


For example:

```terminal
$ oc get nodes -l type=user-node
```

Example output

```terminal
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.32.3
```

* Add labels directly to a node:
1. Edit the Node object for the node:

```terminal
$ oc label nodes <name> <key>=<value>
```


For example, to label a node:

```terminal
$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49 type=user-node region=east
```


[TIP]
----
You can alternatively apply the following YAML to add labels to a node:

```yaml
kind: Node
apiVersion: v1
metadata:
  name: <node_name>
  labels:
    type: "user-node"
    region: "east"
```

----
2. Verify that the labels are added to the node using the oc get command:

```terminal
$ oc get nodes -l <key>=<value>,<key>=<value>
```


For example:

```terminal
$ oc get nodes -l type=user-node,region=east
```

Example output

```terminal
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.32.3
```


# Improving cluster stability in high latency environments using worker latency profiles

If the cluster administrator has performed latency tests for platform verification, they can discover the need to adjust the operation of the cluster to ensure stability in cases of high latency. The cluster administrator needs to change only one parameter, recorded in a file, which controls four parameters affecting how supervisory processes read status and interpret the health of the cluster. Changing only the one parameter provides cluster tuning in an easy, supportable manner.

The Kubelet process provides the starting point for monitoring cluster health. The Kubelet sets status values for all nodes in the Red Hat OpenShift Container Platform cluster. The Kubernetes Controller Manager (kube controller) reads the status values every 10 seconds, by default.
If the kube controller cannot read a node status value, it loses contact with that node after a configured period. The default behavior is:

1. The node controller on the control plane updates the node health to Unhealthy and marks the node Ready condition`Unknown`.
2. In response, the scheduler stops scheduling pods to that node.
3. The Node Lifecycle Controller adds a node.kubernetes.io/unreachable taint with a NoExecute effect to the node and schedules any pods on the node for eviction after five minutes, by default.

This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager might not receive an update from a healthy node due to network latency. The Kubelet evicts pods from the node even though the node is healthy.

To avoid this problem, you can use worker latency profiles to adjust the frequency that the Kubelet and the Kubernetes Controller Manager wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly if network latency between the control plane and the worker nodes is not optimal.

These worker latency profiles contain three sets of parameters that are predefined with carefully tuned values to control the reaction of the cluster to increased latency. There is no need to experimentally find the best values manually.

You can configure worker latency profiles when installing a cluster or at any time you notice increased latency in your cluster network.

## Understanding worker latency profiles

Worker latency profiles are four different categories of carefully-tuned parameters. The four parameters which implement these values are node-status-update-frequency, node-monitor-grace-period, default-not-ready-toleration-seconds and default-unreachable-toleration-seconds. These parameters can use values which allow you to control the reaction of the cluster to latency issues without needing to determine the best values by using manual methods.


[IMPORTANT]
----
Setting these parameters manually is not supported. Incorrect parameter settings adversely affect cluster stability.
----

All worker latency profiles configure the following parameters:

node-status-update-frequency:: Specifies how often the kubelet posts node status to the API server.
node-monitor-grace-period:: Specifies the amount of time in seconds that the Kubernetes Controller Manager waits for an update from a kubelet before marking the node unhealthy and adding the node.kubernetes.io/not-ready or node.kubernetes.io/unreachable taint to the node.
default-not-ready-toleration-seconds:: Specifies the amount of time in seconds after marking a node unhealthy that the Kube API Server Operator waits before evicting pods from that node.
default-unreachable-toleration-seconds:: Specifies the amount of time in seconds after marking a node unreachable that the Kube API Server Operator waits before evicting pods from that node.

The following Operators monitor the changes to the worker latency profiles and respond accordingly:

* The Machine Config Operator (MCO) updates the node-status-update-frequency parameter on the worker nodes.
* The Kubernetes Controller Manager updates the node-monitor-grace-period parameter on the control plane nodes.
* The Kubernetes API Server Operator updates the default-not-ready-toleration-seconds and default-unreachable-toleration-seconds parameters on the control plane nodes.

Although the default configuration works in most cases, Red Hat OpenShift Container Platform offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:

Default worker latency profile:: With the Default profile, each Kubelet updates its status every 10 seconds (node-status-update-frequency). The Kube Controller Manager checks the statuses of Kubelet every 5 seconds.

The Kubernetes Controller Manager waits 40 seconds (node-monitor-grace-period) for a status update from Kubelet before considering the Kubelet unhealthy. If no status is made available to the Kubernetes Controller Manager, it then marks the node with the node.kubernetes.io/not-ready or node.kubernetes.io/unreachable taint and evicts the pods on that node.

If a pod is on a node that has the NoExecute taint, the pod runs according to tolerationSeconds. If the node has no taint, it will be evicted in 300 seconds (default-not-ready-toleration-seconds and default-unreachable-toleration-seconds settings of the Kube API Server).

Medium worker latency profile:: Use the MediumUpdateAverageReaction profile if the network latency is slightly higher than usual.

The MediumUpdateAverageReaction profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the tolerationSeconds parameter, the eviction waits for the period specified by that parameter.

The Kubernetes Controller Manager waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.

Low worker latency profile:: Use the LowUpdateSlowReaction profile if the network latency is extremely high.

The LowUpdateSlowReaction profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the tolerationSeconds parameter, the eviction waits for the period specified by that parameter.

The Kubernetes Controller Manager waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.



[NOTE]
----
The latency profiles do not support custom machine config pools, only the default worker machine config pools.
----

## Using and changing worker latency profiles

To change a worker latency profile to deal with network latency, edit the node.config object to add the name of the profile. You can change the profile at any time as latency increases or decreases.

You must move one worker latency profile at a time. For example, you cannot move directly from the Default profile to the LowUpdateSlowReaction worker latency profile. You must move from the Default worker latency profile to the MediumUpdateAverageReaction profile first, then to LowUpdateSlowReaction. Similarly, when returning to the Default profile, you must move from the low profile to the medium profile first, then to Default.


[NOTE]
----
You can also configure worker latency profiles upon installing an Red Hat OpenShift Container Platform cluster.
----

To move from the default worker latency profile:

1. Move to the medium worker latency profile:
1. Edit the node.config object:

```terminal
$ oc edit nodes.config/cluster
```

2. Add spec.workerLatencyProfile: MediumUpdateAverageReaction:
Example node.config object

```yaml
apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction 1

# ...
```

Specifies the medium worker latency policy.

Scheduling on each worker node is disabled as the change is being applied.
2. Optional: Move to the low worker latency profile:
1. Edit the node.config object:

```terminal
$ oc edit nodes.config/cluster
```

2. Change the spec.workerLatencyProfile value to LowUpdateSlowReaction:
Example node.config object

```yaml
apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction 1

# ...
```

Specifies use of the low worker latency policy.

Scheduling on each worker node is disabled as the change is being applied.

* When all nodes return to the Ready condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:

```terminal
$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5
```

Example output

```terminal
# ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" 1
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...
```

Specifies that the profile is applied and active.

To change the medium profile to default or change the default to medium, edit the node.config object and set the spec.workerLatencyProfile parameter to the appropriate value.

# Managing control plane machines

Control plane machine sets provide management capabilities for control plane machines that are similar to what compute machine sets provide for compute machines. The availability and initial status of control plane machine sets on your cluster depend on your cloud provider and the version of Red Hat OpenShift Container Platform that you installed. For more information, see Getting started with control plane machine sets.

# Creating infrastructure machine sets for production environments

You can create a compute machine set to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.

In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.

For information on infrastructure nodes and which components can run on infrastructure nodes, see Creating infrastructure machine sets.

To create an infrastructure node, you can use a machine set, assign a label to the nodes, or use a machine config pool.

For sample machine sets that you can use with these procedures, see Creating machine sets for different clouds.

Applying a specific node selector to all infrastructure components causes Red Hat OpenShift Container Platform to schedule those workloads on nodes with that label.

## Creating a compute machine set

In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.

* Deploy an Red Hat OpenShift Container Platform cluster.
* Install the OpenShift CLI (oc).
* Log in to oc as a user with cluster-admin permission.

1. Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <file_name>.yaml.

Ensure that you set the <clusterID> and <role> parameter values.
2. Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
1. To list the compute machine sets in your cluster, run the following command:

```terminal
$ oc get machinesets -n openshift-machine-api
```

Example output

```terminal
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
```

2. To view values of a specific compute machine set custom resource (CR), run the following command:

```terminal
$ oc get machineset <machineset_name> \
  -n openshift-machine-api -o yaml
```


```yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> 1
  name: <infrastructure_id>-<role> 2
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
        machine.openshift.io/cluster-api-machine-role: <role>
        machine.openshift.io/cluster-api-machine-type: <role>
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
    spec:
      providerSpec: 3
        ...
```

The cluster infrastructure ID.
A default node label.

[NOTE]
----
For clusters that have user-provisioned infrastructure, a compute machine set can only create worker and infra type machines.
----
The values in the <providerSpec> section of the compute machine set CR are platform-specific. For more information about <providerSpec> parameters in the CR, see the sample compute machine set CR configuration for your provider.
3. Create a MachineSet CR by running the following command:

```terminal
$ oc create -f <file_name>.yaml
```


* View the list of compute machine sets by running the following command:

```terminal
$ oc get machineset -n openshift-machine-api
```

Example output

```terminal
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
```


When the new compute machine set is available, the DESIRED and CURRENT values match. If the compute machine set is not available, wait a few minutes and run the command again.

## Creating an infrastructure node


[IMPORTANT]
----
See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
----

Requirements of the cluster dictate that infrastructure (infra) nodes, be provisioned. The installation program provisions only control plane and worker nodes. Worker nodes can be designated as infrastructure nodes through labeling. You can then use taints and tolerations to move appropriate workloads to the infrastructure nodes. For more information, see "Moving resources to infrastructure machine sets".

You can optionally create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces and creates an intersection with any existing node selectors on a pod, which additionally constrains the pod&#8217;s selector.


[IMPORTANT]
----
If the default node selector key conflicts with the key of a pod's label, then the default node selector is not applied.
However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as node-role.kubernetes.io/infra="", when a pod's label is set to a different node role, such as node-role.kubernetes.io/master="", can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.
You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.
----

1. Add a label to the worker nodes that you want to act as infrastructure nodes:

```terminal
$ oc label node <node-name> node-role.kubernetes.io/infra=""
```

2. Check to see if applicable nodes now have the infra role:

```terminal
$ oc get nodes
```

3. Optional: Create a default cluster-wide node selector:
1. Edit the Scheduler object:

```terminal
$ oc edit scheduler cluster
```

2. Add the defaultNodeSelector field with the appropriate node selector:

```yaml
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: node-role.kubernetes.io/infra="" 1
# ...
```

This example node selector deploys pods on infrastructure nodes by default.
3. Save the file to apply the changes.

You can now move infrastructure resources to the new infrastructure nodes. Also, remove any workloads that you do not want, or that do not belong, on the new infrastructure node. See the list of workloads supported for use on infrastructure nodes in "Red Hat OpenShift Container Platform infrastructure components".

* For information on how to configure project node selectors to avoid cluster-wide node selector key conflicts, see Project node selectors.

## Creating a machine config pool for infrastructure machines

If you need infrastructure machines to have dedicated configurations, you must create an infra pool.


[IMPORTANT]
----
Creating a custom machine configuration pool overrides default worker pool configurations if they refer to the same file or unit.
----

1. Add a label to the node you want to assign as the infra node with a specific label:

```terminal
$ oc label node <node_name> <label>
```


```terminal
$ oc label node ci-ln-n8mqwr2-f76d1-xscn2-worker-c-6fmtx node-role.kubernetes.io/infra=
```

2. Create a machine config pool that contains both the worker role and your custom role as machine config selector:

```terminal
$ cat infra.mcp.yaml
```

Example output

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} 1
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: "" 2
```

Add the worker role and your custom role.
Add the label you added to the node as a nodeSelector.

[NOTE]
----
Custom machine config pools inherit machine configs from the worker pool. Custom pools use any machine config targeted for the worker pool, but add the ability to also deploy changes that are targeted at only the custom pool. Because a custom pool inherits resources from the worker pool, any change to the worker pool also affects the custom pool.
----
3. After you have the YAML file, you can create the machine config pool:

```terminal
$ oc create -f infra.mcp.yaml
```

4. Check the machine configs to ensure that the infrastructure configuration rendered successfully:

```terminal
$ oc get machineconfig
```

Example output

```terminal
NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
00-worker                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
01-master-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
01-master-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
01-worker-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
01-worker-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
99-master-1ae2a1e0-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
99-master-ssh                                                                                          3.2.0             31d
99-worker-1ae64748-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             31d
99-worker-ssh                                                                                          3.2.0             31d
rendered-infra-4e48906dca84ee702959c71a53ee80e7             365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             23m
rendered-master-072d4b2da7f88162636902b074e9e28e            5b6fb8349a29735e48446d435962dec4547d3090   3.5.0             31d
rendered-master-3e88ec72aed3886dec061df60d16d1af            02c07496ba0417b3e12b78fb32baf6293d314f79   3.5.0             31d
rendered-master-419bee7de96134963a15fdf9dd473b25            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             17d
rendered-master-53f5c91c7661708adce18739cc0f40fb            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             13d
rendered-master-a6a357ec18e5bce7f5ac426fc7c5ffcd            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             7d3h
rendered-master-dc7f874ec77fc4b969674204332da037            5b6fb8349a29735e48446d435962dec4547d3090   3.5.0             31d
rendered-worker-1a75960c52ad18ff5dfa6674eb7e533d            5b6fb8349a29735e48446d435962dec4547d3090   3.5.0             31d
rendered-worker-2640531be11ba43c61d72e82dc634ce6            5b6fb8349a29735e48446d435962dec4547d3090   3.5.0             31d
rendered-worker-4e48906dca84ee702959c71a53ee80e7            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             7d3h
rendered-worker-4f110718fe88e5f349987854a1147755            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             17d
rendered-worker-afc758e194d6188677eb837842d3b379            02c07496ba0417b3e12b78fb32baf6293d314f79   3.5.0             31d
rendered-worker-daa08cc1e8f5fcdeba24de60cd955cc3            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.5.0             13d
```


You should see a new machine config, with the rendered-infra-* prefix.
5. Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as infra. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.

[NOTE]
----
After you create the new machine config pool, the MCO generates a new rendered config for that pool, and associated nodes of that pool reboot to apply the new configuration.
----
1. Create a machine config:

```terminal
$ cat infra.mc.yaml
```

Example output

```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 51-infra
  labels:
    machineconfiguration.openshift.io/role: infra 1
spec:
  config:
    ignition:
      version: 3.5.0
    storage:
      files:
      - path: /etc/infratest
        mode: 0644
        contents:
          source: data:,infra
```

Add the label you added to the node as a nodeSelector.
2. Apply the machine config to the infra-labeled nodes:

```terminal
$ oc create -f infra.mc.yaml
```

6. Confirm that your new machine config pool is available:

```terminal
$ oc get mcp
```

Example output

```terminal
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-infra-60e35c2e99f42d976e084fa94da4d0fc    True      False      False      1              1                   1                     0                      4m20s
master   rendered-master-9360fdb895d4c131c7c4bebbae099c90   True      False      False      3              3                   3                     0                      91m
worker   rendered-worker-60e35c2e99f42d976e084fa94da4d0fc   True      False      False      2              2                   2                     0                      91m
```


In this example, a worker node was changed to an infra node.

* See Node configuration management with machine config pools for more information on grouping infra machines in a custom pool.

# Assigning machine set resources to infrastructure nodes

After creating an infrastructure machine set, the worker and infra roles are applied to new infra nodes. Nodes with the infra role are not counted toward the total number of subscriptions that are required to run the environment, even when the worker role is also applied.

However, when an infra node is assigned the worker role, there is a chance that user workloads can get assigned inadvertently to the infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods that you want to control.

## Binding infrastructure node workloads using taints and tolerations

If you have an infrastructure node that has the infra and worker roles assigned, you must configure the node so that user workloads are not assigned to it.


[IMPORTANT]
----
It is recommended that you preserve the dual infra,worker label that is created for infrastructure nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the worker label from the node, you must create a custom pool to manage it. A node with a label other than master or worker is not recognized by the MCO without a custom pool. Maintaining the worker label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The infra label communicates to the cluster that it does not count toward the total number of subscriptions.
----

* Configure additional MachineSet objects in your Red Hat OpenShift Container Platform cluster.

1. Add a taint to the infrastructure node to prevent scheduling user workloads on it:
1. Determine if the node has the taint:

```terminal
$ oc describe nodes <node_name>
```

Sample output

```text
oc describe node ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Name:               ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Roles:              worker
 ...
Taints:             node-role.kubernetes.io/infra=reserved:NoSchedule
 ...
```


This example shows that the node has a taint. You can proceed with adding a toleration to your pod in the next step.
2. If you have not configured a taint to prevent scheduling user workloads on it:

```terminal
$ oc adm taint nodes <node_name> <key>=<value>:<effect>
```


For example:

```terminal
$ oc adm taint nodes node1 node-role.kubernetes.io/infra=reserved:NoSchedule
```


[TIP]
----
You can alternatively edit the pod specification to add the taint:

```yaml
apiVersion: v1
kind: Node
metadata:
  name: node1
# ...
spec:
  taints:
    - key: node-role.kubernetes.io/infra
      value: reserved
      effect: NoSchedule
# ...
```

----

These examples place a taint on node1 that has the node-role.kubernetes.io/infra key and the NoSchedule taint effect. Nodes with the NoSchedule effect schedule only pods that tolerate the taint, but allow existing pods to remain scheduled on the node.

[NOTE]
----
If a descheduler is used, pods violating node taints could be evicted from the cluster.
----
2. Add tolerations to the pods that you want to schedule on the infrastructure node, such as the router, registry, and monitoring workloads. Referencing the previous examples, add the following tolerations to the Pod object specification:

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:

# ...
spec:
# ...
  tolerations:
    - key: node-role.kubernetes.io/infra 1
      value: reserved 2
      effect: NoSchedule 3
      operator: Equal 4
```

Specify the key that you added to the node.
Specify the value of the key-value pair taint that you added to the node.
Specify the effect that you added to the node.
Specify the Equal Operator to require a taint with the key node-role.kubernetes.io/infra to be present on the node.

This toleration matches the taint created by the oc adm taint command. A pod with this toleration can be scheduled onto the infrastructure node.

[NOTE]
----
Moving pods for an Operator installed via OLM to an infrastructure node is not always possible. The capability to move Operator pods depends on the configuration of each Operator.
----
3. Schedule the pod to the infrastructure node by using a scheduler. See the documentation for "Controlling pod placement using the scheduler" for details.
4. Remove any workloads that you do not want, or that do not belong, on the new infrastructure node. See the list of workloads supported for use on infrastructure nodes in "Red Hat OpenShift Container Platform infrastructure components".

* See Controlling pod placement using the scheduler for general information on scheduling a pod to a node.

# Moving resources to infrastructure machine sets

Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created.

## Moving the router

You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.

* Configure additional compute machine sets in your Red Hat OpenShift Container Platform cluster.

1. View the IngressController custom resource for the router Operator:

```terminal
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml
```


The command output resembles the following text:

```yaml
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-18T12:35:39Z
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "11341"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 79509e05-61d6-11e9-bc55-02ce4781844a
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-18T12:36:15Z
    status: "True"
    type: Available
  domain: apps.<cluster>.example.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
```

2. Edit the ingresscontroller resource and change the nodeSelector to use the infra label:

```terminal
$ oc edit ingresscontroller default -n openshift-ingress-operator
```


```yaml
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2025-03-26T21:15:43Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
# ...
spec:
  nodePlacement:
    nodeSelector: 1
      matchLabels:
        node-role.kubernetes.io/infra: ""
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/infra
      value: reserved
# ...
```

Add a nodeSelector parameter with the appropriate value to the component you want to move. You can use a nodeSelector parameter in the format shown or use <key>: <value> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.
3. Confirm that the router pod is running on the infra node.
1. View the list of router pods and note the node name of the running pod:

```terminal
$ oc get pod -n openshift-ingress -o wide
```

Example output

```terminal
NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                           NOMINATED NODE   READINESS GATES
router-default-86798b4b5d-bdlvd   1/1      Running       0          28s       10.130.2.4   ip-10-0-217-226.ec2.internal   <none>           <none>
router-default-955d875f4-255g8    0/1      Terminating   0          19h       10.129.2.4   ip-10-0-148-172.ec2.internal   <none>           <none>
```


In this example, the running pod is on the ip-10-0-217-226.ec2.internal node.
2. View the node status of the running pod:

```terminal
$ oc get node <node_name> 1
```

Specify the <node_name> that you obtained from the pod list.
Example output

```terminal
NAME                          STATUS  ROLES         AGE   VERSION
ip-10-0-217-226.ec2.internal  Ready   infra,worker  17h   v1.32.3
```


Because the role list includes infra, the pod is running on the correct node.

## Moving the default registry

You configure the registry Operator to deploy its pods to different nodes.

* Configure additional compute machine sets in your Red Hat OpenShift Container Platform cluster.

1. View the config/instance object:

```terminal
$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
```

Example output

```yaml
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: 2019-02-05T13:52:05Z
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 1
  name: cluster
  resourceVersion: "56174"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 36fd3724-294d-11e9-a524-12ffeee2931b
spec:
  httpSecret: d9a012ccd117b1e6616ceccb2c3bb66a5fed1b5e481623
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read: {}
    write: {}
  storage:
    s3:
      bucket: image-registry-us-east-1-c92e88cad85b48ec8b312344dff03c82-392c
      region: us-east-1
status:
...
```

2. Edit the config/instance object:

```terminal
$ oc edit configs.imageregistry.operator.openshift.io/cluster
```


```yaml
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  name: cluster
# ...
spec:
  logLevel: Normal
  managementState: Managed
  nodeSelector: 1
    node-role.kubernetes.io/infra: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/infra
    value: reserved
```

Add a nodeSelector parameter with the appropriate value to the component you want to move. You can use a nodeSelector parameter in the format shown or use <key>: <value> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
3. Verify the registry pod has been moved to the infrastructure node.
1. Run the following command to identify the node where the registry pod is located:

```terminal
$ oc get pods -o wide -n openshift-image-registry
```

2. Confirm the node has the label you specified:

```terminal
$ oc describe node <node_name>
```


Review the command output and confirm that node-role.kubernetes.io/infra is in the LABELS list.

## Moving the monitoring solution

The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager.
The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have created the cluster-monitoring-config ConfigMap object.
* You have installed the OpenShift CLI (`oc`).

1. Edit the cluster-monitoring-config config map and change the nodeSelector to use the infra label:

```terminal
$ oc edit configmap cluster-monitoring-config -n openshift-monitoring
```


```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector: 1
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    metricsServer:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
```

Add a nodeSelector parameter with the appropriate value to the component you want to move. You can use a nodeSelector parameter in the format shown or use <key>: <value> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.
2. Watch the monitoring pods move to the new machines:

```terminal
$ watch 'oc get pod -n openshift-monitoring -o wide'
```

3. If a component has not moved to the infra node, delete the pod with this component:

```terminal
$ oc delete pod -n openshift-monitoring <pod>
```


The component from the deleted pod is re-created on the infra node.

# About the cluster autoscaler

The cluster autoscaler adjusts the size of an Red Hat OpenShift Container Platform cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.

The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.

The cluster autoscaler computes the total
memory, CPU, and GPU
on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.


[IMPORTANT]
----
Ensure that the maxNodesTotal value in the ClusterAutoscaler resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.
----

## Automatic node removal

Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:

* The node utilization is less than the node utilization level threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the ClusterAutoscaler custom resource, the cluster autoscaler uses a default value of 0.5, which corresponds to 50% utilization.
* The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.
* The cluster autoscaler does not have scale down disabled annotation.

If the following types of pods are present on a node, the cluster autoscaler will not remove the node:

* Pods with restrictive pod disruption budgets (PDBs).
* Kube-system pods that do not run on the node by default.
* Kube-system pods that do not have a PDB or have a PDB that is too restrictive.
* Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.
* Pods with local storage.
* Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.
* Unless they also have a "cluster-autoscaler.kubernetes.io/safe-to-evict": "true" annotation, pods that have a "cluster-autoscaler.kubernetes.io/safe-to-evict": "false" annotation.

For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.

## Limitations

If you configure the cluster autoscaler, additional usage restrictions apply:

* Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.
* Specify requests for your pods.
* If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.
* Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.
* Do not run additional node group autoscalers, especially the ones offered by your cloud provider.


[NOTE]
----
The cluster autoscaler only adds nodes in autoscaled node groups if doing so would result in a schedulable pod.
If the available node types cannot meet the requirements for a pod request, or if the node groups that could meet these requirements are at their maximum size, the cluster autoscaler cannot scale up.
----

## Interaction with other scheduling features

The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment&#8217;s or replica set&#8217;s number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.

The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.

Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.

## Cluster autoscaler resource definition

This ClusterAutoscaler resource definition shows the parameters and sample values for the cluster autoscaler.


[NOTE]
----
When you change the configuration of an existing cluster autoscaler, it restarts.
----


```yaml
apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 1
  resourceLimits:
    maxNodesTotal: 24 2
    cores:
      min: 8 3
      max: 128 4
    memory:
      min: 4 5
      max: 256 6
    gpus:
    - type: <gpu_type> 7
      min: 0 8
      max: 16 9
  logVerbosity: 4 10
  scaleDown: 11
    enabled: true 12
    delayAfterAdd: 10m 13
    delayAfterDelete: 5m 14
    delayAfterFailure: 30s 15
    unneededTime: 5m 16
    utilizationThreshold: "0.4" 17
  expanders: ["Random"] 18
```


Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The podPriorityThreshold value is compared to the value of the PriorityClass that you assign to each pod.
Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your MachineAutoscaler resources.
Specify the minimum number of cores to deploy in the cluster.
Specify the maximum number of cores to deploy in the cluster.
Specify the minimum amount of memory, in GiB, in the cluster.
Specify the maximum amount of memory, in GiB, in the cluster.
Optional: To configure the cluster autoscaler to deploy GPU-enabled nodes, specify a type value.
This value must match the value of the spec.template.spec.metadata.labels[cluster-api/accelerator] label in the machine set that manages the GPU-enabled nodes of that type.
For example, this value might be nvidia-t4 to represent Nvidia T4 GPUs, or nvidia-a10g for A10G GPUs.
For more information, see "Labeling GPU machine sets for the cluster autoscaler".
Specify the minimum number of GPUs of the specified type to deploy in the cluster.
Specify the maximum number of GPUs of the specified type to deploy in the cluster.
Specify the logging verbosity level between 0 and 10. The following log level thresholds are provided for guidance:
* 1: (Default) Basic information about changes.
* 4: Debug-level verbosity for troubleshooting typical issues.
* 9: Extensive, protocol-level debugging information.

If you do not specify a value, the default value of 1 is used.
In this section, you can specify the period to wait for each action by using any valid ParseDuration interval, including ns, us, ms, s, m, and h.
Specify whether the cluster autoscaler can remove unnecessary nodes.
Optional: Specify the period to wait before deleting a node after a node has recently been added. If you do not specify a value, the default value of 10m is used.
Optional: Specify the period to wait before deleting a node after a node has recently been deleted. If you do not specify a value, the default value of 0s is used.
Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of 3m is used.
Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of 10m is used.
Optional:  Specify the node utilization level. Nodes below this utilization level are eligible for deletion.

The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than "0" but less than "1". If you do not specify a value, the cluster autoscaler uses a default value of "0.5", which corresponds to 50% utilization. You must express this value as a string.
Optional: Specify any expanders that you want the cluster autoscaler to use.
The following values are valid:
* LeastWaste: Selects the machine set that minimizes the idle CPU after scaling.
If multiple machine sets would yield the same amount of idle CPU, the selection minimizes unused memory.
* Priority: Selects the machine set with the highest user-assigned priority.
To use this expander, you must create a config map that defines the priority of your machine sets.
For more information, see "Configuring a priority expander for the cluster autoscaler."
* Random: (Default) Selects the machine set randomly.

If you do not specify a value, the default value of Random is used.

You can specify multiple expanders by using the [LeastWaste, Priority] format.
The cluster autoscaler applies each expander according to the specified order.

In the [LeastWaste, Priority] example, the cluster autoscaler first evaluates according to the LeastWaste criteria.
If more than one machine set satisfies the LeastWaste criteria equally well, the cluster autoscaler then evaluates according to the Priority criteria.
If more than one machine set satisfies all of the specified expanders equally well, the cluster autoscaler selects one to use at random.


[NOTE]
----
When performing a scaling operation, the cluster autoscaler remains within the ranges set in the ClusterAutoscaler resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.
The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
----

## Deploying a cluster autoscaler

To deploy a {FeatureName}, you create an instance of the {FeatureResourceName} resource.

1. Create a YAML file for a {FeatureResourceName} resource that contains the custom resource definition.
2. Create the custom resource in the cluster by running the following command:

```terminal
$ oc create -f <filename>.yaml 1
```

<filename> is the name of the custom resource file.

# Applying autoscaling to your cluster

Applying autoscaling to an Red Hat OpenShift Container Platform cluster involves deploying a cluster autoscaler and then deploying machine autoscalers for each machine type in your cluster.

For more information, see Applying autoscaling to an Red Hat OpenShift Container Platform cluster.

# Enabling Technology Preview features using FeatureGates

You can turn on a subset of the current Technology Preview features on for all nodes in the cluster by editing the FeatureGate custom resource (CR).

## Understanding feature gates

You can use the FeatureGate custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of Red Hat OpenShift Container Platform features that are not enabled by default.

You can activate the following feature set by using the FeatureGate CR:

* TechPreviewNoUpgrade. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.

[WARNING]
----
Enabling the TechPreviewNoUpgrade feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
----

The following Technology Preview features are enabled by this feature set:
* External cloud providers. Enables support for external cloud providers for clusters on vSphere, AWS, Azure, and GCP. Support for OpenStack is GA. This is an internal feature that most users do not need to interact with. (ExternalCloudProvider)
* Swap memory on nodes. Enables swap memory use for Red Hat OpenShift Container Platform workloads on a per-node basis. (NodeSwap)
* OpenStack Machine API Provider. This gate has no effect and is planned to be removed from this feature set in a future release. (MachineAPIProviderOpenStack)
* Insights Operator. Enables the InsightsDataGather CRD, which allows users to configure some Insights data gathering options. The feature set also enables the DataGather CRD, which allows users to run Insights data gathering on-demand. (InsightsConfigAPI)
* Insights Operator. Enables a new data collection feature called 'Insights Runtime Extractor' which, when enabled, allows Red Hat to gather more runtime workload data about your Red Hat OpenShift Container Platform containers. (InsightsRuntimeExtractor)
* Dynamic Resource Allocation API. Enables a new API for requesting and sharing resources between pods and containers. This is an internal feature that most users do not need to interact with. (DynamicResourceAllocation)
* Pod security admission enforcement. Enables the restricted enforcement mode for pod security admission. Instead of only logging a warning, pods are rejected if they violate pod security standards. (OpenShiftPodSecurityAdmission)
* StatefulSet pod availability upgrading limits. Enables users to define the maximum number of statefulset pods unavailable during updates which reduces application downtime. (MaxUnavailableStatefulSet)
* Image mode behavior of image streams. Enables a new API for controlling the import mode behavior of image streams. (imageStreamImportMode)
* OVNObservability resource allows you to verify expected network behavior. Supports the following network APIs: NetworkPolicy, AdminNetworkPolicy, BaselineNetworkPolicy, UserDefinesdNetwork isolation, multicast ACLs, and egress firewalls. When enabled, you can view network events in the terminal.
* gcpLabelsTags
* vSphereStaticIPs
* routeExternalCertificate
* automatedEtcdBackup
* gcpClusterHostedDNS
* vSphereControlPlaneMachineset
* dnsNameResolver
* machineConfigNodes
* metricsServer
* installAlternateInfrastructureAWS
* mixedCPUsAllocation
* managedBootImages
* onClusterBuild
* signatureStores
* SigstoreImageVerification
* DisableKubeletCloudCredentialProviders
* BareMetalLoadBalancer
* ClusterAPIInstallAWS
* ClusterAPIInstallAzure
* ClusterAPIInstallNutanix
* ClusterAPIInstallOpenStack
* ClusterAPIInstallVSphere
* HardwareSpeed
* KMSv1
* NetworkDiagnosticsConfig
* VSphereDriverConfiguration
* ExternalOIDC
* ChunkSizeMiB
* ClusterAPIInstallGCP
* ClusterAPIInstallPowerVS
* EtcdBackendQuota
* InsightsConfig
* InsightsOnDemandDataGather
* MetricsCollectionProfiles
* NewOLM
* AWSClusterHostedDNS
* AdditionalRoutingCapabilities
* AutomatedEtcdBackup
* BootcNodeManagement
* CSIDriverSharedResource
* ClusterMonitoringConfig
* ConsolePluginContentSecurityPolicy
* DNSNameResolver
* DynamicResourceAllocation
* EtcdBackendQuota
* Example
* GCPClusterHostedDNS
* ImageStreamImportMode
* IngressControllerDynamicConfigurationManager
* InsightsConfig
* InsightsConfigAPI
* InsightsOnDemandDataGather
* InsightsRuntimeExtractor
* MachineAPIProviderOpenStack
* MachineConfigNodes
* MaxUnavailableStatefulSet
* MetricsCollectionProfiles
* MinimumKubeletVersion
* MixedCPUsAllocation
* NetworkSegmentation
* NodeSwap
* NutanixMultiSubnets
* OVNObservability
* OnClusterBuild
* OpenShiftPodSecurityAdmission
* PersistentIPsForVirtualization
* PinnedImages
* PlatformOperators
* ProcMountType
* RouteAdvertisements
* RouteExternalCertificate
* ServiceAccountTokenNodeBinding
* SignatureStores
* SigstoreImageVerification
* TranslateStreamCloseWebsocketRequests
* UpgradeStatus
* UserNamespacesPodSecurityStandards
* UserNamespacesSupport
* VSphereMultiNetworks
* VolumeAttributesClass
* VolumeGroupSnapshot
* ExternalOIDC
* AWSEFSDriverVolumeMetrics
* AdminNetworkPolicy
* AlibabaPlatform
* AzureWorkloadIdentity
* BareMetalLoadBalancer
* BuildCSIVolumes
* ChunkSizeMiB
* CloudDualStackNodeIPs
* DisableKubeletCloudCredentialProviders
* GCPLabelsTags
* HardwareSpeed
* IngressControllerLBSubnetsAWS
* KMSv1
* ManagedBootImages
* ManagedBootImagesAWS
* MultiArchInstallAWS
* MultiArchInstallGCP
* NetworkDiagnosticsConfig
* NetworkLiveMigration
* NodeDisruptionPolicy
* PrivateHostedZoneAWS
* SetEIPForNLBIngressController
* VSphereControlPlaneMachineSet
* VSphereDriverConfiguration
* VSphereMultiVCenters
* VSphereStaticIPs
* ValidatingAdmissionPolicy

## Enabling feature sets using the web console

You can use the Red Hat OpenShift Container Platform web console to enable feature sets for all of the nodes in a cluster by editing the FeatureGate custom resource (CR).

To enable feature sets:

1. In the Red Hat OpenShift Container Platform web console, switch to the Administration -> Custom Resource Definitions page.
2. On the Custom Resource Definitions page, click FeatureGate.
3. On the Custom Resource Definition Details page, click the Instances tab.
4. Click the cluster feature gate, then click the YAML tab.
5. Edit the cluster instance to add specific feature sets:

[WARNING]
----
Enabling the TechPreviewNoUpgrade feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
----
Sample Feature Gate custom resource

```yaml
apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster 1
# ...
spec:
  featureSet: TechPreviewNoUpgrade 2
```

The name of the FeatureGate CR must be cluster.
Add the feature set that you want to enable:
* TechPreviewNoUpgrade enables specific Technology Preview features.

After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.

You can verify that the feature gates are enabled by looking at the kubelet.conf file on a node after the nodes return to the ready state.

1. From the Administrator perspective in the web console, navigate to Compute -> Nodes.
2. Select a node.
3. In the Node details page, click Terminal.
4. In the terminal window, change your root directory to /host:

```terminal
sh-4.2# chroot /host
```

5. View the kubelet.conf file:

```terminal
sh-4.2# cat /etc/kubernetes/kubelet.conf
```

Sample output

```terminal
# ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...
```


The features that are listed as true are enabled on your cluster.

[NOTE]
----
The features listed vary depending upon the Red Hat OpenShift Container Platform version.
----

## Enabling feature sets using the CLI

You can use the OpenShift CLI (oc) to enable feature sets for all of the nodes in a cluster by editing the FeatureGate custom resource (CR).

* You have installed the OpenShift CLI (oc).

To enable feature sets:

1. Edit the FeatureGate CR named cluster:

```terminal
$ oc edit featuregate cluster
```


[WARNING]
----
Enabling the TechPreviewNoUpgrade feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
----
Sample FeatureGate custom resource

```yaml
apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster 1
# ...
spec:
  featureSet: TechPreviewNoUpgrade 2
```

The name of the FeatureGate CR must be cluster.
Add the feature set that you want to enable:
* TechPreviewNoUpgrade enables specific Technology Preview features.

After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.

You can verify that the feature gates are enabled by looking at the kubelet.conf file on a node after the nodes return to the ready state.

1. From the Administrator perspective in the web console, navigate to Compute -> Nodes.
2. Select a node.
3. In the Node details page, click Terminal.
4. In the terminal window, change your root directory to /host:

```terminal
sh-4.2# chroot /host
```

5. View the kubelet.conf file:

```terminal
sh-4.2# cat /etc/kubernetes/kubelet.conf
```

Sample output

```terminal
# ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...
```


The features that are listed as true are enabled on your cluster.

[NOTE]
----
The features listed vary depending upon the Red Hat OpenShift Container Platform version.
----

# etcd tasks

Back up etcd, enable or disable etcd encryption, or defragment etcd data.


[NOTE]
----
If you deployed a bare-metal cluster, you can scale the cluster up to 5 nodes as part of your post-installation tasks. For more information, see Node scaling for etcd.
----

## About etcd encryption

By default, etcd data is not encrypted in Red Hat OpenShift Container Platform. You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.

When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:

* Secrets
* Config maps
* Routes
* OAuth access tokens
* OAuth authorize tokens

When you enable etcd encryption, encryption keys are created. You must have these keys to restore from an etcd backup.


[NOTE]
----
Etcd encryption only encrypts values, not keys. Resource types, namespaces, and object names are unencrypted.
If etcd encryption is enabled during a backup, the static_kuberesources_<datetimestamp>.tar.gz file contains the encryption keys for the etcd snapshot. For security reasons, store this file separately from the etcd snapshot. However, this file is required to restore a previous state of etcd from the respective etcd snapshot.
----

## Supported encryption types

The following encryption types are supported for encrypting etcd data in Red Hat OpenShift Container Platform:

AES-CBC:: Uses AES-CBC with PKCS#7 padding and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.
AES-GCM:: Uses AES-GCM with a random nonce and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.

## Enabling etcd encryption

You can enable etcd encryption to encrypt sensitive resources in your cluster.


[WARNING]
----
Do not back up etcd resources until the initial encryption process is completed. If the encryption process is not completed, the backup might be only partially encrypted.
After you enable etcd encryption, several changes can occur:
* The etcd encryption might affect the memory consumption of a few resources.
* You might notice a transient affect on backup performance because the leader must serve the backup.
* A disk I/O can affect the node that receives the backup state.
----

You can encrypt the etcd database in either AES-GCM or AES-CBC encryption.


[NOTE]
----
To migrate your etcd database from one encryption type to the other, you can modify the API server's spec.encryption.type field. Migration of the etcd data to the new encryption type occurs automatically.
----

* Access to the cluster as a user with the cluster-admin role.

1. Modify the APIServer object:

```terminal
$ oc edit apiserver
```

2. Set the spec.encryption.type field to aesgcm or aescbc:

```yaml
spec:
  encryption:
    type: aesgcm 1
```

Set to aesgcm for AES-GCM encryption or aescbc for AES-CBC encryption.
3. Save the file to apply the changes.

The encryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of the etcd database.
4. Verify that etcd encryption was successful.
1. Review the Encrypted status condition for the OpenShift API server to verify that its resources were successfully encrypted:

```terminal
$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows EncryptionCompleted upon successful encryption:

```terminal
EncryptionCompleted
All resources encrypted: routes.route.openshift.io
```


If the output shows EncryptionInProgress, encryption is still in progress. Wait a few minutes and try again.
2. Review the Encrypted status condition for the Kubernetes API server to verify that its resources were successfully encrypted:

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows EncryptionCompleted upon successful encryption:

```terminal
EncryptionCompleted
All resources encrypted: secrets, configmaps
```


If the output shows EncryptionInProgress, encryption is still in progress. Wait a few minutes and try again.
3. Review the Encrypted status condition for the OpenShift OAuth API server to verify that its resources were successfully encrypted:

```terminal
$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows EncryptionCompleted upon successful encryption:

```terminal
EncryptionCompleted
All resources encrypted: oauthaccesstokens.oauth.openshift.io, oauthauthorizetokens.oauth.openshift.io
```


If the output shows EncryptionInProgress, encryption is still in progress. Wait a few minutes and try again.

## Disabling etcd encryption

You can disable encryption of etcd data in your cluster.

* Access to the cluster as a user with the cluster-admin role.

1. Modify the APIServer object:

```terminal
$ oc edit apiserver
```

2. Set the encryption field type to identity:

```yaml
spec:
  encryption:
    type: identity 1
```

The identity type is the default value and means that no encryption is performed.
3. Save the file to apply the changes.

The decryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of your cluster.
4. Verify that etcd decryption was successful.
1. Review the Encrypted status condition for the OpenShift API server to verify that its resources were successfully decrypted:

```terminal
$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows DecryptionCompleted upon successful decryption:

```terminal
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
```


If the output shows DecryptionInProgress, decryption is still in progress. Wait a few minutes and try again.
2. Review the Encrypted status condition for the Kubernetes API server to verify that its resources were successfully decrypted:

```terminal
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows DecryptionCompleted upon successful decryption:

```terminal
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
```


If the output shows DecryptionInProgress, decryption is still in progress. Wait a few minutes and try again.
3. Review the Encrypted status condition for the OpenShift OAuth API server to verify that its resources were successfully decrypted:

```terminal
$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
```


The output shows DecryptionCompleted upon successful decryption:

```terminal
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
```


If the output shows DecryptionInProgress, decryption is still in progress. Wait a few minutes and try again.

## Backing up etcd data

Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.


[IMPORTANT]
----
Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.
----

* You have access to the cluster as a user with the cluster-admin role.
* You have checked whether the cluster-wide proxy is enabled.

[TIP]
----
You can check whether the proxy is enabled by reviewing the output of oc get proxy cluster -o yaml. The proxy is enabled if the httpProxy, httpsProxy, and noProxy fields have values set.
----

1. Start a debug session as root for a control plane node:

```terminal
$ oc debug --as-root node/<node_name>
```

2. Change your root directory to /host in the debug shell:

```terminal
sh-4.4# chroot /host
```

3. If the cluster-wide proxy is enabled, export the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables by running the following commands:

```terminal
$ export HTTP_PROXY=http://<your_proxy.example.com>:8080
```


```terminal
$ export HTTPS_PROXY=https://<your_proxy.example.com>:8080
```


```terminal
$ export NO_PROXY=<example.com>
```

4. Run the cluster-backup.sh script in the debug shell and pass in the location to save the backup to.

[TIP]
----
The cluster-backup.sh script is maintained as a component of the etcd Cluster Operator and is a wrapper around the etcdctl snapshot save command.
----

```terminal
sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup
```

Example script output

```terminal
found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1
etcdctl version: 3.4.14
API version: 3.4
{"level":"info","ts":1624647639.0188997,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part"}
{"level":"info","ts":"2021-06-25T19:00:39.030Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1624647639.0301006,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://10.0.0.5:2379"}
{"level":"info","ts":"2021-06-25T19:00:40.215Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1624647640.6032252,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://10.0.0.5:2379","size":"114 MB","took":1.584090459}
{"level":"info","ts":1624647640.6047094,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db"}
Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db
{"hash":3866667823,"revision":31407,"totalKey":12828,"totalSize":114446336}
snapshot db and kube resources are successfully saved to /home/core/assets/backup
```


In this example, two files are created in the /home/core/assets/backup/ directory on the control plane host:
* snapshot_<datetimestamp>.db: This file is the etcd snapshot. The cluster-backup.sh script confirms its validity.
* static_kuberesources_<datetimestamp>.tar.gz: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot.

[NOTE]
----
If etcd encryption is enabled, it is recommended to store this second file separately from the etcd snapshot for security reasons. However, this file is required to restore from the etcd snapshot.
Keep in mind that etcd encryption only encrypts values, not keys. This means that resource types, namespaces, and object names are unencrypted.
----

## Defragmenting etcd data

For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.

Monitor these key metrics:

* etcd_server_quota_backend_bytes, which is the current quota limit
* etcd_mvcc_db_total_size_in_use_in_bytes, which indicates the actual database usage after a history compaction
* etcd_mvcc_db_total_size_in_bytes, which shows the database size, including free space waiting for defragmentation

Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.

History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.

Defragmentation occurs automatically, but you can also trigger it manually.


[NOTE]
----
Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.
----

### Automatic defragmentation

The etcd Operator automatically defragments disks. No manual intervention is needed.

Verify that the defragmentation process is successful by viewing one of these logs:

* etcd logs
* cluster-etcd-operator pod
* operator status error log


[WARNING]
----
Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.
----


```terminal
etcd member has been defragmented: <member_name>, memberID: <member_id>
```



```terminal
failed defrag on member: <member_name>, memberID: <member_id>: <error_message>
```


### Manual defragmentation

A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:

* When etcd uses more than 50% of its available space for more than 10 minutes
* When etcd is actively using less than 50% of its total database size for more than 10 minutes

You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: (etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024


[WARNING]
----
Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.
----

Follow this procedure to defragment etcd data on each etcd member.

* You have access to the cluster as a user with the cluster-admin role.

1. Determine which etcd member is the leader, because the leader should be defragmented last.
1. Get the list of etcd pods:

```terminal
$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide
```

Example output

```terminal
etcd-ip-10-0-159-225.example.redhat.com                3/3     Running     0          175m   10.0.159.225   ip-10-0-159-225.example.redhat.com   <none>           <none>
etcd-ip-10-0-191-37.example.redhat.com                 3/3     Running     0          173m   10.0.191.37    ip-10-0-191-37.example.redhat.com    <none>           <none>
etcd-ip-10-0-199-170.example.redhat.com                3/3     Running     0          176m   10.0.199.170   ip-10-0-199-170.example.redhat.com   <none>           <none>
```

2. Choose a pod and run the following command to determine which etcd member is the leader:

```terminal
$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com etcdctl endpoint status --cluster -w table
```

Example output

```terminal
Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-ip-10-0-159-225.example.redhat.com -n openshift-etcd' to see all of the containers in this pod.
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```


Based on the IS LEADER column of this output, the https://10.0.199.170:2379 endpoint is the leader. Matching this endpoint with the output of the previous step, the pod name of the leader is etcd-ip-10-0-199-170.example.redhat.com.
2. Defragment an etcd member.
1. Connect to the running etcd container, passing in the name of a pod that is not the leader:

```terminal
$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com
```

2. Unset the ETCDCTL_ENDPOINTS environment variable:

```terminal
sh-4.4# unset ETCDCTL_ENDPOINTS
```

3. Defragment the etcd member:

```terminal
sh-4.4# etcdctl --command-timeout=30s --endpoints=https://localhost:2379 defrag
```

Example output

```terminal
Finished defragmenting etcd member[https://localhost:2379]
```


If a timeout error occurs, increase the value for --command-timeout until the command succeeds.
4. Verify that the database size was reduced:

```terminal
sh-4.4# etcdctl endpoint status -w table --cluster
```

Example output

```terminal
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |   41 MB |     false |      false |         7 |      91624 |              91624 |        | 1
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```


This example shows that the database size for this etcd member is now 41 MB as opposed to the starting size of 104 MB.
5. Repeat these steps to connect to each of the other etcd members and defragment them. Always defragment the leader last.

Wait at least one minute between defragmentation actions to allow the etcd pod to recover. Until the etcd pod recovers, the etcd member will not respond.
3. If any NOSPACE alarms were triggered due to the space quota being exceeded, clear them.
1. Check if there are any NOSPACE alarms:

```terminal
sh-4.4# etcdctl alarm list
```

Example output

```terminal
memberID:12345678912345678912 alarm:NOSPACE
```

2. Clear the alarms:

```terminal
sh-4.4# etcdctl alarm disarm
```


## Restoring to a previous cluster state

You can use a saved etcd backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.

For high availability (HA) clusters, a three-node HA cluster requires you to shut down etcd on two hosts to avoid a cluster split. On four-node and five-node HA clusters, you must shut down three hosts. Quorum requires a simple majority of nodes. The minimum number of nodes required for quorum on a three-node HA cluster is two. On four-node and five-node HA clusters, the minimum number of nodes required for quorum is three. If you start a new cluster from backup on your recovery host, the other etcd members might still be able to form quorum and continue service.


[NOTE]
----
If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure. For Red Hat OpenShift Container Platform on a single node, see "Restoring to a previous cluster state for a single node".
----


[IMPORTANT]
----
When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an Red Hat OpenShift Container Platform 4.19.2 cluster must use an etcd backup that was taken from 4.19.2.
----

* Access to the cluster as a user with the cluster-admin role through a certificate-based kubeconfig file, like the one that was used during installation.
* A healthy control plane host to use as the recovery host.
* You have SSH access to control plane hosts.
* A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: snapshot_<datetimestamp>.db and static_kuberesources_<datetimestamp>.tar.gz.


[IMPORTANT]
----
For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.
----

1. Select a control plane host to use as the recovery host. This is the host that you run the restore operation on.
2. Establish SSH connectivity to each of the control plane nodes, including the recovery host.

kube-apiserver becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal.

[IMPORTANT]
----
If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.
----
3. Using SSH, connect to each control plane node and run the following command to disable etcd:

```terminal
$ sudo -E /usr/local/bin/disable-etcd.sh
```

4. Copy the etcd backup directory to the recovery control plane host.

This procedure assumes that you copied the backup directory containing the etcd snapshot and the resources for the static pods to the /home/core/ directory of your recovery control plane host.
5. Use SSH to connect to the recovery host and restore the cluster from a previous backup by running the following command:

```terminal
$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/<etcd-backup-directory>
```

6. Exit the SSH session.
7. Once the API responds, turn off the etcd Operator quorum guard by runnning the following command:

```terminal
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'
```

8. Monitor the recovery progress of the control plane by running the following command:

```terminal
$ oc adm wait-for-stable-cluster
```


[NOTE]
----
It can take up to 15 minutes for the control plane to recover.
----
9. Once recovered, enable the quorum guard by running the following command:

```terminal
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'
```


If you see no progress rolling out the etcd static pods, you can force redeployment from the cluster-etcd-operator by running the following command:


```terminal
$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$(date --rfc-3339=ns )"'"}}' --type=merge
```


* Recommended etcd practices
* Installing a user-provisioned cluster on bare metal
* Replacing a bare-metal control plane node

## Issues and workarounds for restoring a persistent storage state

If your Red Hat OpenShift Container Platform cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a StatefulSet object. When you restore from an etcd backup, the status of the workloads in Red Hat OpenShift Container Platform is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.


[IMPORTANT]
----
The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an Red Hat OpenShift Container Platform cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.
----

The following are some example scenarios that produce an out-of-date status:

* MySQL database is running in a pod backed up by a PV object. Restoring Red Hat OpenShift Container Platform from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.
* Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. Red Hat OpenShift Container Platform is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.
* Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.
* A device is removed or renamed from Red Hat OpenShift Container Platform nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from /dev/disk/by-id or /dev directories. This situation might cause the local PVs to refer to devices that no longer exist.

To fix this problem, an administrator must:
1. Manually remove the PVs with invalid devices.
2. Remove symlinks from respective nodes.
3. Delete LocalVolume or LocalVolumeSet objects (see Storage -> Configuring persistent storage -> Persistent storage using local volumes -> Deleting the Local Storage Operator Resources).

# Pod disruption budgets

Understand and configure pod disruption budgets.

## Understanding how to use pod disruption budgets to specify the number of pods that must be up

A pod disruption budget allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.

PodDisruptionBudget is an API object that specifies the minimum number or
percentage of replicas that must be up at a time. Setting these in projects can
be helpful during node maintenance (such as scaling a cluster down or a cluster
upgrade) and is only honored on voluntary evictions (not on node failures).

A PodDisruptionBudget object&#8217;s configuration consists of the following key
parts:

* A label selector, which is a label query over a set of pods.
* An availability level, which specifies the minimum number of pods that must be
available simultaneously, either:
* minAvailable is the number of pods must always be available, even during a disruption.
* maxUnavailable is the number of pods can be unavailable during a disruption.


[NOTE]
----
Available refers to the number of pods that has condition Ready=True.
Ready=True refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.
A maxUnavailable of 0% or 0 or a minAvailable of 100% or equal to the number of replicas
is permitted but can block nodes from being drained.
----


[WARNING]
----
The default setting for maxUnavailable is 1 for all the machine config pools in Red Hat OpenShift Container Platform. It is recommended to not change this value and update one control plane node at a time. Do not change this value to 3 for the control plane pool.
----

You can check for pod disruption budgets across all projects with the following:


```terminal
$ oc get poddisruptionbudget --all-namespaces
```



[NOTE]
----
The following example contains some values that are specific to Red Hat OpenShift Container Platform on AWS.
----


```terminal
NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...
```


The PodDisruptionBudget is considered healthy when there are at least
minAvailable pods running in the system. Every pod above that limit can be evicted.


[NOTE]
----
Depending on your pod priority and preemption settings,
lower-priority pods might be removed despite their pod disruption budget requirements.
----

## Specifying the number of pods that must be up with pod disruption budgets

You can use a PodDisruptionBudget object to specify the minimum number or
percentage of replicas that must be up at a time.

To configure a pod disruption budget:

1. Create a YAML file with the an object definition similar to the following:

```yaml
apiVersion: policy/v1 1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  2
  selector:  3
    matchLabels:
      name: my-pod
```

PodDisruptionBudget is part of the policy/v1 API group.
The minimum number of pods that must be available simultaneously. This can
be either an integer or a string specifying a percentage, for example, 20%.
A label query over a set of resources. The result of matchLabels and
matchExpressions are logically conjoined. Leave this parameter blank, for example selector {}, to select all pods in the project.

Or:

```yaml
apiVersion: policy/v1 1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% 2
  selector: 3
    matchLabels:
      name: my-pod
```

PodDisruptionBudget is part of the policy/v1 API group.
The maximum number of pods that can be unavailable simultaneously. This can
be either an integer or a string specifying a percentage, for example, 20%.
A label query over a set of resources. The result of matchLabels and
matchExpressions are logically conjoined. Leave this parameter blank, for example selector {}, to select all pods in the project.
2. Run the following command to add the object to project:

```terminal
$ oc create -f </path/to/file> -n <project_name>
```


## Specifying the eviction policy for unhealthy pods

When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.

You can choose one of the following policies:

IfHealthyBudget:: Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.
AlwaysAllow:: Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the CrashLoopBackOff state or failing to report the Ready status.

[NOTE]
----
It is recommended to set the unhealthyPodEvictionPolicy field to AlwaysAllow in the PodDisruptionBudget object to support the eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.
----

1. Create a YAML file that defines a PodDisruptionBudget object and specify the unhealthy pod eviction policy:
Example pod-disruption-budget.yaml file

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow 1
```

Choose either IfHealthyBudget or AlwaysAllow as the unhealthy pod eviction policy. The default is IfHealthyBudget when the unhealthyPodEvictionPolicy field is empty.
2. Create the PodDisruptionBudget object by running the following command:

```terminal
$ oc create -f pod-disruption-budget.yaml
```


With a PDB that has the AlwaysAllow unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.

* Enabling features using feature gates
* Unhealthy Pod Eviction Policy in the Kubernetes documentation